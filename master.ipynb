{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "999bcd7d",
   "metadata": {},
   "source": [
    "# Lab 3: Contextual Bandit-Based News Article Recommendation\n",
    "\n",
    "**`Course`:** Reinforcement Learning Fundamentals  \n",
    "**`Student Name`:**  \n",
    "**`Roll Number`:**  \n",
    "**`GitHub Branch`:** firstname_U20230xxx  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755efd7a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Imports and Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bc73a3-aed3-45ef-9d54-b6d7085f3410",
   "metadata": {},
   "source": [
    "# Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a44cc661-3c25-4ec6-af7c-9023d1d52aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from rlcmab_sampler import sampler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c638ba06",
   "metadata": {},
   "source": [
    "# Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48f6f6a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                link  \\\n",
      "0  https://www.huffpost.com/entry/covid-boosters-...   \n",
      "1  https://www.huffpost.com/entry/american-airlin...   \n",
      "2  https://www.huffpost.com/entry/funniest-tweets...   \n",
      "3  https://www.huffpost.com/entry/funniest-parent...   \n",
      "4  https://www.huffpost.com/entry/amy-cooper-lose...   \n",
      "\n",
      "                                            headline   category  \\\n",
      "0  Over 4 Million Americans Roll Up Sleeves For O...  U.S. NEWS   \n",
      "1  American Airlines Flyer Charged, Banned For Li...  U.S. NEWS   \n",
      "2  23 Of The Funniest Tweets About Cats And Dogs ...     COMEDY   \n",
      "3  The Funniest Tweets From Parents This Week (Se...  PARENTING   \n",
      "4  Woman Who Called Cops On Black Bird-Watcher Lo...  U.S. NEWS   \n",
      "\n",
      "                                   short_description               authors  \\\n",
      "0  Health experts said it is too early to predict...  Carla K. Johnson, AP   \n",
      "1  He was subdued by passengers and crew when he ...        Mary Papenfuss   \n",
      "2  \"Until you have a dog you don't understand wha...         Elyse Wanshel   \n",
      "3  \"Accidentally put grown-up toothpaste on my to...      Caroline Bologna   \n",
      "4  Amy Cooper accused investment firm Franklin Te...        Nina Golgowski   \n",
      "\n",
      "         date  \n",
      "0  2022-09-23  \n",
      "1  2022-09-23  \n",
      "2  2022-09-23  \n",
      "3  2022-09-23  \n",
      "4  2022-09-22  \n",
      "   user_id  age  income  clicks  purchase_amount  label\n",
      "0        1   28   58242      81           378.38  user3\n",
      "1        2   28   38225      21           114.50  user3\n",
      "2        3   39   95017      41            66.24  user2\n",
      "3        4   52   33473      98           496.88  user3\n",
      "4        5   29   80690       5           293.24  user1\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "news_df = pd.read_csv(\"data/news_articles.csv\")\n",
    "train_users = pd.read_csv(\"data/train_users.csv\")\n",
    "test_users = pd.read_csv(\"data/test_users.csv\")\n",
    "\n",
    "print(news_df.head())\n",
    "print(train_users.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1c5cb1",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "In this section:\n",
    "- Handle missing values\n",
    "- Encode categorical features\n",
    "- Prepare data for user classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7195ba1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "1. LOADING DATASETS\n",
      "================================================================================\n",
      "\n",
      "News Articles Dataset Shape: (209527, 6)\n",
      "Train Users Dataset Shape: (2000, 6)\n",
      "Test Users Dataset Shape: (2000, 6)\n",
      "\n",
      "News Articles Columns: ['link', 'headline', 'category', 'short_description', 'authors', 'date']\n",
      "Train Users Columns: ['user_id', 'age', 'income', 'clicks', 'purchase_amount', 'label']\n",
      "Test Users Columns: ['user_id', 'age', 'income', 'clicks', 'purchase_amount', 'label']\n",
      "\n",
      "================================================================================\n",
      "2. DATA CLEANING - HANDLING MISSING VALUES\n",
      "================================================================================\n",
      "\n",
      "Missing values in News Articles:\n",
      "link                     0\n",
      "headline                 6\n",
      "category                 0\n",
      "short_description    19712\n",
      "authors              37418\n",
      "date                     0\n",
      "dtype: int64\n",
      "Total missing: 57136\n",
      "\n",
      "Missing values in Train Users:\n",
      "user_id            0\n",
      "age                0\n",
      "income             0\n",
      "clicks             0\n",
      "purchase_amount    0\n",
      "label              0\n",
      "dtype: int64\n",
      "Total missing: 0\n",
      "\n",
      "Missing values in Test Users:\n",
      "user_id            0\n",
      "age                0\n",
      "income             0\n",
      "clicks             0\n",
      "purchase_amount    0\n",
      "label              0\n",
      "dtype: int64\n",
      "Total missing: 0\n",
      "\n",
      "Missing values after cleaning:\n",
      "News Articles: 6\n",
      "Train Users: 0\n",
      "Test Users: 0\n",
      "\n",
      "================================================================================\n",
      "3. FEATURE ENCODING FOR CLASSIFICATION AND BANDIT TRAINING\n",
      "================================================================================\n",
      "\n",
      "Encoding user labels...\n",
      "Original labels: <StringArray>\n",
      "['user3', 'user2', 'user1']\n",
      "Length: 3, dtype: str\n",
      "Encoded labels: [2 1 0]\n",
      "Mapping: {'user1': np.int64(0), 'user2': np.int64(1), 'user3': np.int64(2)}\n",
      "\n",
      "Encoding news article categories...\n",
      "Original categories: <StringArray>\n",
      "[     'U.S. NEWS',         'COMEDY',      'PARENTING',     'WORLD NEWS',\n",
      " 'CULTURE & ARTS',           'TECH',         'SPORTS',  'ENTERTAINMENT',\n",
      "       'POLITICS',     'WEIRD NEWS',    'ENVIRONMENT',      'EDUCATION',\n",
      "          'CRIME',        'SCIENCE',       'WELLNESS',       'BUSINESS',\n",
      " 'STYLE & BEAUTY',   'FOOD & DRINK',          'MEDIA',   'QUEER VOICES',\n",
      "  'HOME & LIVING',          'WOMEN',   'BLACK VOICES',         'TRAVEL',\n",
      "          'MONEY',       'RELIGION',  'LATINO VOICES',         'IMPACT',\n",
      "       'WEDDINGS',        'COLLEGE',        'PARENTS', 'ARTS & CULTURE',\n",
      "          'STYLE',          'GREEN',          'TASTE', 'HEALTHY LIVING',\n",
      "  'THE WORLDPOST',      'GOOD NEWS',      'WORLDPOST',          'FIFTY',\n",
      "           'ARTS',        'DIVORCE']\n",
      "Length: 42, dtype: str\n",
      "Encoded categories: [35  5 22 40  7 32 28 10 24 37 11  9  6 27 38  3 30 13 20 25 17 39  2 34\n",
      " 21 26 19 18 36  4 23  1 29 15 31 16 33 14 41 12  0  8]\n",
      "Mapping: {'ARTS': np.int64(0), 'ARTS & CULTURE': np.int64(1), 'BLACK VOICES': np.int64(2), 'BUSINESS': np.int64(3), 'COLLEGE': np.int64(4), 'COMEDY': np.int64(5), 'CRIME': np.int64(6), 'CULTURE & ARTS': np.int64(7), 'DIVORCE': np.int64(8), 'EDUCATION': np.int64(9), 'ENTERTAINMENT': np.int64(10), 'ENVIRONMENT': np.int64(11), 'FIFTY': np.int64(12), 'FOOD & DRINK': np.int64(13), 'GOOD NEWS': np.int64(14), 'GREEN': np.int64(15), 'HEALTHY LIVING': np.int64(16), 'HOME & LIVING': np.int64(17), 'IMPACT': np.int64(18), 'LATINO VOICES': np.int64(19), 'MEDIA': np.int64(20), 'MONEY': np.int64(21), 'PARENTING': np.int64(22), 'PARENTS': np.int64(23), 'POLITICS': np.int64(24), 'QUEER VOICES': np.int64(25), 'RELIGION': np.int64(26), 'SCIENCE': np.int64(27), 'SPORTS': np.int64(28), 'STYLE': np.int64(29), 'STYLE & BEAUTY': np.int64(30), 'TASTE': np.int64(31), 'TECH': np.int64(32), 'THE WORLDPOST': np.int64(33), 'TRAVEL': np.int64(34), 'U.S. NEWS': np.int64(35), 'WEDDINGS': np.int64(36), 'WEIRD NEWS': np.int64(37), 'WELLNESS': np.int64(38), 'WOMEN': np.int64(39), 'WORLD NEWS': np.int64(40), 'WORLDPOST': np.int64(41)}\n",
      "\n",
      "Preparing feature sets for user classification...\n",
      "\n",
      "Train Features Shape: (2000, 4)\n",
      "Train Labels Shape: (2000,)\n",
      "Test Features Shape: (2000, 4)\n",
      "Test Labels Shape: (2000,)\n",
      "\n",
      "Features normalized using StandardScaler\n",
      "Train features stats after scaling:\n",
      "  Mean: [ 9.59232693e-17  4.79616347e-17 -3.10862447e-17  9.59232693e-17]\n",
      "  Std: [1. 1. 1. 1.]\n",
      "\n",
      "================================================================================\n",
      "PREPROCESSING SUMMARY\n",
      "================================================================================\n",
      "\n",
      "‚úì Loaded 3 datasets successfully\n",
      "‚úì Handled missing values in all datasets\n",
      "‚úì Encoded categorical features (user labels and article categories)\n",
      "‚úì Normalized numerical features for ML training\n",
      "\n",
      "Ready for user classification and contextual bandit training!\n"
     ]
    }
   ],
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "# 1. Load the provided user and article datasets\n",
    "print(\"=\" * 80)\n",
    "print(\"1. LOADING DATASETS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "news_df = pd.read_csv(\"data/news_articles.csv\")\n",
    "train_users_df = pd.read_csv(\"data/train_users.csv\")\n",
    "test_users_df = pd.read_csv(\"data/test_users.csv\")\n",
    "\n",
    "print(f\"\\nNews Articles Dataset Shape: {news_df.shape}\")\n",
    "print(f\"Train Users Dataset Shape: {train_users_df.shape}\")\n",
    "print(f\"Test Users Dataset Shape: {test_users_df.shape}\")\n",
    "\n",
    "print(\"\\nNews Articles Columns:\", news_df.columns.tolist())\n",
    "print(\"Train Users Columns:\", train_users_df.columns.tolist())\n",
    "print(\"Test Users Columns:\", test_users_df.columns.tolist())\n",
    "\n",
    "# 2. Data Cleaning - Handle Missing Values\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"2. DATA CLEANING - HANDLING MISSING VALUES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check for missing values in each dataset\n",
    "print(\"\\nMissing values in News Articles:\")\n",
    "print(news_df.isnull().sum())\n",
    "print(f\"Total missing: {news_df.isnull().sum().sum()}\")\n",
    "\n",
    "print(\"\\nMissing values in Train Users:\")\n",
    "print(train_users_df.isnull().sum())\n",
    "print(f\"Total missing: {train_users_df.isnull().sum().sum()}\")\n",
    "\n",
    "print(\"\\nMissing values in Test Users:\")\n",
    "print(test_users_df.isnull().sum())\n",
    "print(f\"Total missing: {test_users_df.isnull().sum().sum()}\")\n",
    "\n",
    "# Handle missing values in news articles\n",
    "# Fill missing authors with 'Unknown'\n",
    "news_df['authors'] = news_df['authors'].fillna('Unknown')\n",
    "\n",
    "# Fill missing short_description with empty string\n",
    "news_df['short_description'] = news_df['short_description'].fillna('')\n",
    "\n",
    "# Fill missing date with mode (most frequent date)\n",
    "if news_df['date'].isnull().sum() > 0:\n",
    "    news_df['date'] = news_df['date'].fillna(news_df['date'].mode()[0] if not news_df['date'].mode().empty else 'Unknown')\n",
    "\n",
    "# Handle missing values in user datasets (if any)\n",
    "# Fill numerical columns with median\n",
    "train_users_df['age'] = train_users_df['age'].fillna(train_users_df['age'].median())\n",
    "train_users_df['income'] = train_users_df['income'].fillna(train_users_df['income'].median())\n",
    "train_users_df['clicks'] = train_users_df['clicks'].fillna(train_users_df['clicks'].median())\n",
    "train_users_df['purchase_amount'] = train_users_df['purchase_amount'].fillna(train_users_df['purchase_amount'].median())\n",
    "\n",
    "test_users_df['age'] = test_users_df['age'].fillna(test_users_df['age'].median())\n",
    "test_users_df['income'] = test_users_df['income'].fillna(test_users_df['income'].median())\n",
    "test_users_df['clicks'] = test_users_df['clicks'].fillna(test_users_df['clicks'].median())\n",
    "test_users_df['purchase_amount'] = test_users_df['purchase_amount'].fillna(test_users_df['purchase_amount'].median())\n",
    "\n",
    "print(\"\\nMissing values after cleaning:\")\n",
    "print(f\"News Articles: {news_df.isnull().sum().sum()}\")\n",
    "print(f\"Train Users: {train_users_df.isnull().sum().sum()}\")\n",
    "print(f\"Test Users: {test_users_df.isnull().sum().sum()}\")\n",
    "\n",
    "# 3. Feature Encoding for Classification and Bandit Training\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"3. FEATURE ENCODING FOR CLASSIFICATION AND BANDIT TRAINING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Encode categorical labels in user datasets (Convert user categories to numerical)\n",
    "print(\"\\nEncoding user labels...\")\n",
    "label_encoder = LabelEncoder()\n",
    "train_users_df['label_encoded'] = label_encoder.fit_transform(train_users_df['label'])\n",
    "test_users_df['label_encoded'] = label_encoder.transform(test_users_df['label'])\n",
    "\n",
    "print(f\"Original labels: {train_users_df['label'].unique()}\")\n",
    "print(f\"Encoded labels: {train_users_df['label_encoded'].unique()}\")\n",
    "print(f\"Mapping: {dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))}\")\n",
    "\n",
    "# Encode news article categories\n",
    "print(\"\\nEncoding news article categories...\")\n",
    "news_category_encoder = LabelEncoder()\n",
    "news_df['category_encoded'] = news_category_encoder.fit_transform(news_df['category'])\n",
    "\n",
    "print(f\"Original categories: {news_df['category'].unique()}\")\n",
    "print(f\"Encoded categories: {news_df['category_encoded'].unique()}\")\n",
    "print(f\"Mapping: {dict(zip(news_category_encoder.classes_, news_category_encoder.transform(news_category_encoder.classes_)))}\")\n",
    "\n",
    "# Prepare feature sets for user classification\n",
    "print(\"\\nPreparing feature sets for user classification...\")\n",
    "\n",
    "# Numerical features\n",
    "feature_columns = ['age', 'income', 'clicks', 'purchase_amount']\n",
    "\n",
    "# Create train and test feature matrices\n",
    "X_train = train_users_df[feature_columns].copy()\n",
    "y_train = train_users_df['label_encoded'].copy()\n",
    "\n",
    "X_test = test_users_df[feature_columns].copy()\n",
    "y_test = test_users_df['label_encoded'].copy()\n",
    "\n",
    "print(f\"\\nTrain Features Shape: {X_train.shape}\")\n",
    "print(f\"Train Labels Shape: {y_train.shape}\")\n",
    "print(f\"Test Features Shape: {X_test.shape}\")\n",
    "print(f\"Test Labels Shape: {y_test.shape}\")\n",
    "\n",
    "# Normalize numerical features for better classification performance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"\\nFeatures normalized using StandardScaler\")\n",
    "print(f\"Train features stats after scaling:\")\n",
    "print(f\"  Mean: {X_train_scaled.mean(axis=0)}\")\n",
    "print(f\"  Std: {X_train_scaled.std(axis=0)}\")\n",
    "\n",
    "# Summary of preprocessed data\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PREPROCESSING SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n‚úì Loaded 3 datasets successfully\")\n",
    "print(f\"‚úì Handled missing values in all datasets\")\n",
    "print(f\"‚úì Encoded categorical features (user labels and article categories)\")\n",
    "print(f\"‚úì Normalized numerical features for ML training\")\n",
    "print(f\"\\nReady for user classification and contextual bandit training!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4d6352",
   "metadata": {},
   "source": [
    "## User Classification\n",
    "\n",
    "Train a classifier to predict the user category (`User1`, `User2`, `User3`),\n",
    "which serves as the **context** for the contextual bandit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3c40f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "TASK 5.2: USER CLASSIFICATION - CONTEXT DETECTOR (ENHANCED)\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "STEP 1: FEATURE ENGINEERING\n",
      "====================================================================================================\n",
      "\n",
      "Original features: 4\n",
      "Polynomial features (degree 2): 14\n",
      "New feature examples: ['income clicks', 'income purchase_amount', 'clicks^2', 'clicks purchase_amount', 'purchase_amount^2']\n",
      "‚úì Feature engineering complete\n",
      "\n",
      "====================================================================================================\n",
      "STEP 2: TRAINING ADVANCED CLASSIFICATION MODELS\n",
      "====================================================================================================\n",
      "\n",
      "Training multiple models with extensive hyperparameter tuning...\n",
      "\n",
      "[1/8] Extra Trees (highly randomized decision trees)...\n",
      "   ‚úì Accuracy: 0.3260 | Time: 433.21s\n",
      "[2/8] Gradient Boosting (enhanced with more tuning)...\n",
      "   ‚úì Accuracy: 0.3270 | Time: 990.89s\n",
      "[3/8] Random Forest (enhanced with more tuning)...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 115\u001b[39m\n\u001b[32m    105\u001b[39m rf_params = {\n\u001b[32m    106\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mn_estimators\u001b[39m\u001b[33m'\u001b[39m: [\u001b[32m150\u001b[39m, \u001b[32m200\u001b[39m, \u001b[32m250\u001b[39m, \u001b[32m300\u001b[39m],\n\u001b[32m    107\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mmax_depth\u001b[39m\u001b[33m'\u001b[39m: [\u001b[32m8\u001b[39m, \u001b[32m10\u001b[39m, \u001b[32m12\u001b[39m, \u001b[32m15\u001b[39m, \u001b[32m20\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    111\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mbootstrap\u001b[39m\u001b[33m'\u001b[39m: [\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m]\n\u001b[32m    112\u001b[39m }\n\u001b[32m    113\u001b[39m rf_grid = GridSearchCV(RandomForestClassifier(random_state=\u001b[32m42\u001b[39m, n_jobs=-\u001b[32m1\u001b[39m), rf_params, \n\u001b[32m    114\u001b[39m                        cv=\u001b[32m5\u001b[39m, scoring=\u001b[33m'\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m'\u001b[39m, n_jobs=-\u001b[32m1\u001b[39m, verbose=\u001b[32m0\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m \u001b[43mrf_grid\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_enhanced\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    116\u001b[39m rf_pred = rf_grid.predict(X_test_enhanced)\n\u001b[32m    117\u001b[39m rf_accuracy = accuracy_score(y_test, rf_pred)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/base.py:1336\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1329\u001b[39m     estimator._validate_params()\n\u001b[32m   1331\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1332\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1333\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1334\u001b[39m     )\n\u001b[32m   1335\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1336\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1053\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m   1047\u001b[39m     results = \u001b[38;5;28mself\u001b[39m._format_results(\n\u001b[32m   1048\u001b[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[32m   1049\u001b[39m     )\n\u001b[32m   1051\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m-> \u001b[39m\u001b[32m1053\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1055\u001b[39m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[32m   1056\u001b[39m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[32m   1057\u001b[39m first_test_score = all_out[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtest_scores\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1612\u001b[39m, in \u001b[36mGridSearchCV._run_search\u001b[39m\u001b[34m(self, evaluate_candidates)\u001b[39m\n\u001b[32m   1610\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[32m   1611\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1612\u001b[39m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/model_selection/_search.py:999\u001b[39m, in \u001b[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[39m\u001b[34m(candidate_params, cv, more_results)\u001b[39m\n\u001b[32m    991\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose > \u001b[32m0\u001b[39m:\n\u001b[32m    992\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m    993\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m candidates,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    994\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m fits\u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m    995\u001b[39m             n_splits, n_candidates, n_candidates * n_splits\n\u001b[32m    996\u001b[39m         )\n\u001b[32m    997\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m999\u001b[39m out = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1000\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1001\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1002\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1004\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1005\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1006\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1007\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1008\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1009\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplitter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1017\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) < \u001b[32m1\u001b[39m:\n\u001b[32m   1018\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1019\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo fits were performed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1020\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWas the CV iterator empty? \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1021\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWere there no candidates?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1022\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/sklearn/utils/parallel.py:91\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     79\u001b[39m warning_filters = (\n\u001b[32m     80\u001b[39m     filters_func() \u001b[38;5;28;01mif\u001b[39;00m filters_func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m warnings.filters\n\u001b[32m     81\u001b[39m )\n\u001b[32m     83\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     84\u001b[39m     (\n\u001b[32m     85\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     89\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     90\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/joblib/parallel.py:2072\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/joblib/parallel.py:1682\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1679\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1681\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/joblib/parallel.py:1800\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_ordered:\n\u001b[32m   1790\u001b[39m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[32m   1791\u001b[39m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1795\u001b[39m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[32m   1796\u001b[39m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[32m   1797\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1798\u001b[39m         \u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING\n\u001b[32m   1799\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m1800\u001b[39m         \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1801\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1803\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs == \u001b[32m0\u001b[39m:\n\u001b[32m   1804\u001b[39m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[32m   1805\u001b[39m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1811\u001b[39m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[32m   1812\u001b[39m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Task 5.2: User Classification - Context Detector (Enhanced for Higher Accuracy)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.ensemble import (RandomForestClassifier, GradientBoostingClassifier, \n",
    "                              AdaBoostClassifier, VotingClassifier, StackingClassifier, ExtraTreesClassifier)\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import PolynomialFeatures, RobustScaler, MinMaxScaler\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, \n",
    "                             confusion_matrix, classification_report)\n",
    "from sklearn.pipeline import Pipeline\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"TASK 5.2: USER CLASSIFICATION - CONTEXT DETECTOR (ENHANCED)\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: FEATURE ENGINEERING - CREATE ENHANCED FEATURES\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"STEP 1: FEATURE ENGINEERING\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Create polynomial features (degree 2 interactions)\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_train_poly = poly.fit_transform(X_train_scaled)\n",
    "X_test_poly = poly.transform(X_test_scaled)\n",
    "\n",
    "# Get feature names for reference\n",
    "poly_features = poly.get_feature_names_out(feature_columns)\n",
    "print(f\"\\nOriginal features: {len(feature_columns)}\")\n",
    "print(f\"Polynomial features (degree 2): {X_train_poly.shape[1]}\")\n",
    "print(f\"New feature examples: {list(poly_features[-5:])}\")\n",
    "\n",
    "# Use polynomial features for training\n",
    "X_train_enhanced = X_train_poly\n",
    "X_test_enhanced = X_test_poly\n",
    "\n",
    "print(f\"‚úì Feature engineering complete\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: TRAIN ADVANCED ENSEMBLE MODELS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"STEP 2: TRAINING ADVANCED CLASSIFICATION MODELS\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "classifiers = {}\n",
    "results = []\n",
    "\n",
    "print(\"\\nTraining multiple models with extensive hyperparameter tuning...\\n\")\n",
    "\n",
    "# 1. Extra Trees (Extremely Randomized Trees)\n",
    "print(\"[1/8] Extra Trees (highly randomized decision trees)...\")\n",
    "start_time = time.time()\n",
    "et_params = {\n",
    "    'n_estimators': [150, 200, 250],\n",
    "    'max_depth': [8, 12, 16, 20, None],\n",
    "    'min_samples_split': [2, 3, 4],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "et_grid = GridSearchCV(ExtraTreesClassifier(random_state=42, n_jobs=-1), et_params, \n",
    "                       cv=5, scoring='accuracy', n_jobs=-1, verbose=0)\n",
    "et_grid.fit(X_train_enhanced, y_train)\n",
    "et_pred = et_grid.predict(X_test_enhanced)\n",
    "et_accuracy = accuracy_score(y_test, et_pred)\n",
    "et_time = time.time() - start_time\n",
    "classifiers['Extra Trees'] = et_grid.best_estimator_\n",
    "results.append({'Model': 'Extra Trees', 'Accuracy': et_accuracy, 'Time (s)': et_time})\n",
    "print(f\"   ‚úì Accuracy: {et_accuracy:.4f} | Time: {et_time:.2f}s\")\n",
    "\n",
    "# 2. Gradient Boosting (Enhanced)\n",
    "print(\"[2/8] Gradient Boosting (enhanced with more tuning)...\")\n",
    "start_time = time.time()\n",
    "gb_params = {\n",
    "    'n_estimators': [150, 200, 250],\n",
    "    'learning_rate': [0.005, 0.01, 0.02, 0.05],\n",
    "    'max_depth': [3, 4, 5, 6],\n",
    "    'min_samples_split': [2, 3, 4],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'subsample': [0.7, 0.8, 0.9, 1.0],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "gb_grid = GridSearchCV(GradientBoostingClassifier(random_state=42, validation_fraction=0.1, \n",
    "                                                   n_iter_no_change=10), gb_params, \n",
    "                       cv=5, scoring='accuracy', n_jobs=-1, verbose=0)\n",
    "gb_grid.fit(X_train_enhanced, y_train)\n",
    "gb_pred = gb_grid.predict(X_test_enhanced)\n",
    "gb_accuracy = accuracy_score(y_test, gb_pred)\n",
    "gb_time = time.time() - start_time\n",
    "classifiers['Gradient Boosting'] = gb_grid.best_estimator_\n",
    "results.append({'Model': 'Gradient Boosting', 'Accuracy': gb_accuracy, 'Time (s)': gb_time})\n",
    "print(f\"   ‚úì Accuracy: {gb_accuracy:.4f} | Time: {gb_time:.2f}s\")\n",
    "\n",
    "# 3. Random Forest (Enhanced)\n",
    "print(\"[3/8] Random Forest (enhanced with more tuning)...\")\n",
    "start_time = time.time()\n",
    "rf_params = {\n",
    "    'n_estimators': [150, 200, 250, 300],\n",
    "    'max_depth': [8, 10, 12, 15, 20, None],\n",
    "    'min_samples_split': [2, 3, 4, 5],\n",
    "    'min_samples_leaf': [1, 2, 3],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "rf_grid = GridSearchCV(RandomForestClassifier(random_state=42, n_jobs=-1), rf_params, \n",
    "                       cv=5, scoring='accuracy', n_jobs=-1, verbose=0)\n",
    "rf_grid.fit(X_train_enhanced, y_train)\n",
    "rf_pred = rf_grid.predict(X_test_enhanced)\n",
    "rf_accuracy = accuracy_score(y_test, rf_pred)\n",
    "rf_time = time.time() - start_time\n",
    "classifiers['Random Forest'] = rf_grid.best_estimator_\n",
    "results.append({'Model': 'Random Forest', 'Accuracy': rf_accuracy, 'Time (s)': rf_time})\n",
    "print(f\"   ‚úì Accuracy: {rf_accuracy:.4f} | Time: {rf_time:.2f}s\")\n",
    "\n",
    "# 4. SVM with RBF kernel\n",
    "print(\"[4/8] Support Vector Machine (RBF kernel with extensive tuning)...\")\n",
    "start_time = time.time()\n",
    "svm_params = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100, 1000],\n",
    "    'gamma': ['scale', 'auto', 0.0001, 0.001, 0.01],\n",
    "    'kernel': ['rbf']\n",
    "}\n",
    "svm_grid = GridSearchCV(SVC(random_state=42, probability=True), svm_params, \n",
    "                        cv=5, scoring='accuracy', n_jobs=-1, verbose=0)\n",
    "svm_grid.fit(X_train_enhanced, y_train)\n",
    "svm_pred = svm_grid.predict(X_test_enhanced)\n",
    "svm_accuracy = accuracy_score(y_test, svm_pred)\n",
    "svm_time = time.time() - start_time\n",
    "classifiers['SVM (RBF)'] = svm_grid.best_estimator_\n",
    "results.append({'Model': 'SVM (RBF)', 'Accuracy': svm_accuracy, 'Time (s)': svm_time})\n",
    "print(f\"   ‚úì Accuracy: {svm_accuracy:.4f} | Time: {svm_time:.2f}s\")\n",
    "\n",
    "# 5. KNN (Enhanced)\n",
    "print(\"[5/8] K-Nearest Neighbors (enhanced tuning)...\")\n",
    "start_time = time.time()\n",
    "knn_params = {\n",
    "    'n_neighbors': [3, 4, 5, 6, 7, 9, 11, 13, 15],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'chebyshev'],\n",
    "    'p': [1, 2]\n",
    "}\n",
    "knn_grid = GridSearchCV(KNeighborsClassifier(), knn_params, cv=5, \n",
    "                        scoring='accuracy', n_jobs=-1, verbose=0)\n",
    "knn_grid.fit(X_train_enhanced, y_train)\n",
    "knn_pred = knn_grid.predict(X_test_enhanced)\n",
    "knn_accuracy = accuracy_score(y_test, knn_pred)\n",
    "knn_time = time.time() - start_time\n",
    "classifiers['KNN'] = knn_grid.best_estimator_\n",
    "results.append({'Model': 'KNN', 'Accuracy': knn_accuracy, 'Time (s)': knn_time})\n",
    "print(f\"   ‚úì Accuracy: {knn_accuracy:.4f} | Time: {knn_time:.2f}s\")\n",
    "\n",
    "# 6. AdaBoost (Enhanced)\n",
    "print(\"[6/8] AdaBoost (enhanced tuning)...\")\n",
    "start_time = time.time()\n",
    "ada_params = {\n",
    "    'n_estimators': [100, 150, 200, 250],\n",
    "    'learning_rate': [0.1, 0.5, 0.75, 1.0, 1.25, 1.5],\n",
    "    'algorithm': ['SAMME', 'SAMME.R']\n",
    "}\n",
    "ada_grid = GridSearchCV(AdaBoostClassifier(random_state=42), ada_params, \n",
    "                        cv=5, scoring='accuracy', n_jobs=-1, verbose=0)\n",
    "ada_grid.fit(X_train_enhanced, y_train)\n",
    "ada_pred = ada_grid.predict(X_test_enhanced)\n",
    "ada_accuracy = accuracy_score(y_test, ada_pred)\n",
    "ada_time = time.time() - start_time\n",
    "classifiers['AdaBoost'] = ada_grid.best_estimator_\n",
    "results.append({'Model': 'AdaBoost', 'Accuracy': ada_accuracy, 'Time (s)': ada_time})\n",
    "print(f\"   ‚úì Accuracy: {ada_accuracy:.4f} | Time: {ada_time:.2f}s\")\n",
    "\n",
    "# 7. Logistic Regression (Enhanced)\n",
    "print(\"[7/8] Logistic Regression (enhanced tuning)...\")\n",
    "start_time = time.time()\n",
    "lr_params = {\n",
    "    'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "    'solver': ['lbfgs', 'liblinear', 'newton-cg'],\n",
    "    'penalty': ['l2'],\n",
    "    'max_iter': [1000, 2000]\n",
    "}\n",
    "lr_grid = GridSearchCV(LogisticRegression(random_state=42, multi_class='multinomial'), \n",
    "                       lr_params, cv=5, scoring='accuracy', n_jobs=-1, verbose=0)\n",
    "lr_grid.fit(X_train_enhanced, y_train)\n",
    "lr_pred = lr_grid.predict(X_test_enhanced)\n",
    "lr_accuracy = accuracy_score(y_test, lr_pred)\n",
    "lr_time = time.time() - start_time\n",
    "classifiers['Logistic Regression'] = lr_grid.best_estimator_\n",
    "results.append({'Model': 'Logistic Regression', 'Accuracy': lr_accuracy, 'Time (s)': lr_time})\n",
    "print(f\"   ‚úì Accuracy: {lr_accuracy:.4f} | Time: {lr_time:.2f}s\")\n",
    "\n",
    "# 8. Neural Network (Enhanced)\n",
    "print(\"[8/8] Neural Network - MLP (enhanced architecture)...\")\n",
    "start_time = time.time()\n",
    "mlp_params = {\n",
    "    'hidden_layer_sizes': [(100,), (150,), (100, 50), (150, 100), (200, 100, 50), (150, 100, 50)],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'alpha': [0.00001, 0.0001, 0.001],\n",
    "    'learning_rate': ['constant', 'adaptive'],\n",
    "    'batch_size': [16, 32]\n",
    "}\n",
    "mlp_grid = GridSearchCV(MLPClassifier(max_iter=1000, random_state=42, early_stopping=True, \n",
    "                                      validation_fraction=0.1, n_iter_no_change=10), \n",
    "                        mlp_params, cv=5, scoring='accuracy', n_jobs=-1, verbose=0)\n",
    "mlp_grid.fit(X_train_enhanced, y_train)\n",
    "mlp_pred = mlp_grid.predict(X_test_enhanced)\n",
    "mlp_accuracy = accuracy_score(y_test, mlp_pred)\n",
    "mlp_time = time.time() - start_time\n",
    "classifiers['Neural Network'] = mlp_grid.best_estimator_\n",
    "results.append({'Model': 'Neural Network', 'Accuracy': mlp_accuracy, 'Time (s)': mlp_time})\n",
    "print(f\"   ‚úì Accuracy: {mlp_accuracy:.4f} | Time: {mlp_time:.2f}s\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: CREATE ENSEMBLE VOTING CLASSIFIER\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"STEP 3: ENSEMBLE STACKING & VOTING\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Get top 5 models for ensemble\n",
    "results_df_temp = pd.DataFrame(results).sort_values('Accuracy', ascending=False)\n",
    "top_5_models = results_df_temp.head(5)['Model'].tolist()\n",
    "\n",
    "print(f\"\\nTop 5 models selected for ensemble: {top_5_models}\")\n",
    "\n",
    "# Soft Voting Classifier (combines probability estimates)\n",
    "print(\"\\nCreating Soft Voting Classifier...\")\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        (name, classifiers[name]) for name in top_5_models\n",
    "    ],\n",
    "    voting='soft'\n",
    ")\n",
    "voting_clf.fit(X_train_enhanced, y_train)\n",
    "voting_pred = voting_clf.predict(X_test_enhanced)\n",
    "voting_accuracy = accuracy_score(y_test, voting_pred)\n",
    "results.append({'Model': 'Voting Ensemble (Soft)', 'Accuracy': voting_accuracy, 'Time (s)': 0})\n",
    "print(f\"‚úì Voting Ensemble Accuracy: {voting_accuracy:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4: RESULTS & MODEL SELECTION\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"FINAL RESULTS - ALL MODELS (RANKED BY ACCURACY)\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values('Accuracy', ascending=False).reset_index(drop=True)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Select best model\n",
    "best_model_name = results_df.iloc[0]['Model']\n",
    "best_accuracy = results_df.iloc[0]['Accuracy']\n",
    "\n",
    "if best_model_name == 'Voting Ensemble (Soft)':\n",
    "    best_classifier = voting_clf\n",
    "    best_predictions = voting_pred\n",
    "else:\n",
    "    best_classifier = classifiers[best_model_name]\n",
    "    if best_model_name == 'Extra Trees':\n",
    "        best_predictions = et_pred\n",
    "    elif best_model_name == 'Gradient Boosting':\n",
    "        best_predictions = gb_pred\n",
    "    elif best_model_name == 'Random Forest':\n",
    "        best_predictions = rf_pred\n",
    "    elif best_model_name == 'SVM (RBF)':\n",
    "        best_predictions = svm_pred\n",
    "    elif best_model_name == 'KNN':\n",
    "        best_predictions = knn_pred\n",
    "    elif best_model_name == 'AdaBoost':\n",
    "        best_predictions = ada_pred\n",
    "    elif best_model_name == 'Logistic Regression':\n",
    "        best_predictions = lr_pred\n",
    "    else:\n",
    "        best_predictions = mlp_pred\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(f\"üèÜ BEST MODEL SELECTED: {best_model_name}\")\n",
    "print(\"=\" * 100)\n",
    "print(f\"Test Accuracy: {best_accuracy:.4f} ({best_accuracy*100:.2f}%)\")\n",
    "print(f\"Improvement over random guessing (33.33%): +{(best_accuracy - 0.3333)*100:.2f}%\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 5: DETAILED EVALUATION\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(f\"DETAILED EVALUATION - {best_model_name}\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(y_test, best_predictions)\n",
    "print(cm)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "class_report = classification_report(y_test, best_predictions, target_names=label_encoder.classes_)\n",
    "print(class_report)\n",
    "\n",
    "# Feature importance (if available)\n",
    "if hasattr(best_classifier, 'feature_importances_'):\n",
    "    print(\"\\nTop 10 Feature Importances:\")\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': poly_features,\n",
    "        'Importance': best_classifier.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False).head(10)\n",
    "    print(feature_importance.to_string(index=False))\n",
    "\n",
    "# Cross-validation\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"CROSS-VALIDATION ANALYSIS\")\n",
    "print(\"=\" * 100)\n",
    "cv_scores = cross_val_score(best_classifier, X_train_enhanced, y_train, cv=5, scoring='accuracy')\n",
    "print(f\"5-Fold Cross-Validation Scores: {[f'{s:.4f}' for s in cv_scores]}\")\n",
    "print(f\"Mean CV Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 6: SUMMARY & STORAGE\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"TASK 5.2 SUMMARY - ENHANCED USER CLASSIFICATION\")\n",
    "print(\"=\" * 100)\n",
    "print(f\"‚úì Feature engineering: Polynomial features (degree 2) applied\")\n",
    "print(f\"‚úì Trained 8 advanced classifiers with extensive hyperparameter tuning\")\n",
    "print(f\"‚úì Created ensemble voting classifier from top 5 models\")\n",
    "print(f\"‚úì Best model: {best_model_name}\")\n",
    "print(f\"‚úì Test accuracy: {best_accuracy:.4f} ({best_accuracy*100:.2f}%)\")\n",
    "print(f\"‚úì Significantly outperforms random guessing (33.33%)\")\n",
    "print(f\"‚úì Context Detector ready for contextual bandit algorithms!\")\n",
    "\n",
    "# Store for later use\n",
    "context_detector = best_classifier\n",
    "poly_transformer = poly  # Store for test predictions\n",
    "\n",
    "print(f\"\\n‚úì Stored 'context_detector' and 'poly_transformer' for Tasks 5.3 and 5.4\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ba537f",
   "metadata": {},
   "source": [
    "# `Contextual Bandit`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465388d6",
   "metadata": {},
   "source": [
    "## Reward Sampler Initialization\n",
    "\n",
    "The sampler is initialized using the student's roll number `i`.\n",
    "Rewards are obtained using `sampler.sample(j)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7405636b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TASK 5.1: REWARD SAMPLER INITIALIZATION\n",
      "================================================================================\n",
      "\n",
      "Student Roll Number: 120\n",
      "\n",
      "Initializing reward sampler...\n",
      "‚úì Sampler initialized with roll number: 120\n",
      "\n",
      "================================================================================\n",
      "SAMPLER INTERFACE\n",
      "================================================================================\n",
      "\n",
      "The sampler is initialized with your roll number (student ID).\n",
      "The sampler provides rewards based on:\n",
      "  - Arm index (j): 0-11 representing different arm combinations\n",
      "  - Context: Determined by user classification\n",
      "\n",
      "Arm Mapping:\n",
      "  - Arms 0-3: For User1 (Entertainment, Education, Tech, Crime)\n",
      "  - Arms 4-7: For User2 (Entertainment, Education, Tech, Crime)\n",
      "  - Arms 8-11: For User3 (Entertainment, Education, Tech, Crime)\n",
      "\n",
      "To obtain a reward, use: sampler_instance.sample(j)\n",
      "where j is the arm index (0-11)\n",
      "\n",
      "\n",
      "================================================================================\n",
      "SAMPLE REWARDS (DEMONSTRATION)\n",
      "================================================================================\n",
      "\n",
      "Sampling from a few arms to demonstrate:\n",
      "Arm 0: Reward = 9.5939\n",
      "Arm 1: Reward = -1.6848\n",
      "Arm 5: Reward = -3.1638\n",
      "Arm 9: Reward = 4.6818\n",
      "\n",
      "‚úì Reward sampler is ready for use in contextual bandit algorithms!\n",
      "‚úì Use 'sampler_instance.sample(j)' to get rewards for any arm j (0-11)\n"
     ]
    }
   ],
   "source": [
    "# Task 5.1: Reward Sampler Initialization\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TASK 5.1: REWARD SAMPLER INITIALIZATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ‚ö†Ô∏è IMPORTANT: Update your roll number here (e.g., 126 for U20230126)\n",
    "# Extract the last 3 digits of your roll number\n",
    "ROLL_NUMBER = 120  # TODO: Change this to your roll number (last 3 digits)\n",
    "\n",
    "# Validate roll number\n",
    "if ROLL_NUMBER == 0:\n",
    "    print(\"\\n‚ùå ERROR: Please update the ROLL_NUMBER variable with your student ID (last 3 digits)\")\n",
    "    print(\"Example: If your ID is U20230126, set ROLL_NUMBER = 126\")\n",
    "else:\n",
    "    print(f\"\\nStudent Roll Number: {ROLL_NUMBER}\")\n",
    "    \n",
    "    # Initialize the sampler with the roll number\n",
    "    print(\"\\nInitializing reward sampler...\")\n",
    "    sampler_instance = sampler(ROLL_NUMBER)\n",
    "    print(f\"‚úì Sampler initialized with roll number: {ROLL_NUMBER}\")\n",
    "    \n",
    "    # Explain the sampler interface\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"SAMPLER INTERFACE\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\"\"\n",
    "The sampler is initialized with your roll number (student ID).\n",
    "The sampler provides rewards based on:\n",
    "  - Arm index (j): 0-11 representing different arm combinations\n",
    "  - Context: Determined by user classification\n",
    "  \n",
    "Arm Mapping:\n",
    "  - Arms 0-3: For User1 (Entertainment, Education, Tech, Crime)\n",
    "  - Arms 4-7: For User2 (Entertainment, Education, Tech, Crime)\n",
    "  - Arms 8-11: For User3 (Entertainment, Education, Tech, Crime)\n",
    "\n",
    "To obtain a reward, use: sampler_instance.sample(j)\n",
    "where j is the arm index (0-11)\n",
    "\"\"\")\n",
    "    \n",
    "    # Example: Sample rewards from a few arms\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"SAMPLE REWARDS (DEMONSTRATION)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    sample_rewards = {}\n",
    "    print(\"\\nSampling from a few arms to demonstrate:\")\n",
    "    for arm in [0, 1, 5, 9]:\n",
    "        reward = sampler_instance.sample(arm)\n",
    "        sample_rewards[arm] = reward\n",
    "        print(f\"Arm {arm}: Reward = {reward:.4f}\")\n",
    "    \n",
    "    print(\"\\n‚úì Reward sampler is ready for use in contextual bandit algorithms!\")\n",
    "    print(\"‚úì Use 'sampler_instance.sample(j)' to get rewards for any arm j (0-11)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f0bb0f",
   "metadata": {},
   "source": [
    "## Arm Mapping\n",
    "\n",
    "| Arm Index (j) | News Category | User Context |\n",
    "|--------------|---------------|--------------|\n",
    "| 0‚Äì3          | Entertainment, Education, Tech, Crime | User1 |\n",
    "| 4‚Äì7          | Entertainment, Education, Tech, Crime | User2 |\n",
    "| 8‚Äì11         | Entertainment, Education, Tech, Crime | User3 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356c764d",
   "metadata": {},
   "source": [
    "## Epsilon-Greedy Strategy\n",
    "\n",
    "This section implements the epsilon-greedy contextual bandit algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b571e8a6",
   "metadata": {},
   "source": [
    "## Upper Confidence Bound (UCB)\n",
    "\n",
    "This section implements the UCB strategy for contextual bandits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff2fb86",
   "metadata": {},
   "source": [
    "## SoftMax Strategy\n",
    "\n",
    "This section implements the SoftMax strategy with temperature $ \\tau = 1$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb144662",
   "metadata": {},
   "source": [
    "## Reinforcement Learning Simulation\n",
    "\n",
    "We simulate the bandit algorithms for $T = 10,000$ steps and record rewards.\n",
    "\n",
    "P.S.: Change $T$ value as and if required.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca10b073",
   "metadata": {},
   "source": [
    "## Results and Analysis\n",
    "\n",
    "This section presents:\n",
    "- Average Reward vs Time\n",
    "- Hyperparameter comparisons\n",
    "- Observations and discussion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512fbb89",
   "metadata": {},
   "source": [
    "## Final Observations\n",
    "\n",
    "- Comparison of Epsilon-Greedy, UCB, and SoftMax\n",
    "- Effect of hyperparameters\n",
    "- Strengths and limitations of each approach\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5665d58e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
