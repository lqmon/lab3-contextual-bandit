{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "999bcd7d",
   "metadata": {},
   "source": [
    "# Lab 3: Contextual Bandit-Based News Article Recommendation\n",
    "\n",
    "**`Course`:** Reinforcement Learning Fundamentals  \n",
    "**`Student Name`:**  \n",
    "**`Roll Number`:**  \n",
    "**`GitHub Branch`:** firstname_U20230xxx  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755efd7a",
   "metadata": {},
   "source": [
    "# Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef4bd959",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from rlcmab_sampler import sampler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c638ba06",
   "metadata": {},
   "source": [
    "# Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48f6f6a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                link  \\\n",
      "0  https://www.huffpost.com/entry/covid-boosters-...   \n",
      "1  https://www.huffpost.com/entry/american-airlin...   \n",
      "2  https://www.huffpost.com/entry/funniest-tweets...   \n",
      "3  https://www.huffpost.com/entry/funniest-parent...   \n",
      "4  https://www.huffpost.com/entry/amy-cooper-lose...   \n",
      "\n",
      "                                            headline   category  \\\n",
      "0  Over 4 Million Americans Roll Up Sleeves For O...  U.S. NEWS   \n",
      "1  American Airlines Flyer Charged, Banned For Li...  U.S. NEWS   \n",
      "2  23 Of The Funniest Tweets About Cats And Dogs ...     COMEDY   \n",
      "3  The Funniest Tweets From Parents This Week (Se...  PARENTING   \n",
      "4  Woman Who Called Cops On Black Bird-Watcher Lo...  U.S. NEWS   \n",
      "\n",
      "                                   short_description               authors  \\\n",
      "0  Health experts said it is too early to predict...  Carla K. Johnson, AP   \n",
      "1  He was subdued by passengers and crew when he ...        Mary Papenfuss   \n",
      "2  \"Until you have a dog you don't understand wha...         Elyse Wanshel   \n",
      "3  \"Accidentally put grown-up toothpaste on my to...      Caroline Bologna   \n",
      "4  Amy Cooper accused investment firm Franklin Te...        Nina Golgowski   \n",
      "\n",
      "         date  \n",
      "0  2022-09-23  \n",
      "1  2022-09-23  \n",
      "2  2022-09-23  \n",
      "3  2022-09-23  \n",
      "4  2022-09-22  \n",
      "   user_id  age  income  clicks  purchase_amount  label\n",
      "0        1   28   58242      81           378.38  user3\n",
      "1        2   28   38225      21           114.50  user3\n",
      "2        3   39   95017      41            66.24  user2\n",
      "3        4   52   33473      98           496.88  user3\n",
      "4        5   29   80690       5           293.24  user1\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "news_df = pd.read_csv(\"data/news_articles.csv\")\n",
    "train_users = pd.read_csv(\"data/train_users.csv\")\n",
    "test_users = pd.read_csv(\"data/test_users.csv\")\n",
    "\n",
    "print(news_df.head())\n",
    "print(train_users.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1c5cb1",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "In this section:\n",
    "- Handle missing values\n",
    "- Encode categorical features\n",
    "- Prepare data for user classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7195ba1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "1. LOADING DATASETS\n",
      "================================================================================\n",
      "\n",
      "News Articles Dataset Shape: (209527, 6)\n",
      "Train Users Dataset Shape: (2000, 6)\n",
      "Test Users Dataset Shape: (2000, 6)\n",
      "\n",
      "News Articles Columns: ['link', 'headline', 'category', 'short_description', 'authors', 'date']\n",
      "Train Users Columns: ['user_id', 'age', 'income', 'clicks', 'purchase_amount', 'label']\n",
      "Test Users Columns: ['user_id', 'age', 'income', 'clicks', 'purchase_amount', 'label']\n",
      "\n",
      "================================================================================\n",
      "2. DATA CLEANING - HANDLING MISSING VALUES\n",
      "================================================================================\n",
      "\n",
      "Missing values in News Articles:\n",
      "link                     0\n",
      "headline                 6\n",
      "category                 0\n",
      "short_description    19712\n",
      "authors              37418\n",
      "date                     0\n",
      "dtype: int64\n",
      "Total missing: 57136\n",
      "\n",
      "Missing values in Train Users:\n",
      "user_id            0\n",
      "age                0\n",
      "income             0\n",
      "clicks             0\n",
      "purchase_amount    0\n",
      "label              0\n",
      "dtype: int64\n",
      "Total missing: 0\n",
      "\n",
      "Missing values in Test Users:\n",
      "user_id            0\n",
      "age                0\n",
      "income             0\n",
      "clicks             0\n",
      "purchase_amount    0\n",
      "label              0\n",
      "dtype: int64\n",
      "Total missing: 0\n",
      "\n",
      "Missing values after cleaning:\n",
      "News Articles: 6\n",
      "Train Users: 0\n",
      "Test Users: 0\n",
      "\n",
      "================================================================================\n",
      "3. FEATURE ENCODING FOR CLASSIFICATION AND BANDIT TRAINING\n",
      "================================================================================\n",
      "\n",
      "Encoding user labels...\n",
      "Original labels: <StringArray>\n",
      "['user3', 'user2', 'user1']\n",
      "Length: 3, dtype: str\n",
      "Encoded labels: [2 1 0]\n",
      "Mapping: {'user1': np.int64(0), 'user2': np.int64(1), 'user3': np.int64(2)}\n",
      "\n",
      "Encoding news article categories...\n",
      "Original categories: <StringArray>\n",
      "[     'U.S. NEWS',         'COMEDY',      'PARENTING',     'WORLD NEWS',\n",
      " 'CULTURE & ARTS',           'TECH',         'SPORTS',  'ENTERTAINMENT',\n",
      "       'POLITICS',     'WEIRD NEWS',    'ENVIRONMENT',      'EDUCATION',\n",
      "          'CRIME',        'SCIENCE',       'WELLNESS',       'BUSINESS',\n",
      " 'STYLE & BEAUTY',   'FOOD & DRINK',          'MEDIA',   'QUEER VOICES',\n",
      "  'HOME & LIVING',          'WOMEN',   'BLACK VOICES',         'TRAVEL',\n",
      "          'MONEY',       'RELIGION',  'LATINO VOICES',         'IMPACT',\n",
      "       'WEDDINGS',        'COLLEGE',        'PARENTS', 'ARTS & CULTURE',\n",
      "          'STYLE',          'GREEN',          'TASTE', 'HEALTHY LIVING',\n",
      "  'THE WORLDPOST',      'GOOD NEWS',      'WORLDPOST',          'FIFTY',\n",
      "           'ARTS',        'DIVORCE']\n",
      "Length: 42, dtype: str\n",
      "Encoded categories: [35  5 22 40  7 32 28 10 24 37 11  9  6 27 38  3 30 13 20 25 17 39  2 34\n",
      " 21 26 19 18 36  4 23  1 29 15 31 16 33 14 41 12  0  8]\n",
      "Mapping: {'ARTS': np.int64(0), 'ARTS & CULTURE': np.int64(1), 'BLACK VOICES': np.int64(2), 'BUSINESS': np.int64(3), 'COLLEGE': np.int64(4), 'COMEDY': np.int64(5), 'CRIME': np.int64(6), 'CULTURE & ARTS': np.int64(7), 'DIVORCE': np.int64(8), 'EDUCATION': np.int64(9), 'ENTERTAINMENT': np.int64(10), 'ENVIRONMENT': np.int64(11), 'FIFTY': np.int64(12), 'FOOD & DRINK': np.int64(13), 'GOOD NEWS': np.int64(14), 'GREEN': np.int64(15), 'HEALTHY LIVING': np.int64(16), 'HOME & LIVING': np.int64(17), 'IMPACT': np.int64(18), 'LATINO VOICES': np.int64(19), 'MEDIA': np.int64(20), 'MONEY': np.int64(21), 'PARENTING': np.int64(22), 'PARENTS': np.int64(23), 'POLITICS': np.int64(24), 'QUEER VOICES': np.int64(25), 'RELIGION': np.int64(26), 'SCIENCE': np.int64(27), 'SPORTS': np.int64(28), 'STYLE': np.int64(29), 'STYLE & BEAUTY': np.int64(30), 'TASTE': np.int64(31), 'TECH': np.int64(32), 'THE WORLDPOST': np.int64(33), 'TRAVEL': np.int64(34), 'U.S. NEWS': np.int64(35), 'WEDDINGS': np.int64(36), 'WEIRD NEWS': np.int64(37), 'WELLNESS': np.int64(38), 'WOMEN': np.int64(39), 'WORLD NEWS': np.int64(40), 'WORLDPOST': np.int64(41)}\n",
      "\n",
      "Preparing feature sets for user classification...\n",
      "\n",
      "Train Features Shape: (2000, 4)\n",
      "Train Labels Shape: (2000,)\n",
      "Test Features Shape: (2000, 4)\n",
      "Test Labels Shape: (2000,)\n",
      "\n",
      "Features normalized using StandardScaler\n",
      "Train features stats after scaling:\n",
      "  Mean: [ 9.59232693e-17  4.79616347e-17 -3.10862447e-17  9.59232693e-17]\n",
      "  Std: [1. 1. 1. 1.]\n",
      "\n",
      "================================================================================\n",
      "PREPROCESSING SUMMARY\n",
      "================================================================================\n",
      "\n",
      "‚úì Loaded 3 datasets successfully\n",
      "‚úì Handled missing values in all datasets\n",
      "‚úì Encoded categorical features (user labels and article categories)\n",
      "‚úì Normalized numerical features for ML training\n",
      "\n",
      "Ready for user classification and contextual bandit training!\n"
     ]
    }
   ],
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "# 1. Load the provided user and article datasets\n",
    "print(\"=\" * 80)\n",
    "print(\"1. LOADING DATASETS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "news_df = pd.read_csv(\"data/news_articles.csv\")\n",
    "train_users_df = pd.read_csv(\"data/train_users.csv\")\n",
    "test_users_df = pd.read_csv(\"data/test_users.csv\")\n",
    "\n",
    "print(f\"\\nNews Articles Dataset Shape: {news_df.shape}\")\n",
    "print(f\"Train Users Dataset Shape: {train_users_df.shape}\")\n",
    "print(f\"Test Users Dataset Shape: {test_users_df.shape}\")\n",
    "\n",
    "print(\"\\nNews Articles Columns:\", news_df.columns.tolist())\n",
    "print(\"Train Users Columns:\", train_users_df.columns.tolist())\n",
    "print(\"Test Users Columns:\", test_users_df.columns.tolist())\n",
    "\n",
    "# 2. Data Cleaning - Handle Missing Values\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"2. DATA CLEANING - HANDLING MISSING VALUES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check for missing values in each dataset\n",
    "print(\"\\nMissing values in News Articles:\")\n",
    "print(news_df.isnull().sum())\n",
    "print(f\"Total missing: {news_df.isnull().sum().sum()}\")\n",
    "\n",
    "print(\"\\nMissing values in Train Users:\")\n",
    "print(train_users_df.isnull().sum())\n",
    "print(f\"Total missing: {train_users_df.isnull().sum().sum()}\")\n",
    "\n",
    "print(\"\\nMissing values in Test Users:\")\n",
    "print(test_users_df.isnull().sum())\n",
    "print(f\"Total missing: {test_users_df.isnull().sum().sum()}\")\n",
    "\n",
    "# Handle missing values in news articles\n",
    "# Fill missing authors with 'Unknown'\n",
    "news_df['authors'] = news_df['authors'].fillna('Unknown')\n",
    "\n",
    "# Fill missing short_description with empty string\n",
    "news_df['short_description'] = news_df['short_description'].fillna('')\n",
    "\n",
    "# Fill missing date with mode (most frequent date)\n",
    "if news_df['date'].isnull().sum() > 0:\n",
    "    news_df['date'] = news_df['date'].fillna(news_df['date'].mode()[0] if not news_df['date'].mode().empty else 'Unknown')\n",
    "\n",
    "# Handle missing values in user datasets (if any)\n",
    "# Fill numerical columns with median\n",
    "train_users_df['age'] = train_users_df['age'].fillna(train_users_df['age'].median())\n",
    "train_users_df['income'] = train_users_df['income'].fillna(train_users_df['income'].median())\n",
    "train_users_df['clicks'] = train_users_df['clicks'].fillna(train_users_df['clicks'].median())\n",
    "train_users_df['purchase_amount'] = train_users_df['purchase_amount'].fillna(train_users_df['purchase_amount'].median())\n",
    "\n",
    "test_users_df['age'] = test_users_df['age'].fillna(test_users_df['age'].median())\n",
    "test_users_df['income'] = test_users_df['income'].fillna(test_users_df['income'].median())\n",
    "test_users_df['clicks'] = test_users_df['clicks'].fillna(test_users_df['clicks'].median())\n",
    "test_users_df['purchase_amount'] = test_users_df['purchase_amount'].fillna(test_users_df['purchase_amount'].median())\n",
    "\n",
    "print(\"\\nMissing values after cleaning:\")\n",
    "print(f\"News Articles: {news_df.isnull().sum().sum()}\")\n",
    "print(f\"Train Users: {train_users_df.isnull().sum().sum()}\")\n",
    "print(f\"Test Users: {test_users_df.isnull().sum().sum()}\")\n",
    "\n",
    "# 3. Feature Encoding for Classification and Bandit Training\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"3. FEATURE ENCODING FOR CLASSIFICATION AND BANDIT TRAINING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Encode categorical labels in user datasets (Convert user categories to numerical)\n",
    "print(\"\\nEncoding user labels...\")\n",
    "label_encoder = LabelEncoder()\n",
    "train_users_df['label_encoded'] = label_encoder.fit_transform(train_users_df['label'])\n",
    "test_users_df['label_encoded'] = label_encoder.transform(test_users_df['label'])\n",
    "\n",
    "print(f\"Original labels: {train_users_df['label'].unique()}\")\n",
    "print(f\"Encoded labels: {train_users_df['label_encoded'].unique()}\")\n",
    "print(f\"Mapping: {dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))}\")\n",
    "\n",
    "# Encode news article categories\n",
    "print(\"\\nEncoding news article categories...\")\n",
    "news_category_encoder = LabelEncoder()\n",
    "news_df['category_encoded'] = news_category_encoder.fit_transform(news_df['category'])\n",
    "\n",
    "print(f\"Original categories: {news_df['category'].unique()}\")\n",
    "print(f\"Encoded categories: {news_df['category_encoded'].unique()}\")\n",
    "print(f\"Mapping: {dict(zip(news_category_encoder.classes_, news_category_encoder.transform(news_category_encoder.classes_)))}\")\n",
    "\n",
    "# Prepare feature sets for user classification\n",
    "print(\"\\nPreparing feature sets for user classification...\")\n",
    "\n",
    "# Numerical features\n",
    "feature_columns = ['age', 'income', 'clicks', 'purchase_amount']\n",
    "\n",
    "# Create train and test feature matrices\n",
    "X_train = train_users_df[feature_columns].copy()\n",
    "y_train = train_users_df['label_encoded'].copy()\n",
    "\n",
    "X_test = test_users_df[feature_columns].copy()\n",
    "y_test = test_users_df['label_encoded'].copy()\n",
    "\n",
    "print(f\"\\nTrain Features Shape: {X_train.shape}\")\n",
    "print(f\"Train Labels Shape: {y_train.shape}\")\n",
    "print(f\"Test Features Shape: {X_test.shape}\")\n",
    "print(f\"Test Labels Shape: {y_test.shape}\")\n",
    "\n",
    "# Normalize numerical features for better classification performance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"\\nFeatures normalized using StandardScaler\")\n",
    "print(f\"Train features stats after scaling:\")\n",
    "print(f\"  Mean: {X_train_scaled.mean(axis=0)}\")\n",
    "print(f\"  Std: {X_train_scaled.std(axis=0)}\")\n",
    "\n",
    "# Summary of preprocessed data\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PREPROCESSING SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n‚úì Loaded 3 datasets successfully\")\n",
    "print(f\"‚úì Handled missing values in all datasets\")\n",
    "print(f\"‚úì Encoded categorical features (user labels and article categories)\")\n",
    "print(f\"‚úì Normalized numerical features for ML training\")\n",
    "print(f\"\\nReady for user classification and contextual bandit training!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4d6352",
   "metadata": {},
   "source": [
    "## User Classification\n",
    "\n",
    "Train a classifier to predict the user category (`User1`, `User2`, `User3`),\n",
    "which serves as the **context** for the contextual bandit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a127c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "USER CLASSIFICATION - TRAINING & MODEL SELECTION\n",
      "================================================================================\n",
      "\n",
      "[1] Training Decision Tree Classifier...\n",
      "‚úì Decision Tree Accuracy: 0.3285 (Training time: 0.0115s)\n",
      "\n",
      "[2] Training Logistic Regression...\n",
      "‚úì Logistic Regression Accuracy: 0.3295 (Training time: 0.0081s)\n",
      "\n",
      "[3] Training Random Forest Classifier...\n",
      "‚úì Random Forest Accuracy: 0.3115 (Training time: 0.4153s)\n",
      "\n",
      "[4] Training Gradient Boosting Classifier...\n",
      "‚úì Gradient Boosting Accuracy: 0.3240 (Training time: 0.8416s)\n",
      "\n",
      "[5] Training Support Vector Machine (SVM)...\n",
      "‚úì SVM Accuracy: 0.3315 (Training time: 0.2584s)\n",
      "\n",
      "[6] Training K-Nearest Neighbors (KNN)...\n",
      "‚úì KNN Accuracy: 0.3320 (Training time: 0.0115s)\n",
      "\n",
      "================================================================================\n",
      "MODEL COMPARISON RESULTS\n",
      "================================================================================\n",
      "              Model  Accuracy  Precision  Recall  F1-Score  Training Time (s)\n",
      "      Decision Tree    0.3285   0.328153  0.3285  0.328198           0.011514\n",
      "Logistic Regression    0.3295   0.319347  0.3295  0.309510           0.008090\n",
      "      Random Forest    0.3115   0.312137  0.3115  0.311604           0.415256\n",
      "  Gradient Boosting    0.3240   0.323989  0.3240  0.323421           0.841609\n",
      "                SVM    0.3315   0.331636  0.3315  0.331494           0.258377\n",
      "                KNN    0.3320   0.336270  0.3320  0.325543           0.011524\n",
      "\n",
      "üèÜ BEST MODEL SELECTED: KNN\n",
      "================================================================================\n",
      "Accuracy: 0.3320 (33.20%)\n",
      "Model: KNeighborsClassifier()\n",
      "\n",
      "================================================================================\n",
      "DETAILED PERFORMANCE - KNN\n",
      "================================================================================\n",
      "\n",
      "Confusion Matrix:\n",
      "[[309 235 128]\n",
      " [340 204 135]\n",
      " [296 202 151]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       user1       0.33      0.46      0.38       672\n",
      "       user2       0.32      0.30      0.31       679\n",
      "       user3       0.36      0.23      0.28       649\n",
      "\n",
      "    accuracy                           0.33      2000\n",
      "   macro avg       0.34      0.33      0.33      2000\n",
      "weighted avg       0.34      0.33      0.33      2000\n",
      "\n",
      "\n",
      "================================================================================\n",
      "CLASSIFICATION SUMMARY\n",
      "================================================================================\n",
      "‚úì Trained 6 different classifiers\n",
      "‚úì Best model: KNN\n",
      "‚úì Test accuracy: 0.3320 (33.20%)\n",
      "‚úì Context detector ready for bandit training!\n"
     ]
    }
   ],
   "source": [
    "# User Classification - Train Multiple Models and Select the Best One\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "import time\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"USER CLASSIFICATION - TRAINING & MODEL SELECTION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Dictionary to store all models and their results\n",
    "models_dict = {}\n",
    "results = []\n",
    "\n",
    "# 1. Decision Tree Classifier\n",
    "print(\"\\n[1] Training Decision Tree Classifier...\")\n",
    "start_time = time.time()\n",
    "dt_clf = DecisionTreeClassifier(random_state=42, max_depth=10)\n",
    "dt_clf.fit(X_train_scaled, y_train)\n",
    "dt_pred = dt_clf.predict(X_test_scaled)\n",
    "dt_accuracy = accuracy_score(y_test, dt_pred)\n",
    "dt_time = time.time() - start_time\n",
    "models_dict['Decision Tree'] = dt_clf\n",
    "results.append({\n",
    "    'Model': 'Decision Tree',\n",
    "    'Accuracy': dt_accuracy,\n",
    "    'Precision': precision_score(y_test, dt_pred, average='weighted'),\n",
    "    'Recall': recall_score(y_test, dt_pred, average='weighted'),\n",
    "    'F1-Score': f1_score(y_test, dt_pred, average='weighted'),\n",
    "    'Training Time (s)': dt_time\n",
    "})\n",
    "print(f\"‚úì Decision Tree Accuracy: {dt_accuracy:.4f} (Training time: {dt_time:.4f}s)\")\n",
    "\n",
    "# 2. Logistic Regression\n",
    "print(\"\\n[2] Training Logistic Regression...\")\n",
    "start_time = time.time()\n",
    "lr_clf = LogisticRegression(random_state=42, max_iter=1000)\n",
    "lr_clf.fit(X_train_scaled, y_train)\n",
    "lr_pred = lr_clf.predict(X_test_scaled)\n",
    "lr_accuracy = accuracy_score(y_test, lr_pred)\n",
    "lr_time = time.time() - start_time\n",
    "models_dict['Logistic Regression'] = lr_clf\n",
    "results.append({\n",
    "    'Model': 'Logistic Regression',\n",
    "    'Accuracy': lr_accuracy,\n",
    "    'Precision': precision_score(y_test, lr_pred, average='weighted'),\n",
    "    'Recall': recall_score(y_test, lr_pred, average='weighted'),\n",
    "    'F1-Score': f1_score(y_test, lr_pred, average='weighted'),\n",
    "    'Training Time (s)': lr_time\n",
    "})\n",
    "print(f\"‚úì Logistic Regression Accuracy: {lr_accuracy:.4f} (Training time: {lr_time:.4f}s)\")\n",
    "\n",
    "# 3. Random Forest Classifier\n",
    "print(\"\\n[3] Training Random Forest Classifier...\")\n",
    "start_time = time.time()\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_clf.fit(X_train_scaled, y_train)\n",
    "rf_pred = rf_clf.predict(X_test_scaled)\n",
    "rf_accuracy = accuracy_score(y_test, rf_pred)\n",
    "rf_time = time.time() - start_time\n",
    "models_dict['Random Forest'] = rf_clf\n",
    "results.append({\n",
    "    'Model': 'Random Forest',\n",
    "    'Accuracy': rf_accuracy,\n",
    "    'Precision': precision_score(y_test, rf_pred, average='weighted'),\n",
    "    'Recall': recall_score(y_test, rf_pred, average='weighted'),\n",
    "    'F1-Score': f1_score(y_test, rf_pred, average='weighted'),\n",
    "    'Training Time (s)': rf_time\n",
    "})\n",
    "print(f\"‚úì Random Forest Accuracy: {rf_accuracy:.4f} (Training time: {rf_time:.4f}s)\")\n",
    "\n",
    "# 4. Gradient Boosting Classifier\n",
    "print(\"\\n[4] Training Gradient Boosting Classifier...\")\n",
    "start_time = time.time()\n",
    "gb_clf = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "gb_clf.fit(X_train_scaled, y_train)\n",
    "gb_pred = gb_clf.predict(X_test_scaled)\n",
    "gb_accuracy = accuracy_score(y_test, gb_pred)\n",
    "gb_time = time.time() - start_time\n",
    "models_dict['Gradient Boosting'] = gb_clf\n",
    "results.append({\n",
    "    'Model': 'Gradient Boosting',\n",
    "    'Accuracy': gb_accuracy,\n",
    "    'Precision': precision_score(y_test, gb_pred, average='weighted'),\n",
    "    'Recall': recall_score(y_test, gb_pred, average='weighted'),\n",
    "    'F1-Score': f1_score(y_test, gb_pred, average='weighted'),\n",
    "    'Training Time (s)': gb_time\n",
    "})\n",
    "print(f\"‚úì Gradient Boosting Accuracy: {gb_accuracy:.4f} (Training time: {gb_time:.4f}s)\")\n",
    "\n",
    "# 5. Support Vector Machine (SVM)\n",
    "print(\"\\n[5] Training Support Vector Machine (SVM)...\")\n",
    "start_time = time.time()\n",
    "svm_clf = SVC(kernel='rbf', random_state=42)\n",
    "svm_clf.fit(X_train_scaled, y_train)\n",
    "svm_pred = svm_clf.predict(X_test_scaled)\n",
    "svm_accuracy = accuracy_score(y_test, svm_pred)\n",
    "svm_time = time.time() - start_time\n",
    "models_dict['SVM'] = svm_clf\n",
    "results.append({\n",
    "    'Model': 'SVM',\n",
    "    'Accuracy': svm_accuracy,\n",
    "    'Precision': precision_score(y_test, svm_pred, average='weighted'),\n",
    "    'Recall': recall_score(y_test, svm_pred, average='weighted'),\n",
    "    'F1-Score': f1_score(y_test, svm_pred, average='weighted'),\n",
    "    'Training Time (s)': svm_time\n",
    "})\n",
    "print(f\"‚úì SVM Accuracy: {svm_accuracy:.4f} (Training time: {svm_time:.4f}s)\")\n",
    "\n",
    "# 6. K-Nearest Neighbors (KNN)\n",
    "print(\"\\n[6] Training K-Nearest Neighbors (KNN)...\")\n",
    "start_time = time.time()\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_clf.fit(X_train_scaled, y_train)\n",
    "knn_pred = knn_clf.predict(X_test_scaled)\n",
    "knn_accuracy = accuracy_score(y_test, knn_pred)\n",
    "knn_time = time.time() - start_time\n",
    "models_dict['KNN'] = knn_clf\n",
    "results.append({\n",
    "    'Model': 'KNN',\n",
    "    'Accuracy': knn_accuracy,\n",
    "    'Precision': precision_score(y_test, knn_pred, average='weighted'),\n",
    "    'Recall': recall_score(y_test, knn_pred, average='weighted'),\n",
    "    'F1-Score': f1_score(y_test, knn_pred, average='weighted'),\n",
    "    'Training Time (s)': knn_time\n",
    "})\n",
    "print(f\"‚úì KNN Accuracy: {knn_accuracy:.4f} (Training time: {knn_time:.4f}s)\")\n",
    "\n",
    "# Select the best model\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODEL COMPARISON RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "best_model_name = results_df.loc[results_df['Accuracy'].idxmax(), 'Model']\n",
    "best_accuracy = results_df['Accuracy'].max()\n",
    "best_clf = models_dict[best_model_name]\n",
    "\n",
    "print(f\"\\n{'üèÜ BEST MODEL SELECTED: ' + best_model_name}\")\n",
    "print(f\"{'=' * 80}\")\n",
    "print(f\"Accuracy: {best_accuracy:.4f} ({best_accuracy*100:.2f}%)\")\n",
    "print(f\"Model: {best_clf}\")\n",
    "\n",
    "# Detailed performance metrics for the best model\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"DETAILED PERFORMANCE - {best_model_name}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if best_model_name == 'Decision Tree':\n",
    "    best_pred = dt_pred\n",
    "elif best_model_name == 'Logistic Regression':\n",
    "    best_pred = lr_pred\n",
    "elif best_model_name == 'Random Forest':\n",
    "    best_pred = rf_pred\n",
    "elif best_model_name == 'Gradient Boosting':\n",
    "    best_pred = gb_pred\n",
    "elif best_model_name == 'SVM':\n",
    "    best_pred = svm_pred\n",
    "else:\n",
    "    best_pred = knn_pred\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(y_test, best_pred)\n",
    "print(cm)\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "class_report = classification_report(y_test, best_pred, target_names=label_encoder.classes_)\n",
    "print(class_report)\n",
    "\n",
    "# Feature Importance (if applicable)\n",
    "if hasattr(best_clf, 'feature_importances_'):\n",
    "    print(\"\\nFeature Importance:\")\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': feature_columns,\n",
    "        'Importance': best_clf.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    print(feature_importance)\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CLASSIFICATION SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"‚úì Trained {len(models_dict)} different classifiers\")\n",
    "print(f\"‚úì Best model: {best_model_name}\")\n",
    "print(f\"‚úì Test accuracy: {best_accuracy:.4f} ({best_accuracy*100:.2f}%)\")\n",
    "print(f\"‚úì Context detector ready for bandit training!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c39ad93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "HYPERPARAMETER TUNING & OPTIMIZATION FOR MAXIMUM ACCURACY\n",
      "================================================================================\n",
      "\n",
      "[1] Tuning KNN Hyperparameters...\n",
      "‚úì Best KNN Accuracy: 0.3225\n",
      "  Best params: {'metric': 'euclidean', 'n_neighbors': 3, 'weights': 'distance'}\n",
      "\n",
      "[2] Tuning SVM Hyperparameters...\n",
      "‚úì Best SVM Accuracy: 0.3455\n",
      "  Best params: {'C': 10, 'gamma': 'scale', 'kernel': 'poly'}\n",
      "\n",
      "[3] Tuning Random Forest Hyperparameters...\n",
      "‚úì Best Random Forest Accuracy: 0.3185\n",
      "  Best params: {'max_depth': 10, 'min_samples_split': 10, 'n_estimators': 300}\n",
      "\n",
      "[4] Tuning Gradient Boosting Hyperparameters...\n",
      "‚úì Best Gradient Boosting Accuracy: 0.3130\n",
      "  Best params: {'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 100}\n",
      "\n",
      "[5] Creating Ensemble Voting Classifier...\n",
      "‚úì Ensemble Voting Classifier Accuracy: 0.3250\n",
      "\n",
      "================================================================================\n",
      "OPTIMIZED MODELS COMPARISON (RANKED BY ACCURACY)\n",
      "================================================================================\n",
      "                    Model  Accuracy\n",
      "              SVM (Tuned)    0.3455\n",
      "          Ensemble Voting    0.3250\n",
      "              KNN (Tuned)    0.3225\n",
      "    Random Forest (Tuned)    0.3185\n",
      "Gradient Boosting (Tuned)    0.3130\n",
      "\n",
      "üèÜ MAXIMUM ACCURACY CLASSIFIER SELECTED: SVM (Tuned)\n",
      "================================================================================\n",
      "Test Accuracy: 0.3455 (34.55%)\n",
      "Improvement: 1.35% over initial best model\n",
      "\n",
      "================================================================================\n",
      "FINAL PERFORMANCE METRICS - SVM (Tuned)\n",
      "================================================================================\n",
      "\n",
      "Confusion Matrix:\n",
      "[[289 260 123]\n",
      " [283 238 158]\n",
      " [277 208 164]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       user1       0.34      0.43      0.38       672\n",
      "       user2       0.34      0.35      0.34       679\n",
      "       user3       0.37      0.25      0.30       649\n",
      "\n",
      "    accuracy                           0.35      2000\n",
      "   macro avg       0.35      0.34      0.34      2000\n",
      "weighted avg       0.35      0.35      0.34      2000\n",
      "\n",
      "\n",
      "================================================================================\n",
      "FINAL CLASSIFICATION SUMMARY\n",
      "================================================================================\n",
      "‚úì Performed hyperparameter tuning on 4 classifiers using GridSearchCV (5-fold CV)\n",
      "‚úì Created ensemble voting classifier combining best models\n",
      "‚úì Final best model: SVM (Tuned)\n",
      "‚úì Final test accuracy: 0.3455 (34.55%)\n",
      "‚úì Context detector optimized with maximum accuracy!\n",
      "‚úì Ready for contextual bandit training!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Hyperparameter Tuning for Maximum Accuracy\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"HYPERPARAMETER TUNING & OPTIMIZATION FOR MAXIMUM ACCURACY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Optimize KNN\n",
    "print(\"\\n[1] Tuning KNN Hyperparameters...\")\n",
    "knn_params = {\n",
    "    'n_neighbors': [3, 5, 7, 9, 11, 13, 15],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "knn_grid = GridSearchCV(KNeighborsClassifier(), knn_params, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "knn_grid.fit(X_train_scaled, y_train)\n",
    "knn_best_pred = knn_grid.predict(X_test_scaled)\n",
    "knn_best_accuracy = accuracy_score(y_test, knn_best_pred)\n",
    "print(f\"‚úì Best KNN Accuracy: {knn_best_accuracy:.4f}\")\n",
    "print(f\"  Best params: {knn_grid.best_params_}\")\n",
    "\n",
    "# 2. Optimize SVM with probability enabled\n",
    "print(\"\\n[2] Tuning SVM Hyperparameters...\")\n",
    "svm_params = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'kernel': ['rbf', 'poly'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "svm_grid = GridSearchCV(SVC(random_state=42, probability=True), svm_params, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "svm_grid.fit(X_train_scaled, y_train)\n",
    "svm_best_pred = svm_grid.predict(X_test_scaled)\n",
    "svm_best_accuracy = accuracy_score(y_test, svm_best_pred)\n",
    "print(f\"‚úì Best SVM Accuracy: {svm_best_accuracy:.4f}\")\n",
    "print(f\"  Best params: {svm_grid.best_params_}\")\n",
    "\n",
    "# 3. Optimize Random Forest\n",
    "print(\"\\n[3] Tuning Random Forest Hyperparameters...\")\n",
    "rf_params = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [5, 10, 15, None],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "rf_grid = GridSearchCV(RandomForestClassifier(random_state=42, n_jobs=-1), rf_params, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "rf_grid.fit(X_train_scaled, y_train)\n",
    "rf_best_pred = rf_grid.predict(X_test_scaled)\n",
    "rf_best_accuracy = accuracy_score(y_test, rf_best_pred)\n",
    "print(f\"‚úì Best Random Forest Accuracy: {rf_best_accuracy:.4f}\")\n",
    "print(f\"  Best params: {rf_grid.best_params_}\")\n",
    "\n",
    "# 4. Optimize Gradient Boosting\n",
    "print(\"\\n[4] Tuning Gradient Boosting Hyperparameters...\")\n",
    "gb_params = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "gb_grid = GridSearchCV(GradientBoostingClassifier(random_state=42), gb_params, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "gb_grid.fit(X_train_scaled, y_train)\n",
    "gb_best_pred = gb_grid.predict(X_test_scaled)\n",
    "gb_best_accuracy = accuracy_score(y_test, gb_best_pred)\n",
    "print(f\"‚úì Best Gradient Boosting Accuracy: {gb_best_accuracy:.4f}\")\n",
    "print(f\"  Best params: {gb_grid.best_params_}\")\n",
    "\n",
    "# 5. Create Voting Classifier (Ensemble with hard voting)\n",
    "print(\"\\n[5] Creating Ensemble Voting Classifier...\")\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('knn', knn_grid.best_estimator_),\n",
    "        ('svm', svm_grid.best_estimator_),\n",
    "        ('rf', rf_grid.best_estimator_),\n",
    "        ('gb', gb_grid.best_estimator_)\n",
    "    ],\n",
    "    voting='hard'\n",
    ")\n",
    "voting_clf.fit(X_train_scaled, y_train)\n",
    "voting_pred = voting_clf.predict(X_test_scaled)\n",
    "voting_accuracy = accuracy_score(y_test, voting_pred)\n",
    "print(f\"‚úì Ensemble Voting Classifier Accuracy: {voting_accuracy:.4f}\")\n",
    "\n",
    "# Compare all tuned models\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"OPTIMIZED MODELS COMPARISON (RANKED BY ACCURACY)\")\n",
    "print(\"=\" * 80)\n",
    "tuned_results = [\n",
    "    {'Model': 'KNN (Tuned)', 'Accuracy': knn_best_accuracy},\n",
    "    {'Model': 'SVM (Tuned)', 'Accuracy': svm_best_accuracy},\n",
    "    {'Model': 'Random Forest (Tuned)', 'Accuracy': rf_best_accuracy},\n",
    "    {'Model': 'Gradient Boosting (Tuned)', 'Accuracy': gb_best_accuracy},\n",
    "    {'Model': 'Ensemble Voting', 'Accuracy': voting_accuracy}\n",
    "]\n",
    "tuned_df = pd.DataFrame(tuned_results).sort_values('Accuracy', ascending=False).reset_index(drop=True)\n",
    "print(tuned_df.to_string(index=False))\n",
    "\n",
    "# Select final best model\n",
    "final_best_model_name = tuned_df.iloc[0]['Model']\n",
    "final_best_accuracy = tuned_df.iloc[0]['Accuracy']\n",
    "\n",
    "# Assign the best model and predictions\n",
    "if 'Ensemble' in final_best_model_name:\n",
    "    final_classifier = voting_clf\n",
    "    final_predictions = voting_pred\n",
    "elif 'KNN' in final_best_model_name:\n",
    "    final_classifier = knn_grid.best_estimator_\n",
    "    final_predictions = knn_best_pred\n",
    "elif 'SVM' in final_best_model_name:\n",
    "    final_classifier = svm_grid.best_estimator_\n",
    "    final_predictions = svm_best_pred\n",
    "elif 'Random Forest' in final_best_model_name:\n",
    "    final_classifier = rf_grid.best_estimator_\n",
    "    final_predictions = rf_best_pred\n",
    "else:\n",
    "    final_classifier = gb_grid.best_estimator_\n",
    "    final_predictions = gb_best_pred\n",
    "\n",
    "print(f\"\\n{'üèÜ MAXIMUM ACCURACY CLASSIFIER SELECTED: ' + final_best_model_name}\")\n",
    "print(f\"{'=' * 80}\")\n",
    "print(f\"Test Accuracy: {final_best_accuracy:.4f} ({final_best_accuracy*100:.2f}%)\")\n",
    "print(f\"Improvement: {(final_best_accuracy - best_accuracy)*100:.2f}% over initial best model\")\n",
    "\n",
    "# Final detailed performance\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"FINAL PERFORMANCE METRICS - {final_best_model_name}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "final_cm = confusion_matrix(y_test, final_predictions)\n",
    "print(final_cm)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "final_report = classification_report(y_test, final_predictions, target_names=label_encoder.classes_)\n",
    "print(final_report)\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FINAL CLASSIFICATION SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"‚úì Performed hyperparameter tuning on 4 classifiers using GridSearchCV (5-fold CV)\")\n",
    "print(f\"‚úì Created ensemble voting classifier combining best models\")\n",
    "print(f\"‚úì Final best model: {final_best_model_name}\")\n",
    "print(f\"‚úì Final test accuracy: {final_best_accuracy:.4f} ({final_best_accuracy*100:.2f}%)\")\n",
    "print(f\"‚úì Context detector optimized with maximum accuracy!\")\n",
    "print(f\"‚úì Ready for contextual bandit training!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ba537f",
   "metadata": {},
   "source": [
    "# `Contextual Bandit`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465388d6",
   "metadata": {},
   "source": [
    "## Reward Sampler Initialization\n",
    "\n",
    "The sampler is initialized using the student's roll number `i`.\n",
    "Rewards are obtained using `sampler.sample(j)`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f0bb0f",
   "metadata": {},
   "source": [
    "## Arm Mapping\n",
    "\n",
    "| Arm Index (j) | News Category | User Context |\n",
    "|--------------|---------------|--------------|\n",
    "| 0‚Äì3          | Entertainment, Education, Tech, Crime | User1 |\n",
    "| 4‚Äì7          | Entertainment, Education, Tech, Crime | User2 |\n",
    "| 8‚Äì11         | Entertainment, Education, Tech, Crime | User3 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356c764d",
   "metadata": {},
   "source": [
    "## Epsilon-Greedy Strategy\n",
    "\n",
    "This section implements the epsilon-greedy contextual bandit algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b571e8a6",
   "metadata": {},
   "source": [
    "## Upper Confidence Bound (UCB)\n",
    "\n",
    "This section implements the UCB strategy for contextual bandits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff2fb86",
   "metadata": {},
   "source": [
    "## SoftMax Strategy\n",
    "\n",
    "This section implements the SoftMax strategy with temperature $ \\tau = 1$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb144662",
   "metadata": {},
   "source": [
    "## Reinforcement Learning Simulation\n",
    "\n",
    "We simulate the bandit algorithms for $T = 10,000$ steps and record rewards.\n",
    "\n",
    "P.S.: Change $T$ value as and if required.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca10b073",
   "metadata": {},
   "source": [
    "## Results and Analysis\n",
    "\n",
    "This section presents:\n",
    "- Average Reward vs Time\n",
    "- Hyperparameter comparisons\n",
    "- Observations and discussion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512fbb89",
   "metadata": {},
   "source": [
    "## Final Observations\n",
    "\n",
    "- Comparison of Epsilon-Greedy, UCB, and SoftMax\n",
    "- Effect of hyperparameters\n",
    "- Strengths and limitations of each approach\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5665d58e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
