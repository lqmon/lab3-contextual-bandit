{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "999bcd7d",
   "metadata": {},
   "source": [
    "# Lab 3: Contextual Bandit-Based News Article Recommendation\n",
    "\n",
    "**`Course`:** Reinforcement Learning Fundamentals  \n",
    "**`Student Name`:**  \n",
    "**`Roll Number`:**  \n",
    "**`GitHub Branch`:** firstname_U20230xxx  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755efd7a",
   "metadata": {},
   "source": [
    "# Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4bd959",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Import advanced classifiers\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "from rlcmab_sampler import sampler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c638ba06",
   "metadata": {},
   "source": [
    "# Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f6f6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "news_df = pd.read_csv(\"data/news_articles.csv\")\n",
    "train_users = pd.read_csv(\"data/train_users.csv\")\n",
    "test_users = pd.read_csv(\"data/test_users.csv\")\n",
    "\n",
    "print(\"News Articles Dataset:\")\n",
    "print(news_df.head())\n",
    "print(f\"\\nShape: {news_df.shape}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Train Users Dataset:\")\n",
    "print(train_users.head())\n",
    "print(f\"\\nShape: {train_users.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1c5cb1",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "In this section:\n",
    "- Handle missing values\n",
    "- Encode categorical features\n",
    "- Prepare data for user classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preprocessing_1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in all datasets\n",
    "print(\"=\" * 60)\n",
    "print(\"Missing Values Analysis\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nNews Articles Dataset:\")\n",
    "print(news_df.isnull().sum())\n",
    "print(f\"\\nShape: {news_df.shape}\")\n",
    "\n",
    "print(\"\\nTrain Users Dataset:\")\n",
    "print(train_users.isnull().sum())\n",
    "print(f\"\\nShape: {train_users.shape}\")\n",
    "\n",
    "print(\"\\nTest Users Dataset:\")\n",
    "print(test_users.isnull().sum())\n",
    "print(f\"\\nShape: {test_users.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preprocessing_2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "# For numerical columns: fill with median\n",
    "# For categorical columns: fill with mode\n",
    "\n",
    "def handle_missing_values(df):\n",
    "    \"\"\"\n",
    "    Handle missing values in the dataset.\n",
    "    - Numerical columns: filled with median\n",
    "    - Categorical columns: filled with mode\n",
    "    \"\"\"\n",
    "    df_cleaned = df.copy()\n",
    "    \n",
    "    for col in df_cleaned.columns:\n",
    "        if df_cleaned[col].isnull().sum() > 0:\n",
    "            if df_cleaned[col].dtype in ['int64', 'float64']:\n",
    "                # Fill numerical columns with median\n",
    "                df_cleaned[col].fillna(df_cleaned[col].median(), inplace=True)\n",
    "            else:\n",
    "                # Fill categorical columns with mode\n",
    "                df_cleaned[col].fillna(df_cleaned[col].mode()[0], inplace=True)\n",
    "    \n",
    "    return df_cleaned\n",
    "\n",
    "# Clean all datasets\n",
    "news_df_clean = handle_missing_values(news_df)\n",
    "train_users_clean = handle_missing_values(train_users)\n",
    "test_users_clean = handle_missing_values(test_users)\n",
    "\n",
    "print(\"Missing values after cleaning:\")\n",
    "print(f\"News Articles: {news_df_clean.isnull().sum().sum()}\")\n",
    "print(f\"Train Users: {train_users_clean.isnull().sum().sum()}\")\n",
    "print(f\"Test Users: {test_users_clean.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preprocessing_3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display dataset information\n",
    "print(\"=\" * 60)\n",
    "print(\"Dataset Information\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nNews Articles Categories:\")\n",
    "print(news_df_clean['category'].value_counts())\n",
    "\n",
    "print(\"\\nUser Categories (Train):\")\n",
    "print(train_users_clean['category'].value_counts())\n",
    "\n",
    "print(\"\\nColumn names (Train Users):\")\n",
    "print(train_users_clean.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preprocessing_4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature encoding for user data\n",
    "# Separate features and labels\n",
    "\n",
    "# For training users\n",
    "X_train_full = train_users_clean.drop('category', axis=1)\n",
    "y_train_full = train_users_clean['category']\n",
    "\n",
    "# For test users\n",
    "X_test = test_users_clean.drop('category', axis=1) if 'category' in test_users_clean.columns else test_users_clean\n",
    "\n",
    "# Identify categorical columns (non-numeric)\n",
    "categorical_cols = X_train_full.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_cols = X_train_full.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "print(f\"Categorical columns: {categorical_cols}\")\n",
    "print(f\"Numerical columns: {numerical_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preprocessing_5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical features using LabelEncoder\n",
    "label_encoders = {}\n",
    "\n",
    "X_train_encoded = X_train_full.copy()\n",
    "X_test_encoded = X_test.copy()\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    \n",
    "    # Fit on training data\n",
    "    X_train_encoded[col] = le.fit_transform(X_train_full[col].astype(str))\n",
    "    \n",
    "    # Transform test data (handle unseen labels)\n",
    "    if col in X_test_encoded.columns:\n",
    "        # Create a mapping for unseen categories\n",
    "        test_col_values = X_test[col].astype(str)\n",
    "        X_test_encoded[col] = test_col_values.map(\n",
    "            lambda x: le.transform([x])[0] if x in le.classes_ else -1\n",
    "        )\n",
    "    \n",
    "    label_encoders[col] = le\n",
    "\n",
    "print(\"\\nEncoding completed!\")\n",
    "print(f\"Encoded training data shape: {X_train_encoded.shape}\")\n",
    "print(f\"Encoded test data shape: {X_test_encoded.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preprocessing_6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample of encoded data\n",
    "print(\"Sample of encoded training data:\")\n",
    "print(X_train_encoded.head())\n",
    "print(\"\\nData types:\")\n",
    "print(X_train_encoded.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preprocessing_7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map news categories to standard format (Entertainment, Education, Tech, Crime)\n",
    "# Based on the assignment requirements\n",
    "print(\"=\" * 60)\n",
    "print(\"News Category Mapping\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# The actual categories in the dataset might differ from the required ones\n",
    "# We'll map them appropriately\n",
    "category_mapping = {\n",
    "    'U.S. NEWS': 'Crime',\n",
    "    'COMEDY': 'Entertainment',\n",
    "    'PARENTING': 'Education',\n",
    "    'POLITICS': 'Crime',\n",
    "    'WELLNESS': 'Education',\n",
    "    'ENTERTAINMENT': 'Entertainment',\n",
    "    'TECH': 'Tech',\n",
    "    'SCIENCE': 'Tech',\n",
    "    'BUSINESS': 'Tech',\n",
    "    'EDUCATION': 'Education',\n",
    "    'CRIME': 'Crime'\n",
    "}\n",
    "\n",
    "# Apply mapping if categories exist in news dataset\n",
    "if 'category' in news_df_clean.columns:\n",
    "    # Check unique categories\n",
    "    print(\"\\nOriginal categories in news dataset:\")\n",
    "    print(news_df_clean['category'].unique())\n",
    "    \n",
    "    # Map to standard categories\n",
    "    news_df_clean['category_mapped'] = news_df_clean['category'].map(\n",
    "        lambda x: category_mapping.get(x.upper() if isinstance(x, str) else x, x)\n",
    "    )\n",
    "    \n",
    "    print(\"\\nMapped categories:\")\n",
    "    print(news_df_clean['category_mapped'].value_counts())\n",
    "    \n",
    "    # Use mapped categories\n",
    "    news_df_clean['category'] = news_df_clean['category_mapped']\n",
    "\n",
    "# Define the arm mapping according to assignment specifications\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Arm Index Mapping (j values)\")\n",
    "print(\"=\"*60)\n",
    "arm_mapping = {\n",
    "    0: ('Entertainment', 'user_1'),\n",
    "    1: ('Education', 'user_1'),\n",
    "    2: ('Tech', 'user_1'),\n",
    "    3: ('Crime', 'user_1'),\n",
    "    4: ('Entertainment', 'user_2'),\n",
    "    5: ('Education', 'user_2'),\n",
    "    6: ('Tech', 'user_2'),\n",
    "    7: ('Crime', 'user_2'),\n",
    "    8: ('Entertainment', 'user_3'),\n",
    "    9: ('Education', 'user_3'),\n",
    "    10: ('Tech', 'user_3'),\n",
    "    11: ('Crime', 'user_3')\n",
    "}\n",
    "\n",
    "for arm_idx, (category, user) in arm_mapping.items():\n",
    "    print(f\"Arm {arm_idx:2d}: ({category:13s}, {user})\")\n",
    "\n",
    "# Create reverse mapping for easy lookup\n",
    "category_user_to_arm = {v: k for k, v in arm_mapping.items()}\n",
    "print(\"\\nArm mapping created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4d6352",
   "metadata": {},
   "source": [
    "## User Classification\n",
    "\n",
    "Train a classifier to predict the user category (`User1`, `User2`, `User3`),\n",
    "which serves as the **context** for the contextual bandit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classification_1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training data into train and validation sets (80-20 split)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_encoded, \n",
    "    y_train_full, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=y_train_full\n",
    ")\n",
    "\n",
    "print(\"Data Split Summary:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Validation set size: {X_val.shape[0]} samples\")\n",
    "print(f\"Test set size: {X_test_encoded.shape[0]} samples\")\n",
    "print(\"\\nClass distribution in training set:\")\n",
    "print(y_train.value_counts())\n",
    "print(\"\\nClass distribution in validation set:\")\n",
    "print(y_val.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classification_2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: We will use state-of-the-art gradient boosting classifiers:\n",
    "# - LightGBM: Fast, efficient, and excellent for structured data\n",
    "# - XGBoost: Robust and widely used in competitions\n",
    "# - CatBoost: Handles categorical features natively\n",
    "print(\"Using advanced gradient boosting classifiers...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classification_3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM Classifier\n",
    "print(\"=\" * 60)\n",
    "print(\"Training LightGBM Classifier\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "lgb_classifier = lgb.LGBMClassifier(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=7,\n",
    "    num_leaves=31,\n",
    "    random_state=42,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "lgb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_lgb = lgb_classifier.predict(X_val)\n",
    "\n",
    "# Evaluation\n",
    "lgb_accuracy = accuracy_score(y_val, y_pred_lgb)\n",
    "print(f\"\\nValidation Accuracy: {lgb_accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_pred_lgb))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_val, y_pred_lgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classification_4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost Classifier\n",
    "print(\"=\" * 60)\n",
    "print(\"Training XGBoost Classifier\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Encode labels for XGBoost (requires 0, 1, 2 format)\n",
    "label_encoder_y = LabelEncoder()\n",
    "y_train_encoded = label_encoder_y.fit_transform(y_train)\n",
    "y_val_encoded = label_encoder_y.transform(y_val)\n",
    "\n",
    "xgb_classifier = xgb.XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=7,\n",
    "    random_state=42,\n",
    "    eval_metric='mlogloss',\n",
    "    verbosity=0\n",
    ")\n",
    "\n",
    "xgb_classifier.fit(X_train, y_train_encoded)\n",
    "\n",
    "# Predictions\n",
    "y_pred_xgb_encoded = xgb_classifier.predict(X_val)\n",
    "y_pred_xgb = label_encoder_y.inverse_transform(y_pred_xgb_encoded)\n",
    "\n",
    "# Evaluation\n",
    "xgb_accuracy = accuracy_score(y_val, y_pred_xgb)\n",
    "print(f\"\\nValidation Accuracy: {xgb_accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_pred_xgb))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_val, y_pred_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classification_5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CatBoost Classifier\n",
    "print(\"=\" * 60)\n",
    "print(\"Training CatBoost Classifier\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "catboost_classifier = CatBoostClassifier(\n",
    "    iterations=200,\n",
    "    learning_rate=0.05,\n",
    "    depth=7,\n",
    "    random_state=42,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "catboost_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_cat = catboost_classifier.predict(X_val)\n",
    "\n",
    "# Evaluation\n",
    "cat_accuracy = accuracy_score(y_val, y_pred_cat)\n",
    "print(f\"\\nValidation Accuracy: {cat_accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_pred_cat))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_val, y_pred_cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classification_6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all classifiers\n",
    "print(\"=\" * 60)\n",
    "print(\"Classifier Performance Comparison\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"LightGBM Accuracy:  {lgb_accuracy:.4f}\")\n",
    "print(f\"XGBoost Accuracy:   {xgb_accuracy:.4f}\")\n",
    "print(f\"CatBoost Accuracy:  {cat_accuracy:.4f}\")\n",
    "\n",
    "# Select the best classifier\n",
    "classifiers = {\n",
    "    'LightGBM': (lgb_classifier, lgb_accuracy),\n",
    "    'XGBoost': (xgb_classifier, xgb_accuracy),\n",
    "    'CatBoost': (catboost_classifier, cat_accuracy)\n",
    "}\n",
    "\n",
    "best_classifier_name = max(classifiers, key=lambda k: classifiers[k][1])\n",
    "best_classifier = classifiers[best_classifier_name][0]\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ðŸ† Best Classifier: {best_classifier_name}\")\n",
    "print(f\"ðŸŽ¯ Best Accuracy: {classifiers[best_classifier_name][1]:.4f}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classification_7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the best classifier on the full training data for final use\n",
    "print(f\"\\nRetraining {best_classifier_name} on full training data...\")\n",
    "\n",
    "if best_classifier_name == 'LightGBM':\n",
    "    final_classifier = lgb.LGBMClassifier(\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=7,\n",
    "        num_leaves=31,\n",
    "        random_state=42,\n",
    "        verbose=-1\n",
    "    )\n",
    "    final_classifier.fit(X_train_encoded, y_train_full)\n",
    "    \n",
    "elif best_classifier_name == 'XGBoost':\n",
    "    final_classifier = xgb.XGBClassifier(\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=7,\n",
    "        random_state=42,\n",
    "        eval_metric='mlogloss',\n",
    "        verbosity=0\n",
    "    )\n",
    "    y_train_full_encoded = label_encoder_y.fit_transform(y_train_full)\n",
    "    final_classifier.fit(X_train_encoded, y_train_full_encoded)\n",
    "    \n",
    "elif best_classifier_name == 'CatBoost':\n",
    "    final_classifier = CatBoostClassifier(\n",
    "        iterations=200,\n",
    "        learning_rate=0.05,\n",
    "        depth=7,\n",
    "        random_state=42,\n",
    "        verbose=0\n",
    "    )\n",
    "    final_classifier.fit(X_train_encoded, y_train_full)\n",
    "\n",
    "print(f\"âœ… {best_classifier_name} trained on full training data!\")\n",
    "print(\"\\nThis classifier will be used as the Context Detector for the bandit system.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classification_8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to predict user context\n",
    "def predict_user_context(user_features):\n",
    "    \"\"\"\n",
    "    Predict the user category (user_1, user_2, or user_3) given user features.\n",
    "    \n",
    "    Args:\n",
    "        user_features: DataFrame or array of user features\n",
    "    \n",
    "    Returns:\n",
    "        Predicted user category\n",
    "    \"\"\"\n",
    "    if best_classifier_name == 'XGBoost':\n",
    "        # XGBoost returns encoded labels, need to inverse transform\n",
    "        predictions_encoded = final_classifier.predict(user_features)\n",
    "        return label_encoder_y.inverse_transform(predictions_encoded)\n",
    "    else:\n",
    "        return final_classifier.predict(user_features)\n",
    "\n",
    "# Test the function on a sample\n",
    "sample_prediction = predict_user_context(X_test_encoded.iloc[:5])\n",
    "print(\"Sample predictions on test users:\")\n",
    "print(sample_prediction)\n",
    "\n",
    "# Display the mapping for reference\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"User Context to Index Mapping (for Bandit):\")\n",
    "print(\"=\"*60)\n",
    "user_to_index = {'user_1': 0, 'user_2': 1, 'user_3': 2}\n",
    "for user, idx in user_to_index.items():\n",
    "    print(f\"{user} -> Index {idx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ba537f",
   "metadata": {},
   "source": [
    "# `Contextual Bandit`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465388d6",
   "metadata": {},
   "source": [
    "## Reward Sampler Initialization\n",
    "\n",
    "The sampler is initialized using the student's roll number `i`.\n",
    "Rewards are obtained using `sampler.sample(j)`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f0bb0f",
   "metadata": {},
   "source": [
    "## Arm Mapping\n",
    "\n",
    "| Arm Index (j) | News Category | User Context |\n",
    "|--------------|---------------|--------------|\n",
    "| 0â€“3          | Entertainment, Education, Tech, Crime | User1 |\n",
    "| 4â€“7          | Entertainment, Education, Tech, Crime | User2 |\n",
    "| 8â€“11         | Entertainment, Education, Tech, Crime | User3 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356c764d",
   "metadata": {},
   "source": [
    "## Epsilon-Greedy Strategy\n",
    "\n",
    "This section implements the epsilon-greedy contextual bandit algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b571e8a6",
   "metadata": {},
   "source": [
    "## Upper Confidence Bound (UCB)\n",
    "\n",
    "This section implements the UCB strategy for contextual bandits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff2fb86",
   "metadata": {},
   "source": [
    "## SoftMax Strategy\n",
    "\n",
    "This section implements the SoftMax strategy with temperature $ \\tau = 1$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb144662",
   "metadata": {},
   "source": [
    "## Reinforcement Learning Simulation\n",
    "\n",
    "We simulate the bandit algorithms for $T = 10,000$ steps and record rewards.\n",
    "\n",
    "P.S.: Change $T$ value as and if required.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca10b073",
   "metadata": {},
   "source": [
    "## Results and Analysis\n",
    "\n",
    "This section presents:\n",
    "- Average Reward vs Time\n",
    "- Hyperparameter comparisons\n",
    "- Observations and discussion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512fbb89",
   "metadata": {},
   "source": [
    "## Final Observations\n",
    "\n",
    "- Comparison of Epsilon-Greedy, UCB, and SoftMax\n",
    "- Effect of hyperparameters\n",
    "- Strengths and limitations of each approach\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5665d58e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
