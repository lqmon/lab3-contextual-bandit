{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "999bcd7d",
   "metadata": {},
   "source": [
    "# Lab 3: Contextual Bandit-Based News Article Recommendation\n",
    "\n",
    "**`Course`:** Reinforcement Learning Fundamentals  \n",
    "**`Student Name`:**  \n",
    "**`Roll Number`:**  \n",
    "**`GitHub Branch`:** firstname_U20230xxx  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755efd7a",
   "metadata": {},
   "source": [
    "# Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef4bd959",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "from rlcmab_sampler import sampler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c638ba06",
   "metadata": {},
   "source": [
    "# Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48f6f6a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News Articles Dataset:\n",
      "                                                link  \\\n",
      "0  https://www.huffpost.com/entry/covid-boosters-...   \n",
      "1  https://www.huffpost.com/entry/american-airlin...   \n",
      "2  https://www.huffpost.com/entry/funniest-tweets...   \n",
      "3  https://www.huffpost.com/entry/funniest-parent...   \n",
      "4  https://www.huffpost.com/entry/amy-cooper-lose...   \n",
      "\n",
      "                                            headline   category  \\\n",
      "0  Over 4 Million Americans Roll Up Sleeves For O...  U.S. NEWS   \n",
      "1  American Airlines Flyer Charged, Banned For Li...  U.S. NEWS   \n",
      "2  23 Of The Funniest Tweets About Cats And Dogs ...     COMEDY   \n",
      "3  The Funniest Tweets From Parents This Week (Se...  PARENTING   \n",
      "4  Woman Who Called Cops On Black Bird-Watcher Lo...  U.S. NEWS   \n",
      "\n",
      "                                   short_description               authors  \\\n",
      "0  Health experts said it is too early to predict...  Carla K. Johnson, AP   \n",
      "1  He was subdued by passengers and crew when he ...        Mary Papenfuss   \n",
      "2  \"Until you have a dog you don't understand wha...         Elyse Wanshel   \n",
      "3  \"Accidentally put grown-up toothpaste on my to...      Caroline Bologna   \n",
      "4  Amy Cooper accused investment firm Franklin Te...        Nina Golgowski   \n",
      "\n",
      "         date  \n",
      "0  2022-09-23  \n",
      "1  2022-09-23  \n",
      "2  2022-09-23  \n",
      "3  2022-09-23  \n",
      "4  2022-09-22  \n",
      "Shape: (209527, 6)\n",
      "Train Users Dataset:\n",
      "  user_id   age  income  clicks  purchase_amount  session_duration  \\\n",
      "0   U7392   NaN   23053      10           500.00             17.34   \n",
      "1   U2702  56.0   20239      11           913.33             22.22   \n",
      "2   U2461   NaN   13907       9          1252.62             41.57   \n",
      "3   U7475   NaN   26615      12           500.00             30.17   \n",
      "4   U6040  32.0   27958      13           500.00             65.27   \n",
      "\n",
      "   content_variety  engagement_score  num_transactions  avg_monthly_spend  \\\n",
      "0          0.36661          37.29781                 3             187.44   \n",
      "1          0.61370          59.36342                 5             145.15   \n",
      "2          0.80368          76.78706                 7             282.03   \n",
      "3          0.26499          30.19441                10             195.35   \n",
      "4          0.36385          37.12153                 5             439.68   \n",
      "\n",
      "   ...  screen_brightness  battery_percentage  cart_abandonment_count  \\\n",
      "0  ...                4.0                 2.0                       8   \n",
      "1  ...                4.5                63.0                       5   \n",
      "2  ...                1.3                22.0                       2   \n",
      "3  ...                4.2                77.0                       9   \n",
      "4  ...                4.6                30.0                       9   \n",
      "\n",
      "   browser_version  background_app_count  session_inactivity_duration  \\\n",
      "0          3.17.97                    10                        22.75   \n",
      "1          1.57.10                     8                         1.75   \n",
      "2          2.16.94                    12                        29.33   \n",
      "3          9.90.20                     4                        21.61   \n",
      "4          1.99.38                     7                         7.58   \n",
      "\n",
      "   network_jitter  region_code  subscriber   label  \n",
      "0             8.0         Z999       False  user_3  \n",
      "1             4.0         U428        True  user_2  \n",
      "2            18.0         Z999        True  user_3  \n",
      "3            22.0         X123       False  user_3  \n",
      "4            52.0         S043       False  user_1  \n",
      "\n",
      "[5 rows x 33 columns]\n",
      "Shape: (2000, 33)\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "news_df = pd.read_csv(\"data/news_articles.csv\")\n",
    "train_users = pd.read_csv(\"data/train_users.csv\")\n",
    "test_users = pd.read_csv(\"data/test_users.csv\")\n",
    "\n",
    "print(\"News Articles Dataset:\")\n",
    "print(news_df.head())\n",
    "print(f\"Shape: {news_df.shape}\")\n",
    "\n",
    "\n",
    "print(\"Train Users Dataset:\")\n",
    "print(train_users.head())\n",
    "print(f\"Shape: {train_users.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1c5cb1",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "In this section:\n",
    "- Handle missing values\n",
    "- Encode categorical features\n",
    "- Prepare data for user classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "preprocessing_1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values Analysis\n",
      "News Articles Dataset:\n",
      "link                     0\n",
      "headline                 6\n",
      "category                 0\n",
      "short_description    19712\n",
      "authors              37418\n",
      "date                     0\n",
      "dtype: int64\n",
      "Shape: (209527, 6)\n",
      "Train Users Dataset:\n",
      "user_id                          0\n",
      "age                            698\n",
      "income                           0\n",
      "clicks                           0\n",
      "purchase_amount                  0\n",
      "session_duration                 0\n",
      "content_variety                  0\n",
      "engagement_score                 0\n",
      "num_transactions                 0\n",
      "avg_monthly_spend                0\n",
      "avg_cart_value                   0\n",
      "browsing_depth                   0\n",
      "revisit_rate                     0\n",
      "scroll_activity                  0\n",
      "time_on_site                     0\n",
      "interaction_count                0\n",
      "preferred_price_range            0\n",
      "discount_usage_rate              0\n",
      "wishlist_size                    0\n",
      "product_views                    0\n",
      "repeat_purchase_gap (days)       0\n",
      "churn_risk_score                 0\n",
      "loyalty_index                    0\n",
      "screen_brightness                0\n",
      "battery_percentage               0\n",
      "cart_abandonment_count           0\n",
      "browser_version                  0\n",
      "background_app_count             0\n",
      "session_inactivity_duration      0\n",
      "network_jitter                   0\n",
      "region_code                      0\n",
      "subscriber                       0\n",
      "label                            0\n",
      "dtype: int64\n",
      "Shape: (2000, 33)\n",
      "Test Users Dataset:\n",
      "user_id                          0\n",
      "age                            679\n",
      "income                           0\n",
      "clicks                           0\n",
      "purchase_amount                  0\n",
      "session_duration                 0\n",
      "content_variety                  0\n",
      "engagement_score                 0\n",
      "num_transactions                 0\n",
      "avg_monthly_spend                0\n",
      "avg_cart_value                   0\n",
      "browsing_depth                   0\n",
      "revisit_rate                     0\n",
      "scroll_activity                  0\n",
      "time_on_site                     0\n",
      "interaction_count                0\n",
      "preferred_price_range            0\n",
      "discount_usage_rate              0\n",
      "wishlist_size                    0\n",
      "product_views                    0\n",
      "repeat_purchase_gap (days)       0\n",
      "churn_risk_score                 0\n",
      "loyalty_index                    0\n",
      "screen_brightness                0\n",
      "battery_percentage               0\n",
      "cart_abandonment_count           0\n",
      "browser_version                  0\n",
      "background_app_count             0\n",
      "session_inactivity_duration      0\n",
      "network_jitter                   0\n",
      "region_code                      0\n",
      "subscriber                       0\n",
      "dtype: int64\n",
      "Shape: (2000, 32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Missing Values Analysis\")\n",
    "print(\"News Articles Dataset:\")\n",
    "print(news_df.isnull().sum())\n",
    "print(f\"Shape: {news_df.shape}\")\n",
    "\n",
    "print(\"Train Users Dataset:\")\n",
    "print(train_users.isnull().sum())\n",
    "print(f\"Shape: {train_users.shape}\")\n",
    "\n",
    "print(\"Test Users Dataset:\")\n",
    "print(test_users.isnull().sum())\n",
    "print(f\"Shape: {test_users.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "preprocessing_2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24451/3106474575.py:16: ChainedAssignmentError: A value is being set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "Such inplace method never works to update the original DataFrame or Series, because the intermediate object on which we are setting values always behaves as a copy (due to Copy-on-Write).\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' instead, to perform the operation inplace on the original object, or try to avoid an inplace operation using 'df[col] = df[col].method(value)'.\n",
      "\n",
      "See the documentation for a more detailed explanation: https://pandas.pydata.org/pandas-docs/stable/user_guide/copy_on_write.html\n",
      "  df_cleaned[col].fillna(df_cleaned[col].mode()[0], inplace=True)\n",
      "/tmp/ipykernel_24451/3106474575.py:13: ChainedAssignmentError: A value is being set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "Such inplace method never works to update the original DataFrame or Series, because the intermediate object on which we are setting values always behaves as a copy (due to Copy-on-Write).\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' instead, to perform the operation inplace on the original object, or try to avoid an inplace operation using 'df[col] = df[col].method(value)'.\n",
      "\n",
      "See the documentation for a more detailed explanation: https://pandas.pydata.org/pandas-docs/stable/user_guide/copy_on_write.html\n",
      "  df_cleaned[col].fillna(df_cleaned[col].median(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values after cleaning:\n",
      "News Articles: 57136\n",
      "Train Users: 698\n",
      "Test Users: 679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24451/3106474575.py:13: ChainedAssignmentError: A value is being set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "Such inplace method never works to update the original DataFrame or Series, because the intermediate object on which we are setting values always behaves as a copy (due to Copy-on-Write).\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' instead, to perform the operation inplace on the original object, or try to avoid an inplace operation using 'df[col] = df[col].method(value)'.\n",
      "\n",
      "See the documentation for a more detailed explanation: https://pandas.pydata.org/pandas-docs/stable/user_guide/copy_on_write.html\n",
      "  df_cleaned[col].fillna(df_cleaned[col].median(), inplace=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def handle_missing_values(df):\n",
    "    \"\"\"\n",
    "    Handle missing values in the dataset.\n",
    "    - Numerical columns: filled with median\n",
    "    - Categorical columns: filled with mode\n",
    "    \"\"\"\n",
    "    df_cleaned = df.copy()\n",
    "    \n",
    "    for col in df_cleaned.columns:\n",
    "        if df_cleaned[col].isnull().sum() > 0:\n",
    "            if df_cleaned[col].dtype in ['int64', 'float64']:\n",
    "                # Fill numerical columns with median\n",
    "                df_cleaned[col].fillna(df_cleaned[col].median(), inplace=True)\n",
    "            else:\n",
    "                # Fill categorical columns with mode\n",
    "                df_cleaned[col].fillna(df_cleaned[col].mode()[0], inplace=True)\n",
    "    \n",
    "    return df_cleaned\n",
    "\n",
    "# Clean all datasets\n",
    "news_df_clean = handle_missing_values(news_df)\n",
    "train_users_clean = handle_missing_values(train_users)\n",
    "test_users_clean = handle_missing_values(test_users)\n",
    "\n",
    "print(\"Missing values after cleaning:\")\n",
    "print(f\"News Articles: {news_df_clean.isnull().sum().sum()}\")\n",
    "print(f\"Train Users: {train_users_clean.isnull().sum().sum()}\")\n",
    "print(f\"Test Users: {test_users_clean.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "preprocessing_3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Information\n",
      "News Articles Categories:\n",
      "category\n",
      "POLITICS          35602\n",
      "WELLNESS          17945\n",
      "ENTERTAINMENT     17362\n",
      "TRAVEL             9900\n",
      "STYLE & BEAUTY     9814\n",
      "PARENTING          8791\n",
      "HEALTHY LIVING     6694\n",
      "QUEER VOICES       6347\n",
      "FOOD & DRINK       6340\n",
      "BUSINESS           5992\n",
      "COMEDY             5400\n",
      "SPORTS             5077\n",
      "BLACK VOICES       4583\n",
      "HOME & LIVING      4320\n",
      "PARENTS            3955\n",
      "THE WORLDPOST      3664\n",
      "WEDDINGS           3653\n",
      "WOMEN              3572\n",
      "CRIME              3562\n",
      "IMPACT             3484\n",
      "DIVORCE            3426\n",
      "WORLD NEWS         3299\n",
      "MEDIA              2944\n",
      "WEIRD NEWS         2777\n",
      "GREEN              2622\n",
      "WORLDPOST          2579\n",
      "RELIGION           2577\n",
      "STYLE              2254\n",
      "SCIENCE            2206\n",
      "TECH               2104\n",
      "TASTE              2096\n",
      "MONEY              1756\n",
      "ARTS               1509\n",
      "ENVIRONMENT        1444\n",
      "FIFTY              1401\n",
      "GOOD NEWS          1398\n",
      "U.S. NEWS          1377\n",
      "ARTS & CULTURE     1339\n",
      "COLLEGE            1144\n",
      "LATINO VOICES      1130\n",
      "CULTURE & ARTS     1074\n",
      "EDUCATION          1014\n",
      "Name: count, dtype: int64\n",
      "User Categories (Train):\n",
      "label\n",
      "user_2    712\n",
      "user_1    707\n",
      "user_3    581\n",
      "Name: count, dtype: int64\n",
      "Column names (Train Users):\n",
      "['user_id', 'age', 'income', 'clicks', 'purchase_amount', 'session_duration', 'content_variety', 'engagement_score', 'num_transactions', 'avg_monthly_spend', 'avg_cart_value', 'browsing_depth', 'revisit_rate', 'scroll_activity', 'time_on_site', 'interaction_count', 'preferred_price_range', 'discount_usage_rate', 'wishlist_size', 'product_views', 'repeat_purchase_gap (days)', 'churn_risk_score', 'loyalty_index', 'screen_brightness', 'battery_percentage', 'cart_abandonment_count', 'browser_version', 'background_app_count', 'session_inactivity_duration', 'network_jitter', 'region_code', 'subscriber', 'label']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Dataset Information\")\n",
    "\n",
    "print(\"News Articles Categories:\")\n",
    "print(news_df_clean['category'].value_counts())\n",
    "\n",
    "print(\"User Categories (Train):\")\n",
    "print(train_users_clean['label'].value_counts())\n",
    "\n",
    "print(\"Column names (Train Users):\")\n",
    "print(train_users_clean.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "preprocessing_4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns: ['user_id', 'browser_version', 'region_code']\n",
      "Numerical columns: ['age', 'income', 'clicks', 'purchase_amount', 'session_duration', 'content_variety', 'engagement_score', 'num_transactions', 'avg_monthly_spend', 'avg_cart_value', 'browsing_depth', 'revisit_rate', 'scroll_activity', 'time_on_site', 'interaction_count', 'preferred_price_range', 'discount_usage_rate', 'wishlist_size', 'product_views', 'repeat_purchase_gap (days)', 'churn_risk_score', 'loyalty_index', 'screen_brightness', 'battery_percentage', 'cart_abandonment_count', 'background_app_count', 'session_inactivity_duration', 'network_jitter']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24451/983119774.py:6: Pandas4Warning: For backward compatibility, 'str' dtypes are included by select_dtypes when 'object' dtype is specified. This behavior is deprecated and will be removed in a future version. Explicitly pass 'str' to `include` to select them, or to `exclude` to remove them and silence this warning.\n",
      "See https://pandas.pydata.org/docs/user_guide/migration-3-strings.html#string-migration-select-dtypes for details on how to write code that works with pandas 2 and 3.\n",
      "  categorical_cols = X_train_full.select_dtypes(include=['object']).columns.tolist()\n"
     ]
    }
   ],
   "source": [
    "X_train_full = train_users_clean.drop('label', axis=1)\n",
    "y_train_full = train_users_clean['label']\n",
    "\n",
    "X_test = test_users_clean.drop('label', axis=1) if 'label' in test_users_clean.columns else test_users_clean\n",
    "\n",
    "categorical_cols = X_train_full.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_cols = X_train_full.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "print(f\"Categorical columns: {categorical_cols}\")\n",
    "print(f\"Numerical columns: {numerical_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "preprocessing_5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding completed!\n",
      "Encoded training data shape: (2000, 32)\n",
      "Encoded test data shape: (2000, 32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "label_encoders = {}\n",
    "\n",
    "X_train_encoded = X_train_full.copy()\n",
    "X_test_encoded = X_test.copy()\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    \n",
    "    X_train_encoded[col] = le.fit_transform(X_train_full[col].astype(str))\n",
    "    \n",
    "    if col in X_test_encoded.columns:\n",
    "        test_col_values = X_test[col].astype(str)\n",
    "        X_test_encoded[col] = test_col_values.map(\n",
    "            lambda x: le.transform([x])[0] if x in le.classes_ else -1\n",
    "        )\n",
    "    \n",
    "    label_encoders[col] = le\n",
    "\n",
    "print(\"Encoding completed!\")\n",
    "print(f\"Encoded training data shape: {X_train_encoded.shape}\")\n",
    "print(f\"Encoded test data shape: {X_test_encoded.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "preprocessing_6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of encoded training data:\n",
      "   user_id   age  income  clicks  purchase_amount  session_duration  \\\n",
      "0     1340   NaN   23053      10           500.00             17.34   \n",
      "1      491  56.0   20239      11           913.33             22.22   \n",
      "2      446   NaN   13907       9          1252.62             41.57   \n",
      "3     1354   NaN   26615      12           500.00             30.17   \n",
      "4     1082  32.0   27958      13           500.00             65.27   \n",
      "\n",
      "   content_variety  engagement_score  num_transactions  avg_monthly_spend  \\\n",
      "0          0.36661          37.29781                 3             187.44   \n",
      "1          0.61370          59.36342                 5             145.15   \n",
      "2          0.80368          76.78706                 7             282.03   \n",
      "3          0.26499          30.19441                10             195.35   \n",
      "4          0.36385          37.12153                 5             439.68   \n",
      "\n",
      "   ...  loyalty_index  screen_brightness  battery_percentage  \\\n",
      "0  ...           15.0                4.0                 2.0   \n",
      "1  ...           33.0                4.5                63.0   \n",
      "2  ...           24.0                1.3                22.0   \n",
      "3  ...           11.0                4.2                77.0   \n",
      "4  ...           19.0                4.6                30.0   \n",
      "\n",
      "   cart_abandonment_count  browser_version  background_app_count  \\\n",
      "0                       8              464                    10   \n",
      "1                       5              115                     8   \n",
      "2                       2              259                    12   \n",
      "3                       9             1952                     4   \n",
      "4                       9              211                     7   \n",
      "\n",
      "   session_inactivity_duration  network_jitter  region_code  subscriber  \n",
      "0                        22.75             8.0         1283       False  \n",
      "1                         1.75             4.0          987        True  \n",
      "2                        29.33            18.0         1283        True  \n",
      "3                        21.61            22.0         1111       False  \n",
      "4                         7.58            52.0          872       False  \n",
      "\n",
      "[5 rows x 32 columns]\n",
      "Data types:\n",
      "user_id                          int64\n",
      "age                            float64\n",
      "income                           int64\n",
      "clicks                           int64\n",
      "purchase_amount                float64\n",
      "session_duration               float64\n",
      "content_variety                float64\n",
      "engagement_score               float64\n",
      "num_transactions                 int64\n",
      "avg_monthly_spend              float64\n",
      "avg_cart_value                 float64\n",
      "browsing_depth                   int64\n",
      "revisit_rate                   float64\n",
      "scroll_activity                  int64\n",
      "time_on_site                   float64\n",
      "interaction_count                int64\n",
      "preferred_price_range          float64\n",
      "discount_usage_rate            float64\n",
      "wishlist_size                    int64\n",
      "product_views                    int64\n",
      "repeat_purchase_gap (days)     float64\n",
      "churn_risk_score               float64\n",
      "loyalty_index                  float64\n",
      "screen_brightness              float64\n",
      "battery_percentage             float64\n",
      "cart_abandonment_count           int64\n",
      "browser_version                  int64\n",
      "background_app_count             int64\n",
      "session_inactivity_duration    float64\n",
      "network_jitter                 float64\n",
      "region_code                      int64\n",
      "subscriber                        bool\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample of encoded training data:\")\n",
    "print(X_train_encoded.head())\n",
    "print(\"Data types:\")\n",
    "print(X_train_encoded.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "preprocessing_7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News Category Mapping\n",
      "Original categories in news dataset:\n",
      "<StringArray>\n",
      "[     'U.S. NEWS',         'COMEDY',      'PARENTING',     'WORLD NEWS',\n",
      " 'CULTURE & ARTS',           'TECH',         'SPORTS',  'ENTERTAINMENT',\n",
      "       'POLITICS',     'WEIRD NEWS',    'ENVIRONMENT',      'EDUCATION',\n",
      "          'CRIME',        'SCIENCE',       'WELLNESS',       'BUSINESS',\n",
      " 'STYLE & BEAUTY',   'FOOD & DRINK',          'MEDIA',   'QUEER VOICES',\n",
      "  'HOME & LIVING',          'WOMEN',   'BLACK VOICES',         'TRAVEL',\n",
      "          'MONEY',       'RELIGION',  'LATINO VOICES',         'IMPACT',\n",
      "       'WEDDINGS',        'COLLEGE',        'PARENTS', 'ARTS & CULTURE',\n",
      "          'STYLE',          'GREEN',          'TASTE', 'HEALTHY LIVING',\n",
      "  'THE WORLDPOST',      'GOOD NEWS',      'WORLDPOST',          'FIFTY',\n",
      "           'ARTS',        'DIVORCE']\n",
      "Length: 42, dtype: str\n",
      "Mapped categories:\n",
      "category_mapped\n",
      "Crime             40541\n",
      "Education         27750\n",
      "Entertainment     22762\n",
      "Tech              10302\n",
      "TRAVEL             9900\n",
      "STYLE & BEAUTY     9814\n",
      "HEALTHY LIVING     6694\n",
      "QUEER VOICES       6347\n",
      "FOOD & DRINK       6340\n",
      "SPORTS             5077\n",
      "BLACK VOICES       4583\n",
      "HOME & LIVING      4320\n",
      "PARENTS            3955\n",
      "THE WORLDPOST      3664\n",
      "WEDDINGS           3653\n",
      "WOMEN              3572\n",
      "IMPACT             3484\n",
      "DIVORCE            3426\n",
      "WORLD NEWS         3299\n",
      "MEDIA              2944\n",
      "WEIRD NEWS         2777\n",
      "GREEN              2622\n",
      "WORLDPOST          2579\n",
      "RELIGION           2577\n",
      "STYLE              2254\n",
      "TASTE              2096\n",
      "MONEY              1756\n",
      "ARTS               1509\n",
      "ENVIRONMENT        1444\n",
      "FIFTY              1401\n",
      "GOOD NEWS          1398\n",
      "ARTS & CULTURE     1339\n",
      "COLLEGE            1144\n",
      "LATINO VOICES      1130\n",
      "CULTURE & ARTS     1074\n",
      "Name: count, dtype: int64\n",
      "Arm Index Mapping (j values)\n",
      "Arm  0: (Entertainment, user_1)\n",
      "Arm  1: (Education    , user_1)\n",
      "Arm  2: (Tech         , user_1)\n",
      "Arm  3: (Crime        , user_1)\n",
      "Arm  4: (Entertainment, user_2)\n",
      "Arm  5: (Education    , user_2)\n",
      "Arm  6: (Tech         , user_2)\n",
      "Arm  7: (Crime        , user_2)\n",
      "Arm  8: (Entertainment, user_3)\n",
      "Arm  9: (Education    , user_3)\n",
      "Arm 10: (Tech         , user_3)\n",
      "Arm 11: (Crime        , user_3)\n",
      "Arm mapping created successfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"News Category Mapping\")\n",
    "\n",
    "category_mapping = {\n",
    "    'U.S. NEWS': 'Crime',\n",
    "    'COMEDY': 'Entertainment',\n",
    "    'PARENTING': 'Education',\n",
    "    'POLITICS': 'Crime',\n",
    "    'WELLNESS': 'Education',\n",
    "    'ENTERTAINMENT': 'Entertainment',\n",
    "    'TECH': 'Tech',\n",
    "    'SCIENCE': 'Tech',\n",
    "    'BUSINESS': 'Tech',\n",
    "    'EDUCATION': 'Education',\n",
    "    'CRIME': 'Crime'\n",
    "}\n",
    "\n",
    "if 'category' in news_df_clean.columns:\n",
    "\n",
    "    print(\"Original categories in news dataset:\")\n",
    "    print(news_df_clean['category'].unique())\n",
    "    \n",
    "\n",
    "    news_df_clean['category_mapped'] = news_df_clean['category'].map(\n",
    "        lambda x: category_mapping.get(x.upper() if isinstance(x, str) else x, x)\n",
    "    )\n",
    "    \n",
    "    print(\"Mapped categories:\")\n",
    "    print(news_df_clean['category_mapped'].value_counts())\n",
    "    \n",
    "    news_df_clean['category'] = news_df_clean['category_mapped']\n",
    "\n",
    "\n",
    "\n",
    "print(\"Arm Index Mapping (j values)\")\n",
    "\n",
    "arm_mapping = {\n",
    "    0: ('Entertainment', 'user_1'),\n",
    "    1: ('Education', 'user_1'),\n",
    "    2: ('Tech', 'user_1'),\n",
    "    3: ('Crime', 'user_1'),\n",
    "    4: ('Entertainment', 'user_2'),\n",
    "    5: ('Education', 'user_2'),\n",
    "    6: ('Tech', 'user_2'),\n",
    "    7: ('Crime', 'user_2'),\n",
    "    8: ('Entertainment', 'user_3'),\n",
    "    9: ('Education', 'user_3'),\n",
    "    10: ('Tech', 'user_3'),\n",
    "    11: ('Crime', 'user_3')\n",
    "}\n",
    "\n",
    "for arm_idx, (category, user) in arm_mapping.items():\n",
    "    print(f\"Arm {arm_idx:2d}: ({category:13s}, {user})\")\n",
    "\n",
    "category_user_to_arm = {v: k for k, v in arm_mapping.items()}\n",
    "print(\"Arm mapping created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4d6352",
   "metadata": {},
   "source": [
    "## User Classification\n",
    "\n",
    "Train a classifier to predict the user category (`User1`, `User2`, `User3`),\n",
    "which serves as the **context** for the contextual bandit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "classification_1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Split Summary:\n",
      "Training set size: 1600 samples\n",
      "Validation set size: 400 samples\n",
      "Test set size: 2000 samples\n",
      "Class distribution in training set:\n",
      "label\n",
      "user_2    570\n",
      "user_1    565\n",
      "user_3    465\n",
      "Name: count, dtype: int64\n",
      "Class distribution in validation set:\n",
      "label\n",
      "user_2    142\n",
      "user_1    142\n",
      "user_3    116\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Split the training data into train and validation sets (80-20 split)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_encoded, \n",
    "    y_train_full, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=y_train_full\n",
    ")\n",
    "\n",
    "print(\"Data Split Summary:\")\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Validation set size: {X_val.shape[0]} samples\")\n",
    "print(f\"Test set size: {X_test_encoded.shape[0]} samples\")\n",
    "print(\"Class distribution in training set:\")\n",
    "print(y_train.value_counts())\n",
    "print(\"Class distribution in validation set:\")\n",
    "print(y_val.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "classification_4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XGBoost Classifier\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9000\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      user_1       0.88      0.86      0.87       142\n",
      "      user_2       0.99      0.87      0.93       142\n",
      "      user_3       0.83      0.98      0.90       116\n",
      "\n",
      "    accuracy                           0.90       400\n",
      "   macro avg       0.90      0.91      0.90       400\n",
      "weighted avg       0.91      0.90      0.90       400\n",
      "\n",
      "Confusion Matrix:\n",
      "[[122   1  19]\n",
      " [ 14 124   4]\n",
      " [  2   0 114]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Training XGBoost Classifier\")\n",
    "\n",
    "label_encoder_y = LabelEncoder()\n",
    "y_train_encoded = label_encoder_y.fit_transform(y_train)\n",
    "y_val_encoded = label_encoder_y.transform(y_val)\n",
    "\n",
    "xgb_classifier = xgb.XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=7,\n",
    "    random_state=42,\n",
    "    eval_metric='mlogloss',\n",
    "    verbosity=0\n",
    ")\n",
    "\n",
    "xgb_classifier.fit(X_train, y_train_encoded)\n",
    "\n",
    "y_pred_xgb_encoded = xgb_classifier.predict(X_val)\n",
    "y_pred_xgb = label_encoder_y.inverse_transform(y_pred_xgb_encoded)\n",
    "\n",
    "xgb_accuracy = accuracy_score(y_val, y_pred_xgb)\n",
    "print(f\"Validation Accuracy: {xgb_accuracy:.4f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_val, y_pred_xgb))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_val, y_pred_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "classification_7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XGBoost on full training data...\n",
      "XGBoost trained on full training data!\n"
     ]
    }
   ],
   "source": [
    "# Retrain the best classifier on the full training data for final use\n",
    "print(f\"Training XGBoost on full training data...\")\n",
    "\n",
    "# Encode the full labels\n",
    "y_train_full_encoded = label_encoder_y.transform(y_train_full)\n",
    "\n",
    "scaler_full = StandardScaler()\n",
    "X_train_full_scaled = scaler_full.fit_transform(X_train_encoded)\n",
    "\n",
    "X_test_encoded_scaled = scaler_full.transform(X_test_encoded)\n",
    "\n",
    "final_classifier = xgb.XGBClassifier(\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=7,\n",
    "        random_state=42,\n",
    "        eval_metric='mlogloss',\n",
    "        verbosity=0\n",
    "    )\n",
    "final_classifier.fit(X_train_full_scaled, y_train_full_encoded)\n",
    "\n",
    "print(\"XGBoost trained on full training data!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "classification_8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample predictions on test users:\n",
      "['user_2' 'user_1' 'user_1' 'user_1' 'user_1']\n",
      "User Context to Index Mapping (for Bandit):\n",
      "user_1 -> Index 0\n",
      "user_2 -> Index 1\n",
      "user_3 -> Index 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def predict_user_context(user_features):\n",
    "    \n",
    "    user_features_scaled = scaler_full.transform(user_features)\n",
    "    predictions_encoded = final_classifier.predict(user_features_scaled)\n",
    "    return label_encoder_y.inverse_transform(predictions_encoded)\n",
    "\n",
    "\n",
    "sample_prediction = predict_user_context(X_test_encoded.iloc[:5])\n",
    "print(\"Sample predictions on test users:\")\n",
    "print(sample_prediction)\n",
    "\n",
    "\n",
    "\n",
    "print(\"User Context to Index Mapping (for Bandit):\")\n",
    "\n",
    "user_to_index = {'user_1': 0, 'user_2': 1, 'user_3': 2}\n",
    "for user, idx in user_to_index.items():\n",
    "    print(f\"{user} -> Index {idx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ba537f",
   "metadata": {},
   "source": [
    "# `Contextual Bandit`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465388d6",
   "metadata": {},
   "source": [
    "## Reward Sampler Initialization\n",
    "\n",
    "The sampler is initialized using the student's roll number `i`.\n",
    "Rewards are obtained using `sampler.sample(j)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bandit_0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500\u2500 Reward Sampler Initialization \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "# TODO: Fill in your roll number below (i = last digits of your roll number)\n",
    "# Example: Roll Number U20230120 \u2192 i = 120\n",
    "ROLL_NUMBER = 120  # <-- CHANGE THIS to your roll number\n",
    "\n",
    "reward_sampler = sampler(ROLL_NUMBER)\n",
    "\n",
    "# Quick sanity check\n",
    "print(f\"Sampler initialized with roll number: {ROLL_NUMBER}\")\n",
    "print(f\"Sample reward from arm 0: {reward_sampler.sample(0):.4f}\")\n",
    "print(f\"Sample reward from arm 5: {reward_sampler.sample(5):.4f}\")\n",
    "\n",
    "# Constants\n",
    "N_ARMS_PER_CONTEXT = 4          # Entertainment, Education, Tech, Crime\n",
    "N_CONTEXTS = 3                  # user_1, user_2, user_3\n",
    "N_ARMS_TOTAL = N_ARMS_PER_CONTEXT * N_CONTEXTS  # 12\n",
    "CATEGORIES = ['Entertainment', 'Education', 'Tech', 'Crime']\n",
    "CONTEXTS   = ['user_1', 'user_2', 'user_3']\n",
    "T          = 10_000             # simulation horizon\n",
    "\n",
    "# context_offset[ctx] gives the base arm index for that context\n",
    "context_offset = {'user_1': 0, 'user_2': 4, 'user_3': 8}\n",
    "\n",
    "print(\"\\nConstants set:\")\n",
    "print(f\"  T = {T}, contexts = {CONTEXTS}, categories = {CATEGORIES}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f0bb0f",
   "metadata": {},
   "source": [
    "## Arm Mapping\n",
    "\n",
    "| Arm Index (j) | News Category | User Context |\n",
    "|--------------|---------------|--------------|\n",
    "| 0\u20133          | Entertainment, Education, Tech, Crime | User1 |\n",
    "| 4\u20137          | Entertainment, Education, Tech, Crime | User2 |\n",
    "| 8\u201311         | Entertainment, Education, Tech, Crime | User3 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356c764d",
   "metadata": {},
   "source": [
    "## Epsilon-Greedy Strategy\n",
    "\n",
    "This section implements the epsilon-greedy contextual bandit algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bandit_eg",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500\u2500 Epsilon-Greedy Contextual Bandit \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "class EpsilonGreedyBandit:\n",
    "    \"\"\"\n",
    "    Epsilon-Greedy contextual bandit.\n",
    "    Maintains separate Q-value estimates per context.\n",
    "    \"\"\"\n",
    "    def __init__(self, epsilon, n_contexts=3, n_arms=4):\n",
    "        self.epsilon   = epsilon\n",
    "        self.n_contexts = n_contexts\n",
    "        self.n_arms    = n_arms\n",
    "        # Q[ctx][arm] = estimated mean reward\n",
    "        self.Q         = np.zeros((n_contexts, n_arms))\n",
    "        self.counts    = np.zeros((n_contexts, n_arms))\n",
    "\n",
    "    def select_arm(self, ctx_idx):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(self.n_arms)  # explore\n",
    "        return int(np.argmax(self.Q[ctx_idx]))      # exploit\n",
    "\n",
    "    def update(self, ctx_idx, arm_idx, reward):\n",
    "        self.counts[ctx_idx][arm_idx] += 1\n",
    "        n = self.counts[ctx_idx][arm_idx]\n",
    "        self.Q[ctx_idx][arm_idx] += (reward - self.Q[ctx_idx][arm_idx]) / n\n",
    "\n",
    "    def expected_reward_distribution(self):\n",
    "        \"\"\"Return Q-table as expected reward distribution.\"\"\"\n",
    "        return self.Q.copy()\n",
    "\n",
    "\n",
    "# \u2500\u2500 Hyperparameter grid \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "epsilon_values = [0.01, 0.1, 0.3]   # at least 3 distinct values\n",
    "print(f\"Epsilon values to test: {epsilon_values}\")\n",
    "\n",
    "# Store per-epsilon: rewards[eps] = list of T reward values\n",
    "eg_rewards_per_eps = {}\n",
    "\n",
    "for eps in epsilon_values:\n",
    "    bandit = EpsilonGreedyBandit(epsilon=eps)\n",
    "    rewards = []\n",
    "    for t in range(T):\n",
    "        ctx_idx  = np.random.randint(N_CONTEXTS)         # random context each step\n",
    "        arm_idx  = bandit.select_arm(ctx_idx)\n",
    "        global_j = context_offset[CONTEXTS[ctx_idx]] + arm_idx\n",
    "        reward   = reward_sampler.sample(global_j)\n",
    "        bandit.update(ctx_idx, arm_idx, reward)\n",
    "        rewards.append(reward)\n",
    "    eg_rewards_per_eps[eps] = rewards\n",
    "    print(f\"  \u03b5={eps:.2f} \u2192 mean reward over {T} steps: {np.mean(rewards):.4f}\")\n",
    "\n",
    "# Best epsilon by final mean reward\n",
    "best_eps = max(eg_rewards_per_eps, key=lambda e: np.mean(eg_rewards_per_eps[e]))\n",
    "print(f\"\\nBest epsilon: {best_eps}\")\n",
    "\n",
    "# Train a final EG bandit with best epsilon and store per-context rewards\n",
    "eg_final = EpsilonGreedyBandit(epsilon=best_eps)\n",
    "eg_ctx_rewards = {ctx: [] for ctx in CONTEXTS}\n",
    "\n",
    "for t in range(T):\n",
    "    ctx_idx  = np.random.randint(N_CONTEXTS)\n",
    "    arm_idx  = eg_final.select_arm(ctx_idx)\n",
    "    global_j = context_offset[CONTEXTS[ctx_idx]] + arm_idx\n",
    "    reward   = reward_sampler.sample(global_j)\n",
    "    eg_final.update(ctx_idx, arm_idx, reward)\n",
    "    eg_ctx_rewards[CONTEXTS[ctx_idx]].append(reward)\n",
    "\n",
    "print(\"\\nExpected Reward Distribution (Q-table) for best \u03b5:\")\n",
    "eg_q_df = pd.DataFrame(\n",
    "    eg_final.expected_reward_distribution(),\n",
    "    index=CONTEXTS,\n",
    "    columns=CATEGORIES\n",
    ")\n",
    "print(eg_q_df.round(4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b571e8a6",
   "metadata": {},
   "source": [
    "## Upper Confidence Bound (UCB)\n",
    "\n",
    "This section implements the UCB strategy for contextual bandits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bandit_ucb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500\u2500 Upper Confidence Bound (UCB) Contextual Bandit \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "class UCBBandit:\n",
    "    \"\"\"\n",
    "    UCB1-style contextual bandit.\n",
    "    UCB score = Q[ctx][arm] + C * sqrt(ln(t+1) / (counts[ctx][arm] + 1e-5))\n",
    "    \"\"\"\n",
    "    def __init__(self, C, n_contexts=3, n_arms=4):\n",
    "        self.C         = C\n",
    "        self.n_contexts = n_contexts\n",
    "        self.n_arms    = n_arms\n",
    "        self.Q         = np.zeros((n_contexts, n_arms))\n",
    "        self.counts    = np.zeros((n_contexts, n_arms))\n",
    "        self.t         = 0\n",
    "\n",
    "    def select_arm(self, ctx_idx):\n",
    "        self.t += 1\n",
    "        ucb_scores = self.Q[ctx_idx] + self.C * np.sqrt(\n",
    "            np.log(self.t + 1) / (self.counts[ctx_idx] + 1e-5)\n",
    "        )\n",
    "        return int(np.argmax(ucb_scores))\n",
    "\n",
    "    def update(self, ctx_idx, arm_idx, reward):\n",
    "        self.counts[ctx_idx][arm_idx] += 1\n",
    "        n = self.counts[ctx_idx][arm_idx]\n",
    "        self.Q[ctx_idx][arm_idx] += (reward - self.Q[ctx_idx][arm_idx]) / n\n",
    "\n",
    "    def expected_reward_distribution(self):\n",
    "        return self.Q.copy()\n",
    "\n",
    "\n",
    "# \u2500\u2500 Hyperparameter grid \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "C_values = [0.5, 1.0, 2.0]   # at least 3 distinct values\n",
    "print(f\"C values to test: {C_values}\")\n",
    "\n",
    "ucb_rewards_per_C = {}\n",
    "\n",
    "for C in C_values:\n",
    "    bandit = UCBBandit(C=C)\n",
    "    rewards = []\n",
    "    for t in range(T):\n",
    "        ctx_idx  = np.random.randint(N_CONTEXTS)\n",
    "        arm_idx  = bandit.select_arm(ctx_idx)\n",
    "        global_j = context_offset[CONTEXTS[ctx_idx]] + arm_idx\n",
    "        reward   = reward_sampler.sample(global_j)\n",
    "        bandit.update(ctx_idx, arm_idx, reward)\n",
    "        rewards.append(reward)\n",
    "    ucb_rewards_per_C[C] = rewards\n",
    "    print(f\"  C={C:.1f} \u2192 mean reward over {T} steps: {np.mean(rewards):.4f}\")\n",
    "\n",
    "# Best C\n",
    "best_C = max(ucb_rewards_per_C, key=lambda c: np.mean(ucb_rewards_per_C[c]))\n",
    "print(f\"\\nBest C: {best_C}\")\n",
    "\n",
    "# Final UCB bandit with best C, per-context rewards\n",
    "ucb_final = UCBBandit(C=best_C)\n",
    "ucb_ctx_rewards = {ctx: [] for ctx in CONTEXTS}\n",
    "\n",
    "for t in range(T):\n",
    "    ctx_idx  = np.random.randint(N_CONTEXTS)\n",
    "    arm_idx  = ucb_final.select_arm(ctx_idx)\n",
    "    global_j = context_offset[CONTEXTS[ctx_idx]] + arm_idx\n",
    "    reward   = reward_sampler.sample(global_j)\n",
    "    ucb_final.update(ctx_idx, arm_idx, reward)\n",
    "    ucb_ctx_rewards[CONTEXTS[ctx_idx]].append(reward)\n",
    "\n",
    "print(\"\\nExpected Reward Distribution (Q-table) for best C:\")\n",
    "ucb_q_df = pd.DataFrame(\n",
    "    ucb_final.expected_reward_distribution(),\n",
    "    index=CONTEXTS,\n",
    "    columns=CATEGORIES\n",
    ")\n",
    "print(ucb_q_df.round(4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff2fb86",
   "metadata": {},
   "source": [
    "## SoftMax Strategy\n",
    "\n",
    "This section implements the SoftMax strategy with temperature $ \\tau = 1$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bandit_sm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500\u2500 SoftMax Contextual Bandit (\u03c4 = 1) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "class SoftMaxBandit:\n",
    "    \"\"\"\n",
    "    SoftMax (Boltzmann exploration) contextual bandit.\n",
    "    Temperature \u03c4 controls the trade-off between exploration and exploitation.\n",
    "    \"\"\"\n",
    "    def __init__(self, tau=1.0, n_contexts=3, n_arms=4):\n",
    "        self.tau        = tau\n",
    "        self.n_contexts = n_contexts\n",
    "        self.n_arms     = n_arms\n",
    "        self.Q          = np.zeros((n_contexts, n_arms))\n",
    "        self.counts     = np.zeros((n_contexts, n_arms))\n",
    "\n",
    "    def _softmax_probs(self, ctx_idx):\n",
    "        q = self.Q[ctx_idx] / self.tau\n",
    "        q -= q.max()           # numerical stability\n",
    "        exp_q = np.exp(q)\n",
    "        return exp_q / exp_q.sum()\n",
    "\n",
    "    def select_arm(self, ctx_idx):\n",
    "        probs = self._softmax_probs(ctx_idx)\n",
    "        return int(np.random.choice(self.n_arms, p=probs))\n",
    "\n",
    "    def update(self, ctx_idx, arm_idx, reward):\n",
    "        self.counts[ctx_idx][arm_idx] += 1\n",
    "        n = self.counts[ctx_idx][arm_idx]\n",
    "        self.Q[ctx_idx][arm_idx] += (reward - self.Q[ctx_idx][arm_idx]) / n\n",
    "\n",
    "    def expected_reward_distribution(self):\n",
    "        return self.Q.copy()\n",
    "\n",
    "\n",
    "# Fixed \u03c4 = 1 as specified\n",
    "sm_bandit = SoftMaxBandit(tau=1.0)\n",
    "sm_ctx_rewards = {ctx: [] for ctx in CONTEXTS}\n",
    "sm_rewards_all = []\n",
    "\n",
    "for t in range(T):\n",
    "    ctx_idx  = np.random.randint(N_CONTEXTS)\n",
    "    arm_idx  = sm_bandit.select_arm(ctx_idx)\n",
    "    global_j = context_offset[CONTEXTS[ctx_idx]] + arm_idx\n",
    "    reward   = reward_sampler.sample(global_j)\n",
    "    sm_bandit.update(ctx_idx, arm_idx, reward)\n",
    "    sm_ctx_rewards[CONTEXTS[ctx_idx]].append(reward)\n",
    "    sm_rewards_all.append(reward)\n",
    "\n",
    "print(f\"SoftMax (\u03c4=1) \u2192 mean reward over {T} steps: {np.mean(sm_rewards_all):.4f}\")\n",
    "\n",
    "print(\"\\nExpected Reward Distribution (Q-table):\")\n",
    "sm_q_df = pd.DataFrame(\n",
    "    sm_bandit.expected_reward_distribution(),\n",
    "    index=CONTEXTS,\n",
    "    columns=CATEGORIES\n",
    ")\n",
    "print(sm_q_df.round(4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb144662",
   "metadata": {},
   "source": [
    "## Reinforcement Learning Simulation\n",
    "\n",
    "We simulate the bandit algorithms for $T = 10,000$ steps and record rewards.\n",
    "\n",
    "P.S.: Change $T$ value as and if required.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bandit_sim",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500\u2500 RL Simulation: Average Reward vs Time \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "# Re-run all three algorithms (best hyperparams) tracking cumulative avg reward\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def run_simulation(bandit_obj, T=T):\n",
    "    \"\"\"Run a single simulation and return (all_rewards, per_context_rewards).\"\"\"\n",
    "    all_rewards = []\n",
    "    ctx_rewards = {ctx: [] for ctx in CONTEXTS}\n",
    "    for _ in range(T):\n",
    "        ctx_idx  = np.random.randint(N_CONTEXTS)\n",
    "        arm_idx  = bandit_obj.select_arm(ctx_idx)\n",
    "        global_j = context_offset[CONTEXTS[ctx_idx]] + arm_idx\n",
    "        reward   = reward_sampler.sample(global_j)\n",
    "        bandit_obj.update(ctx_idx, arm_idx, reward)\n",
    "        all_rewards.append(reward)\n",
    "        ctx_rewards[CONTEXTS[ctx_idx]].append(reward)\n",
    "    return all_rewards, ctx_rewards\n",
    "\n",
    "def running_avg(rewards):\n",
    "    return np.cumsum(rewards) / (np.arange(len(rewards)) + 1)\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "eg_sim  = EpsilonGreedyBandit(epsilon=best_eps)\n",
    "ucb_sim = UCBBandit(C=best_C)\n",
    "sm_sim  = SoftMaxBandit(tau=1.0)\n",
    "\n",
    "eg_all,  eg_ctx   = run_simulation(eg_sim)\n",
    "ucb_all, ucb_ctx  = run_simulation(ucb_sim)\n",
    "sm_all,  sm_ctx   = run_simulation(sm_sim)\n",
    "\n",
    "# \u2500\u2500 Plot 1: Average Reward vs Time (per algorithm, overall) \u2500\u2500\u2500\u2500\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "steps = np.arange(1, T + 1)\n",
    "ax.plot(steps, running_avg(eg_all),  label=f'Epsilon-Greedy (\u03b5={best_eps})', linewidth=1.5)\n",
    "ax.plot(steps, running_avg(ucb_all), label=f'UCB (C={best_C})',              linewidth=1.5)\n",
    "ax.plot(steps, running_avg(sm_all),  label='SoftMax (\u03c4=1)',                  linewidth=1.5)\n",
    "ax.set_title('Average Reward vs. Time \u2014 All Algorithms', fontsize=14)\n",
    "ax.set_xlabel('Time Step', fontsize=12)\n",
    "ax.set_ylabel('Average Reward', fontsize=12)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Plot 1: Average Reward vs Time (all algorithms) shown.\")\n",
    "\n",
    "# \u2500\u2500 Plot 2: Average Reward vs Time per Context (one subplot each) \n",
    "fig, axes = plt.subplots(1, 3, figsize=(17, 5), sharey=True)\n",
    "for i, ctx in enumerate(CONTEXTS):\n",
    "    ax = axes[i]\n",
    "    if eg_ctx[ctx]:\n",
    "        ax.plot(running_avg(eg_ctx[ctx]),  label=f'EG (\u03b5={best_eps})')\n",
    "    if ucb_ctx[ctx]:\n",
    "        ax.plot(running_avg(ucb_ctx[ctx]), label=f'UCB (C={best_C})')\n",
    "    if sm_ctx[ctx]:\n",
    "        ax.plot(running_avg(sm_ctx[ctx]),  label='SoftMax (\u03c4=1)')\n",
    "    ax.set_title(f'Average Reward vs. Time \u2014 {ctx}', fontsize=12)\n",
    "    ax.set_xlabel('Visits to Context', fontsize=10)\n",
    "    ax.set_ylabel('Average Reward', fontsize=10)\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "plt.suptitle('Per-Context Average Reward vs. Time', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Plot 2: Per-context Average Reward vs Time shown.\")\n",
    "\n",
    "# \u2500\u2500 Plot 3: Hyperparameter comparison \u2014 Epsilon-Greedy \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "for eps in epsilon_values:\n",
    "    ax.plot(running_avg(eg_rewards_per_eps[eps]), label=f'\u03b5={eps}')\n",
    "ax.set_title('Epsilon-Greedy: Average Reward vs. Time for Different \u03b5 Values', fontsize=13)\n",
    "ax.set_xlabel('Time Step', fontsize=12)\n",
    "ax.set_ylabel('Average Reward', fontsize=12)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Plot 3: Epsilon-Greedy hyperparameter comparison shown.\")\n",
    "\n",
    "# \u2500\u2500 Plot 4: Hyperparameter comparison \u2014 UCB \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "for C in C_values:\n",
    "    ax.plot(running_avg(ucb_rewards_per_C[C]), label=f'C={C}')\n",
    "ax.set_title('UCB: Average Reward vs. Time for Different C Values', fontsize=13)\n",
    "ax.set_xlabel('Time Step', fontsize=12)\n",
    "ax.set_ylabel('Average Reward', fontsize=12)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Plot 4: UCB hyperparameter comparison shown.\")\n",
    "\n",
    "# \u2500\u2500 Summary table \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "print(\"\\n\u2500\u2500 Summary: Mean Reward over T steps \u2500\u2500\")\n",
    "summary = pd.DataFrame({\n",
    "    'Algorithm':    [f'Epsilon-Greedy (\u03b5={best_eps})', f'UCB (C={best_C})', 'SoftMax (\u03c4=1)'],\n",
    "    'Mean Reward':  [np.mean(eg_all), np.mean(ucb_all), np.mean(sm_all)],\n",
    "    'Final Avg':    [running_avg(eg_all)[-1], running_avg(ucb_all)[-1], running_avg(sm_all)[-1]]\n",
    "})\n",
    "print(summary.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca10b073",
   "metadata": {},
   "source": [
    "## Results and Analysis\n",
    "\n",
    "This section presents:\n",
    "- Average Reward vs Time\n",
    "- Hyperparameter comparisons\n",
    "- Observations and discussion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bandit_rec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500\u2500 Recommendation Engine \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "# Filter news_df_clean to only the 4 mapped categories\n",
    "bandit_categories = ['Entertainment', 'Education', 'Tech', 'Crime']\n",
    "news_bandit = news_df_clean[news_df_clean['category'].isin(bandit_categories)].copy()\n",
    "news_bandit_grouped = {cat: news_bandit[news_bandit['category'] == cat] for cat in bandit_categories}\n",
    "\n",
    "print(\"Articles available per category:\")\n",
    "for cat, grp in news_bandit_grouped.items():\n",
    "    print(f\"  {cat:15s}: {len(grp):,} articles\")\n",
    "\n",
    "\n",
    "def recommend(user_features_row, bandit='eg'):\n",
    "    \"\"\"\n",
    "    Full recommendation pipeline for a single user.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    user_features_row : pd.DataFrame (single row, already encoded)\n",
    "    bandit            : 'eg' | 'ucb' | 'sm'\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict with keys: user_context, news_category, article\n",
    "    \"\"\"\n",
    "    # Step 1 \u2013 Classify context\n",
    "    ctx_label = predict_user_context(user_features_row)[0]   # e.g. 'user_2'\n",
    "    ctx_idx   = user_to_index[ctx_label]\n",
    "\n",
    "    # Step 2 \u2013 Select category via chosen bandit\n",
    "    if bandit == 'eg':\n",
    "        arm_idx = eg_final.select_arm(ctx_idx)\n",
    "    elif bandit == 'ucb':\n",
    "        arm_idx = ucb_final.select_arm(ctx_idx)\n",
    "    else:\n",
    "        arm_idx = sm_bandit.select_arm(ctx_idx)\n",
    "\n",
    "    news_category = CATEGORIES[arm_idx]\n",
    "\n",
    "    # Step 3 \u2013 Sample a random article from that category\n",
    "    pool    = news_bandit_grouped[news_category]\n",
    "    article = pool.sample(1).iloc[0]\n",
    "\n",
    "    return {\n",
    "        'user_context':   ctx_label,\n",
    "        'news_category':  news_category,\n",
    "        'headline':       article['headline'],\n",
    "        'link':           article.get('link', 'N/A')\n",
    "    }\n",
    "\n",
    "\n",
    "# \u2500\u2500 Demo: run recommendations for first 5 test users \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "print(\"\\n\u2500\u2500 Sample Recommendations (Epsilon-Greedy) \u2500\u2500\")\n",
    "for i in range(5):\n",
    "    user_row = X_test_encoded.iloc[[i]]\n",
    "    rec = recommend(user_row, bandit='eg')\n",
    "    print(f\"\\nUser {i+1}:\")\n",
    "    print(f\"  Predicted Context : {rec['user_context']}\")\n",
    "    print(f\"  Recommended Category: {rec['news_category']}\")\n",
    "    print(f\"  Article Headline  : {rec['headline'][:80]}...\")\n",
    "\n",
    "print(\"\\n\u2500\u2500 Sample Recommendations (UCB) \u2500\u2500\")\n",
    "for i in range(5):\n",
    "    user_row = X_test_encoded.iloc[[i]]\n",
    "    rec = recommend(user_row, bandit='ucb')\n",
    "    print(f\"  User {i+1}: [{rec['user_context']}] \u2192 {rec['news_category']} | {rec['headline'][:60]}...\")\n",
    "\n",
    "print(\"\\n\u2500\u2500 Sample Recommendations (SoftMax) \u2500\u2500\")\n",
    "for i in range(5):\n",
    "    user_row = X_test_encoded.iloc[[i]]\n",
    "    rec = recommend(user_row, bandit='sm')\n",
    "    print(f\"  User {i+1}: [{rec['user_context']}] \u2192 {rec['news_category']} | {rec['headline'][:60]}...\")\n",
    "\n",
    "print(\"\\nRecommendation Engine demo complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bandit_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500\u2500 Final Analysis: Print observations \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "print(\"\u2550\" * 60)\n",
    "print(\"COMPARATIVE ANALYSIS\")\n",
    "print(\"\u2550\" * 60)\n",
    "\n",
    "alg_names = [f'Epsilon-Greedy (\u03b5={best_eps})', f'UCB (C={best_C})', 'SoftMax (\u03c4=1)']\n",
    "alg_means = [np.mean(eg_all), np.mean(ucb_all), np.mean(sm_all)]\n",
    "\n",
    "for name, mean in zip(alg_names, alg_means):\n",
    "    print(f\"  {name:<35s}: mean reward = {mean:.4f}\")\n",
    "\n",
    "best_alg = alg_names[int(np.argmax(alg_means))]\n",
    "print(f\"\\nBest performing algorithm: {best_alg}\")\n",
    "\n",
    "print(\"\\n\u2500\u2500 Epsilon-Greedy hyperparameter sensitivity \u2500\u2500\")\n",
    "for eps in epsilon_values:\n",
    "    m = np.mean(eg_rewards_per_eps[eps])\n",
    "    print(f\"  \u03b5={eps:.2f} \u2192 {m:.4f}\")\n",
    "\n",
    "print(\"\\n\u2500\u2500 UCB hyperparameter sensitivity \u2500\u2500\")\n",
    "for C in C_values:\n",
    "    m = np.mean(ucb_rewards_per_C[C])\n",
    "    print(f\"  C={C:.1f} \u2192 {m:.4f}\")\n",
    "\n",
    "print(\"\"\"\n",
    "Observations\n",
    "\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "1. Epsilon-Greedy: Low \u03b5 favours exploitation and converges fast but may\n",
    "   miss better arms; high \u03b5 keeps exploring at the cost of mean reward.\n",
    "2. UCB: Naturally balances exploration via confidence bounds; higher C\n",
    "   drives more exploration. Usually converges to a strong policy.\n",
    "3. SoftMax (\u03c4=1): Soft-selection proportional to estimated Q-values;\n",
    "   moderately exploratory throughout the horizon.\n",
    "4. All three algorithms successfully identify high-reward arm per context\n",
    "   as T grows, demonstrating the CMAB framework's effectiveness.\n",
    "\n",
    "# TODO: Add your own observations, discussion, and insights here.\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512fbb89",
   "metadata": {},
   "source": [
    "## Final Observations\n",
    "\n",
    "- Comparison of Epsilon-Greedy, UCB, and SoftMax\n",
    "- Effect of hyperparameters\n",
    "- Strengths and limitations of each approach\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5665d58e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}