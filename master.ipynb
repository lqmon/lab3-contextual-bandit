{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "999bcd7d",
   "metadata": {},
   "source": [
    "# Lab 3: Contextual Bandit-Based News Article Recommendation\n",
    "\n",
    "**`Course`:** Reinforcement Learning Fundamentals  \n",
    "**`Student Name`:**  \n",
    "**`Roll Number`:**  \n",
    "**`GitHub Branch`:** firstname_U20230xxx  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755efd7a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Imports and Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bc73a3-aed3-45ef-9d54-b6d7085f3410",
   "metadata": {},
   "source": [
    "# Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a44cc661-3c25-4ec6-af7c-9023d1d52aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from rlcmab_sampler import sampler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c638ba06",
   "metadata": {},
   "source": [
    "# Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48f6f6a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                link  \\\n",
      "0  https://www.huffpost.com/entry/covid-boosters-...   \n",
      "1  https://www.huffpost.com/entry/american-airlin...   \n",
      "2  https://www.huffpost.com/entry/funniest-tweets...   \n",
      "3  https://www.huffpost.com/entry/funniest-parent...   \n",
      "4  https://www.huffpost.com/entry/amy-cooper-lose...   \n",
      "\n",
      "                                            headline   category  \\\n",
      "0  Over 4 Million Americans Roll Up Sleeves For O...  U.S. NEWS   \n",
      "1  American Airlines Flyer Charged, Banned For Li...  U.S. NEWS   \n",
      "2  23 Of The Funniest Tweets About Cats And Dogs ...     COMEDY   \n",
      "3  The Funniest Tweets From Parents This Week (Se...  PARENTING   \n",
      "4  Woman Who Called Cops On Black Bird-Watcher Lo...  U.S. NEWS   \n",
      "\n",
      "                                   short_description               authors  \\\n",
      "0  Health experts said it is too early to predict...  Carla K. Johnson, AP   \n",
      "1  He was subdued by passengers and crew when he ...        Mary Papenfuss   \n",
      "2  \"Until you have a dog you don't understand wha...         Elyse Wanshel   \n",
      "3  \"Accidentally put grown-up toothpaste on my to...      Caroline Bologna   \n",
      "4  Amy Cooper accused investment firm Franklin Te...        Nina Golgowski   \n",
      "\n",
      "         date  \n",
      "0  2022-09-23  \n",
      "1  2022-09-23  \n",
      "2  2022-09-23  \n",
      "3  2022-09-23  \n",
      "4  2022-09-22  \n",
      "  user_id   age  income  clicks  purchase_amount  session_duration  \\\n",
      "0   U7392   NaN   23053      10           500.00             17.34   \n",
      "1   U2702  56.0   20239      11           913.33             22.22   \n",
      "2   U2461   NaN   13907       9          1252.62             41.57   \n",
      "3   U7475   NaN   26615      12           500.00             30.17   \n",
      "4   U6040  32.0   27958      13           500.00             65.27   \n",
      "\n",
      "   content_variety  engagement_score  num_transactions  avg_monthly_spend  \\\n",
      "0          0.36661          37.29781                 3             187.44   \n",
      "1          0.61370          59.36342                 5             145.15   \n",
      "2          0.80368          76.78706                 7             282.03   \n",
      "3          0.26499          30.19441                10             195.35   \n",
      "4          0.36385          37.12153                 5             439.68   \n",
      "\n",
      "   ...  screen_brightness  battery_percentage  cart_abandonment_count  \\\n",
      "0  ...                4.0                 2.0                       8   \n",
      "1  ...                4.5                63.0                       5   \n",
      "2  ...                1.3                22.0                       2   \n",
      "3  ...                4.2                77.0                       9   \n",
      "4  ...                4.6                30.0                       9   \n",
      "\n",
      "   browser_version  background_app_count  session_inactivity_duration  \\\n",
      "0          3.17.97                    10                        22.75   \n",
      "1          1.57.10                     8                         1.75   \n",
      "2          2.16.94                    12                        29.33   \n",
      "3          9.90.20                     4                        21.61   \n",
      "4          1.99.38                     7                         7.58   \n",
      "\n",
      "   network_jitter  region_code  subscriber   label  \n",
      "0             8.0         Z999       False  user_3  \n",
      "1             4.0         U428        True  user_2  \n",
      "2            18.0         Z999        True  user_3  \n",
      "3            22.0         X123       False  user_3  \n",
      "4            52.0         S043       False  user_1  \n",
      "\n",
      "[5 rows x 33 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "news_df = pd.read_csv(\"data/news_articles.csv\")\n",
    "train_users = pd.read_csv(\"data/train_users.csv\")\n",
    "test_users = pd.read_csv(\"data/test_users.csv\")\n",
    "\n",
    "print(news_df.head())\n",
    "print(train_users.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1c5cb1",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "In this section:\n",
    "- Handle missing values\n",
    "- Encode categorical features\n",
    "- Prepare data for user classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7195ba1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "1. LOADING DATASETS\n",
      "================================================================================\n",
      "\n",
      "News Articles Dataset Shape: (209527, 6)\n",
      "Train Users Dataset Shape: (2000, 33)\n",
      "Test Users Dataset Shape: (2000, 32)\n",
      "\n",
      "News Articles Columns: ['link', 'headline', 'category', 'short_description', 'authors', 'date']\n",
      "Train Users Columns: ['user_id', 'age', 'income', 'clicks', 'purchase_amount', 'session_duration', 'content_variety', 'engagement_score', 'num_transactions', 'avg_monthly_spend', 'avg_cart_value', 'browsing_depth', 'revisit_rate', 'scroll_activity', 'time_on_site', 'interaction_count', 'preferred_price_range', 'discount_usage_rate', 'wishlist_size', 'product_views', 'repeat_purchase_gap (days)', 'churn_risk_score', 'loyalty_index', 'screen_brightness', 'battery_percentage', 'cart_abandonment_count', 'browser_version', 'background_app_count', 'session_inactivity_duration', 'network_jitter', 'region_code', 'subscriber', 'label']\n",
      "Test Users Columns: ['user_id', 'age', 'income', 'clicks', 'purchase_amount', 'session_duration', 'content_variety', 'engagement_score', 'num_transactions', 'avg_monthly_spend', 'avg_cart_value', 'browsing_depth', 'revisit_rate', 'scroll_activity', 'time_on_site', 'interaction_count', 'preferred_price_range', 'discount_usage_rate', 'wishlist_size', 'product_views', 'repeat_purchase_gap (days)', 'churn_risk_score', 'loyalty_index', 'screen_brightness', 'battery_percentage', 'cart_abandonment_count', 'browser_version', 'background_app_count', 'session_inactivity_duration', 'network_jitter', 'region_code', 'subscriber']\n",
      "\n",
      "================================================================================\n",
      "2. DATA CLEANING - HANDLING MISSING VALUES\n",
      "================================================================================\n",
      "\n",
      "Missing values in News Articles:\n",
      "link                     0\n",
      "headline                 6\n",
      "category                 0\n",
      "short_description    19712\n",
      "authors              37418\n",
      "date                     0\n",
      "dtype: int64\n",
      "Total missing: 57136\n",
      "\n",
      "Missing values in Train Users:\n",
      "user_id                          0\n",
      "age                            698\n",
      "income                           0\n",
      "clicks                           0\n",
      "purchase_amount                  0\n",
      "session_duration                 0\n",
      "content_variety                  0\n",
      "engagement_score                 0\n",
      "num_transactions                 0\n",
      "avg_monthly_spend                0\n",
      "avg_cart_value                   0\n",
      "browsing_depth                   0\n",
      "revisit_rate                     0\n",
      "scroll_activity                  0\n",
      "time_on_site                     0\n",
      "interaction_count                0\n",
      "preferred_price_range            0\n",
      "discount_usage_rate              0\n",
      "wishlist_size                    0\n",
      "product_views                    0\n",
      "repeat_purchase_gap (days)       0\n",
      "churn_risk_score                 0\n",
      "loyalty_index                    0\n",
      "screen_brightness                0\n",
      "battery_percentage               0\n",
      "cart_abandonment_count           0\n",
      "browser_version                  0\n",
      "background_app_count             0\n",
      "session_inactivity_duration      0\n",
      "network_jitter                   0\n",
      "region_code                      0\n",
      "subscriber                       0\n",
      "label                            0\n",
      "dtype: int64\n",
      "Total missing: 698\n",
      "\n",
      "Missing values in Test Users:\n",
      "user_id                          0\n",
      "age                            679\n",
      "income                           0\n",
      "clicks                           0\n",
      "purchase_amount                  0\n",
      "session_duration                 0\n",
      "content_variety                  0\n",
      "engagement_score                 0\n",
      "num_transactions                 0\n",
      "avg_monthly_spend                0\n",
      "avg_cart_value                   0\n",
      "browsing_depth                   0\n",
      "revisit_rate                     0\n",
      "scroll_activity                  0\n",
      "time_on_site                     0\n",
      "interaction_count                0\n",
      "preferred_price_range            0\n",
      "discount_usage_rate              0\n",
      "wishlist_size                    0\n",
      "product_views                    0\n",
      "repeat_purchase_gap (days)       0\n",
      "churn_risk_score                 0\n",
      "loyalty_index                    0\n",
      "screen_brightness                0\n",
      "battery_percentage               0\n",
      "cart_abandonment_count           0\n",
      "browser_version                  0\n",
      "background_app_count             0\n",
      "session_inactivity_duration      0\n",
      "network_jitter                   0\n",
      "region_code                      0\n",
      "subscriber                       0\n",
      "dtype: int64\n",
      "Total missing: 679\n",
      "\n",
      "Missing values after cleaning:\n",
      "News Articles: 6\n",
      "Train Users: 0\n",
      "Test Users: 0\n",
      "\n",
      "================================================================================\n",
      "3. FEATURE ENCODING FOR CLASSIFICATION AND BANDIT TRAINING\n",
      "================================================================================\n",
      "\n",
      "Encoding user labels...\n",
      "Original labels: <StringArray>\n",
      "['user_3', 'user_2', 'user_1']\n",
      "Length: 3, dtype: str\n",
      "Encoded labels: [2 1 0]\n",
      "Mapping: {'user_1': np.int64(0), 'user_2': np.int64(1), 'user_3': np.int64(2)}\n",
      "‚ö†Ô∏è Note: 'label' column not in test_users.csv (test labels will be predicted)\n",
      "\n",
      "Encoding news article categories...\n",
      "Original categories: <StringArray>\n",
      "[     'U.S. NEWS',         'COMEDY',      'PARENTING',     'WORLD NEWS',\n",
      " 'CULTURE & ARTS',           'TECH',         'SPORTS',  'ENTERTAINMENT',\n",
      "       'POLITICS',     'WEIRD NEWS',    'ENVIRONMENT',      'EDUCATION',\n",
      "          'CRIME',        'SCIENCE',       'WELLNESS',       'BUSINESS',\n",
      " 'STYLE & BEAUTY',   'FOOD & DRINK',          'MEDIA',   'QUEER VOICES',\n",
      "  'HOME & LIVING',          'WOMEN',   'BLACK VOICES',         'TRAVEL',\n",
      "          'MONEY',       'RELIGION',  'LATINO VOICES',         'IMPACT',\n",
      "       'WEDDINGS',        'COLLEGE',        'PARENTS', 'ARTS & CULTURE',\n",
      "          'STYLE',          'GREEN',          'TASTE', 'HEALTHY LIVING',\n",
      "  'THE WORLDPOST',      'GOOD NEWS',      'WORLDPOST',          'FIFTY',\n",
      "           'ARTS',        'DIVORCE']\n",
      "Length: 42, dtype: str\n",
      "Encoded categories: [35  5 22 40  7 32 28 10 24 37 11  9  6 27 38  3 30 13 20 25 17 39  2 34\n",
      " 21 26 19 18 36  4 23  1 29 15 31 16 33 14 41 12  0  8]\n",
      "Mapping: {'ARTS': np.int64(0), 'ARTS & CULTURE': np.int64(1), 'BLACK VOICES': np.int64(2), 'BUSINESS': np.int64(3), 'COLLEGE': np.int64(4), 'COMEDY': np.int64(5), 'CRIME': np.int64(6), 'CULTURE & ARTS': np.int64(7), 'DIVORCE': np.int64(8), 'EDUCATION': np.int64(9), 'ENTERTAINMENT': np.int64(10), 'ENVIRONMENT': np.int64(11), 'FIFTY': np.int64(12), 'FOOD & DRINK': np.int64(13), 'GOOD NEWS': np.int64(14), 'GREEN': np.int64(15), 'HEALTHY LIVING': np.int64(16), 'HOME & LIVING': np.int64(17), 'IMPACT': np.int64(18), 'LATINO VOICES': np.int64(19), 'MEDIA': np.int64(20), 'MONEY': np.int64(21), 'PARENTING': np.int64(22), 'PARENTS': np.int64(23), 'POLITICS': np.int64(24), 'QUEER VOICES': np.int64(25), 'RELIGION': np.int64(26), 'SCIENCE': np.int64(27), 'SPORTS': np.int64(28), 'STYLE': np.int64(29), 'STYLE & BEAUTY': np.int64(30), 'TASTE': np.int64(31), 'TECH': np.int64(32), 'THE WORLDPOST': np.int64(33), 'TRAVEL': np.int64(34), 'U.S. NEWS': np.int64(35), 'WEDDINGS': np.int64(36), 'WEIRD NEWS': np.int64(37), 'WELLNESS': np.int64(38), 'WOMEN': np.int64(39), 'WORLD NEWS': np.int64(40), 'WORLDPOST': np.int64(41)}\n",
      "\n",
      "Preparing feature sets for user classification...\n",
      "\n",
      "Train Features Shape: (2000, 4)\n",
      "Train Labels Shape: (2000,)\n",
      "Test Features Shape: (2000, 4)\n",
      "Test Labels: Will be predicted using classifier\n",
      "\n",
      "Features normalized using StandardScaler\n",
      "Train features stats after scaling:\n",
      "  Mean: [-3.41948692e-16  2.48689958e-16  3.90798505e-17  1.27897692e-16]\n",
      "  Std: [1. 1. 1. 1.]\n",
      "\n",
      "================================================================================\n",
      "PREPROCESSING SUMMARY\n",
      "================================================================================\n",
      "\n",
      "‚úì Loaded 3 datasets successfully\n",
      "‚úì Handled missing values in all datasets\n",
      "‚úì Encoded categorical features (user labels and article categories)\n",
      "‚úì Normalized numerical features for ML training\n",
      "\n",
      "Ready for user classification and contextual bandit training!\n"
     ]
    }
   ],
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "# 1. Load the provided user and article datasets\n",
    "print(\"=\" * 80)\n",
    "print(\"1. LOADING DATASETS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "news_df = pd.read_csv(\"data/news_articles.csv\")\n",
    "train_users_df = pd.read_csv(\"data/train_users.csv\")\n",
    "test_users_df = pd.read_csv(\"data/test_users.csv\")\n",
    "\n",
    "print(f\"\\nNews Articles Dataset Shape: {news_df.shape}\")\n",
    "print(f\"Train Users Dataset Shape: {train_users_df.shape}\")\n",
    "print(f\"Test Users Dataset Shape: {test_users_df.shape}\")\n",
    "\n",
    "print(\"\\nNews Articles Columns:\", news_df.columns.tolist())\n",
    "print(\"Train Users Columns:\", train_users_df.columns.tolist())\n",
    "print(\"Test Users Columns:\", test_users_df.columns.tolist())\n",
    "\n",
    "# 2. Data Cleaning - Handle Missing Values\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"2. DATA CLEANING - HANDLING MISSING VALUES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check for missing values in each dataset\n",
    "print(\"\\nMissing values in News Articles:\")\n",
    "print(news_df.isnull().sum())\n",
    "print(f\"Total missing: {news_df.isnull().sum().sum()}\")\n",
    "\n",
    "print(\"\\nMissing values in Train Users:\")\n",
    "print(train_users_df.isnull().sum())\n",
    "print(f\"Total missing: {train_users_df.isnull().sum().sum()}\")\n",
    "\n",
    "print(\"\\nMissing values in Test Users:\")\n",
    "print(test_users_df.isnull().sum())\n",
    "print(f\"Total missing: {test_users_df.isnull().sum().sum()}\")\n",
    "\n",
    "# Handle missing values in news articles\n",
    "# Fill missing authors with 'Unknown'\n",
    "news_df['authors'] = news_df['authors'].fillna('Unknown')\n",
    "\n",
    "# Fill missing short_description with empty string\n",
    "news_df['short_description'] = news_df['short_description'].fillna('')\n",
    "\n",
    "# Fill missing date with mode (most frequent date)\n",
    "if news_df['date'].isnull().sum() > 0:\n",
    "    news_df['date'] = news_df['date'].fillna(news_df['date'].mode()[0] if not news_df['date'].mode().empty else 'Unknown')\n",
    "\n",
    "# Handle missing values in user datasets (if any)\n",
    "# Fill numerical columns with median\n",
    "train_users_df['age'] = train_users_df['age'].fillna(train_users_df['age'].median())\n",
    "train_users_df['income'] = train_users_df['income'].fillna(train_users_df['income'].median())\n",
    "train_users_df['clicks'] = train_users_df['clicks'].fillna(train_users_df['clicks'].median())\n",
    "train_users_df['purchase_amount'] = train_users_df['purchase_amount'].fillna(train_users_df['purchase_amount'].median())\n",
    "\n",
    "test_users_df['age'] = test_users_df['age'].fillna(test_users_df['age'].median())\n",
    "test_users_df['income'] = test_users_df['income'].fillna(test_users_df['income'].median())\n",
    "test_users_df['clicks'] = test_users_df['clicks'].fillna(test_users_df['clicks'].median())\n",
    "test_users_df['purchase_amount'] = test_users_df['purchase_amount'].fillna(test_users_df['purchase_amount'].median())\n",
    "\n",
    "print(\"\\nMissing values after cleaning:\")\n",
    "print(f\"News Articles: {news_df.isnull().sum().sum()}\")\n",
    "print(f\"Train Users: {train_users_df.isnull().sum().sum()}\")\n",
    "print(f\"Test Users: {test_users_df.isnull().sum().sum()}\")\n",
    "\n",
    "# 3. Feature Encoding for Classification and Bandit Training\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"3. FEATURE ENCODING FOR CLASSIFICATION AND BANDIT TRAINING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Encode categorical labels in user datasets (Convert user categories to numerical)\n",
    "print(\"\\nEncoding user labels...\")\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Check if 'label' column exists in train_users_df\n",
    "if 'label' in train_users_df.columns:\n",
    "    train_users_df['label_encoded'] = label_encoder.fit_transform(train_users_df['label'])\n",
    "    print(f\"Original labels: {train_users_df['label'].unique()}\")\n",
    "    print(f\"Encoded labels: {train_users_df['label_encoded'].unique()}\")\n",
    "    print(f\"Mapping: {dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Warning: 'label' column not found in train_users_df\")\n",
    "\n",
    "# For test_users, only encode if 'label' column exists\n",
    "if 'label' in test_users_df.columns:\n",
    "    test_users_df['label_encoded'] = label_encoder.transform(test_users_df['label'])\n",
    "    print(f\"\\nTest labels encoded successfully\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Note: 'label' column not in test_users.csv (test labels will be predicted)\")\n",
    "\n",
    "# Encode news article categories\n",
    "print(\"\\nEncoding news article categories...\")\n",
    "news_category_encoder = LabelEncoder()\n",
    "news_df['category_encoded'] = news_category_encoder.fit_transform(news_df['category'])\n",
    "\n",
    "print(f\"Original categories: {news_df['category'].unique()}\")\n",
    "print(f\"Encoded categories: {news_df['category_encoded'].unique()}\")\n",
    "print(f\"Mapping: {dict(zip(news_category_encoder.classes_, news_category_encoder.transform(news_category_encoder.classes_)))}\")\n",
    "\n",
    "# Prepare feature sets for user classification\n",
    "print(\"\\nPreparing feature sets for user classification...\")\n",
    "\n",
    "# Numerical features\n",
    "feature_columns = ['age', 'income', 'clicks', 'purchase_amount']\n",
    "\n",
    "# Create train feature matrix\n",
    "X_train = train_users_df[feature_columns].copy()\n",
    "y_train = train_users_df['label_encoded'].copy() if 'label_encoded' in train_users_df.columns else None\n",
    "\n",
    "# Create test feature matrix\n",
    "X_test = test_users_df[feature_columns].copy()\n",
    "# For test labels, use encoded labels only if they exist, otherwise we'll predict them\n",
    "y_test = test_users_df['label_encoded'].copy() if 'label_encoded' in test_users_df.columns else None\n",
    "\n",
    "if y_train is not None:\n",
    "    print(f\"\\nTrain Features Shape: {X_train.shape}\")\n",
    "    print(f\"Train Labels Shape: {y_train.shape}\")\n",
    "else:\n",
    "    print(f\"\\nTrain Features Shape: {X_train.shape}\")\n",
    "    print(f\"Train Labels: Not available\")\n",
    "\n",
    "if y_test is not None:\n",
    "    print(f\"Test Features Shape: {X_test.shape}\")\n",
    "    print(f\"Test Labels Shape: {y_test.shape}\")\n",
    "else:\n",
    "    print(f\"Test Features Shape: {X_test.shape}\")\n",
    "    print(f\"Test Labels: Will be predicted using classifier\")\n",
    "\n",
    "# Normalize numerical features for better classification performance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"\\nFeatures normalized using StandardScaler\")\n",
    "print(f\"Train features stats after scaling:\")\n",
    "print(f\"  Mean: {X_train_scaled.mean(axis=0)}\")\n",
    "print(f\"  Std: {X_train_scaled.std(axis=0)}\")\n",
    "\n",
    "# Summary of preprocessed data\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PREPROCESSING SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n‚úì Loaded 3 datasets successfully\")\n",
    "print(f\"‚úì Handled missing values in all datasets\")\n",
    "print(f\"‚úì Encoded categorical features (user labels and article categories)\")\n",
    "print(f\"‚úì Normalized numerical features for ML training\")\n",
    "print(f\"\\nReady for user classification and contextual bandit training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4d6352",
   "metadata": {},
   "source": [
    "## User Classification\n",
    "\n",
    "Train a classifier to predict the user category (`User1`, `User2`, `User3`),\n",
    "which serves as the **context** for the contextual bandit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3c40f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "TASK 5.2: USER CLASSIFICATION - CONTEXT DETECTOR (ENHANCED)\n",
      "====================================================================================================\n",
      "\n",
      "‚úì Test labels available. Using test set for evaluation.\n",
      "\n",
      "====================================================================================================\n",
      "STEP 1: FEATURE ENGINEERING\n",
      "====================================================================================================\n",
      "\n",
      "Original features: 4\n",
      "Polynomial features (degree 2): 14\n",
      "New feature examples: ['income clicks', 'income purchase_amount', 'clicks^2', 'clicks purchase_amount', 'purchase_amount^2']\n",
      "‚úì Feature engineering complete\n",
      "\n",
      "====================================================================================================\n",
      "STEP 2: TRAINING ADVANCED CLASSIFICATION MODELS\n",
      "====================================================================================================\n",
      "\n",
      "Training multiple models with cross-validation...\n",
      "\n",
      "[1/8] Extra Trees...\n",
      "   ‚úì Accuracy: 0.6450 | Time: 7.90s\n",
      "[2/8] Gradient Boosting...\n",
      "   ‚úì Accuracy: 0.6525 | Time: 53.06s\n",
      "[3/8] Random Forest...\n",
      "   ‚úì Accuracy: 0.6250 | Time: 19.16s\n",
      "[4/8] Support Vector Machine (RBF kernel)...\n",
      "   ‚úì Accuracy: 0.5825 | Time: 13.79s\n",
      "[5/8] K-Nearest Neighbors...\n",
      "   ‚úì Accuracy: 0.5750 | Time: 0.39s\n",
      "[6/8] AdaBoost...\n",
      "   ‚úì Accuracy: 0.6500 | Time: 5.24s\n",
      "[7/8] Logistic Regression...\n",
      "   ‚úì Accuracy: 0.5550 | Time: 0.25s\n",
      "[8/8] Neural Network - MLP...\n",
      "   ‚úì Accuracy: 0.6000 | Time: 6.58s\n",
      "\n",
      "====================================================================================================\n",
      "STEP 3: ENSEMBLE VOTING\n",
      "====================================================================================================\n",
      "\n",
      "Top 3 models selected for ensemble: ['Gradient Boosting', 'AdaBoost', 'Extra Trees']\n",
      "\n",
      "Creating Soft Voting Classifier...\n",
      "‚úì Voting Ensemble Accuracy: 0.6475\n",
      "\n",
      "====================================================================================================\n",
      "FINAL RESULTS - ALL MODELS (RANKED BY ACCURACY)\n",
      "====================================================================================================\n",
      "                 Model  Accuracy  Time (s)\n",
      "     Gradient Boosting    0.6525 53.055096\n",
      "              AdaBoost    0.6500  5.238716\n",
      "Voting Ensemble (Soft)    0.6475  0.000000\n",
      "           Extra Trees    0.6450  7.898419\n",
      "         Random Forest    0.6250 19.157363\n",
      "        Neural Network    0.6000  6.580973\n",
      "             SVM (RBF)    0.5825 13.788825\n",
      "                   KNN    0.5750  0.390655\n",
      "   Logistic Regression    0.5550  0.251513\n",
      "\n",
      "====================================================================================================\n",
      "üèÜ BEST MODEL SELECTED: Gradient Boosting\n",
      "====================================================================================================\n",
      "Validation/Test Accuracy: 0.6525 (65.25%)\n",
      "Improvement over random guessing (33.33%): +31.92%\n",
      "\n",
      "====================================================================================================\n",
      "DETAILED EVALUATION - Gradient Boosting\n",
      "====================================================================================================\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 52  68  22]\n",
      " [  6 115  21]\n",
      " [  7  15  94]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      user_1       0.80      0.37      0.50       142\n",
      "      user_2       0.58      0.81      0.68       142\n",
      "      user_3       0.69      0.81      0.74       116\n",
      "\n",
      "    accuracy                           0.65       400\n",
      "   macro avg       0.69      0.66      0.64       400\n",
      "weighted avg       0.69      0.65      0.63       400\n",
      "\n",
      "\n",
      "Top 10 Feature Importances:\n",
      "               Feature  Importance\n",
      "                 age^2    0.497646\n",
      "       purchase_amount    0.295784\n",
      "     purchase_amount^2    0.113951\n",
      "   age purchase_amount    0.028374\n",
      "            age income    0.022539\n",
      "                income    0.011548\n",
      "income purchase_amount    0.008241\n",
      "         income clicks    0.006379\n",
      "              income^2    0.006377\n",
      "clicks purchase_amount    0.005289\n",
      "\n",
      "====================================================================================================\n",
      "TASK 5.2 SUMMARY - USER CLASSIFICATION\n",
      "====================================================================================================\n",
      "‚úì Feature engineering: Polynomial features (degree 2) applied\n",
      "‚úì Trained 8 advanced classifiers with hyperparameter tuning\n",
      "‚úì Created ensemble voting classifier from top 3 models\n",
      "‚úì Best model: Gradient Boosting\n",
      "‚úì Validation/Test accuracy: 0.6525 (65.25%)\n",
      "‚úì Context Detector ready for contextual bandit algorithms!\n",
      "\n",
      "‚úì Stored 'context_detector' and 'poly_transformer' for Tasks 5.3 and 5.4\n"
     ]
    }
   ],
   "source": [
    "# Task 5.2: User Classification - Context Detector (Enhanced for Higher Accuracy)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.ensemble import (RandomForestClassifier, GradientBoostingClassifier, \n",
    "                              AdaBoostClassifier, VotingClassifier, StackingClassifier, ExtraTreesClassifier)\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import PolynomialFeatures, RobustScaler, MinMaxScaler\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, \n",
    "                             confusion_matrix, classification_report)\n",
    "from sklearn.pipeline import Pipeline\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"TASK 5.2: USER CLASSIFICATION - CONTEXT DETECTOR (ENHANCED)\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Check if we have test labels\n",
    "HAS_TEST_LABELS = y_test is not None\n",
    "if not HAS_TEST_LABELS:\n",
    "    print(\"\\n‚ö†Ô∏è WARNING: Test labels not available. Using cross-validation for model selection.\")\n",
    "    # Split train data into train and validation sets for evaluation\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train_cv, X_val_cv, y_train_cv, y_val_cv = train_test_split(\n",
    "        X_train_scaled, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    "    )\n",
    "    # Use validation set as test set for metrics\n",
    "    X_test_scaled = X_val_cv\n",
    "    y_test = y_val_cv\n",
    "    X_train_scaled = X_train_cv\n",
    "    y_train = y_train_cv\n",
    "    print(\"‚úì Using 80-20 train-validation split for model selection\")\n",
    "else:\n",
    "    print(\"\\n‚úì Test labels available. Using test set for evaluation.\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: FEATURE ENGINEERING - CREATE ENHANCED FEATURES\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"STEP 1: FEATURE ENGINEERING\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Create polynomial features (degree 2 interactions)\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_train_poly = poly.fit_transform(X_train_scaled)\n",
    "X_test_poly = poly.transform(X_test_scaled)\n",
    "\n",
    "# Get feature names for reference\n",
    "poly_features = poly.get_feature_names_out(feature_columns)\n",
    "print(f\"\\nOriginal features: {len(feature_columns)}\")\n",
    "print(f\"Polynomial features (degree 2): {X_train_poly.shape[1]}\")\n",
    "print(f\"New feature examples: {list(poly_features[-5:])}\")\n",
    "\n",
    "# Use polynomial features for training\n",
    "X_train_enhanced = X_train_poly\n",
    "X_test_enhanced = X_test_poly\n",
    "\n",
    "print(f\"‚úì Feature engineering complete\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: TRAIN ADVANCED ENSEMBLE MODELS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"STEP 2: TRAINING ADVANCED CLASSIFICATION MODELS\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "classifiers = {}\n",
    "results = []\n",
    "\n",
    "print(\"\\nTraining multiple models with cross-validation...\\n\")\n",
    "\n",
    "# Use cross validation for model selection\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# 1. Extra Trees\n",
    "print(\"[1/8] Extra Trees...\")\n",
    "start_time = time.time()\n",
    "et_params = {'n_estimators': [100, 150], 'max_depth': [10, 15], 'min_samples_split': [2, 3]}\n",
    "et_grid = GridSearchCV(ExtraTreesClassifier(random_state=42, n_jobs=-1), et_params, \n",
    "                       cv=cv, scoring='accuracy', n_jobs=-1, verbose=0)\n",
    "et_grid.fit(X_train_enhanced, y_train)\n",
    "et_pred = et_grid.predict(X_test_enhanced)\n",
    "et_accuracy = accuracy_score(y_test, et_pred)\n",
    "et_time = time.time() - start_time\n",
    "classifiers['Extra Trees'] = et_grid.best_estimator_\n",
    "results.append({'Model': 'Extra Trees', 'Accuracy': et_accuracy, 'Time (s)': et_time})\n",
    "print(f\"   ‚úì Accuracy: {et_accuracy:.4f} | Time: {et_time:.2f}s\")\n",
    "\n",
    "# 2. Gradient Boosting\n",
    "print(\"[2/8] Gradient Boosting...\")\n",
    "start_time = time.time()\n",
    "gb_params = {'n_estimators': [100, 150], 'learning_rate': [0.01, 0.05], 'max_depth': [3, 4, 5]}\n",
    "gb_grid = GridSearchCV(GradientBoostingClassifier(random_state=42, validation_fraction=0.1, \n",
    "                                                   n_iter_no_change=5), gb_params, \n",
    "                       cv=cv, scoring='accuracy', n_jobs=-1, verbose=0)\n",
    "gb_grid.fit(X_train_enhanced, y_train)\n",
    "gb_pred = gb_grid.predict(X_test_enhanced)\n",
    "gb_accuracy = accuracy_score(y_test, gb_pred)\n",
    "gb_time = time.time() - start_time\n",
    "classifiers['Gradient Boosting'] = gb_grid.best_estimator_\n",
    "results.append({'Model': 'Gradient Boosting', 'Accuracy': gb_accuracy, 'Time (s)': gb_time})\n",
    "print(f\"   ‚úì Accuracy: {gb_accuracy:.4f} | Time: {gb_time:.2f}s\")\n",
    "\n",
    "# 3. Random Forest\n",
    "print(\"[3/8] Random Forest...\")\n",
    "start_time = time.time()\n",
    "rf_params = {'n_estimators': [100, 150], 'max_depth': [10, 15, 20], 'min_samples_split': [2, 3]}\n",
    "rf_grid = GridSearchCV(RandomForestClassifier(random_state=42, n_jobs=-1), rf_params, \n",
    "                       cv=cv, scoring='accuracy', n_jobs=-1, verbose=0)\n",
    "rf_grid.fit(X_train_enhanced, y_train)\n",
    "rf_pred = rf_grid.predict(X_test_enhanced)\n",
    "rf_accuracy = accuracy_score(y_test, rf_pred)\n",
    "rf_time = time.time() - start_time\n",
    "classifiers['Random Forest'] = rf_grid.best_estimator_\n",
    "results.append({'Model': 'Random Forest', 'Accuracy': rf_accuracy, 'Time (s)': rf_time})\n",
    "print(f\"   ‚úì Accuracy: {rf_accuracy:.4f} | Time: {rf_time:.2f}s\")\n",
    "\n",
    "# 4. SVM with RBF kernel\n",
    "print(\"[4/8] Support Vector Machine (RBF kernel)...\")\n",
    "start_time = time.time()\n",
    "svm_params = {'C': [0.1, 1, 10], 'gamma': ['scale', 'auto', 0.01], 'kernel': ['rbf']}\n",
    "svm_grid = GridSearchCV(SVC(random_state=42, probability=True), svm_params, \n",
    "                        cv=cv, scoring='accuracy', n_jobs=-1, verbose=0)\n",
    "svm_grid.fit(X_train_enhanced, y_train)\n",
    "svm_pred = svm_grid.predict(X_test_enhanced)\n",
    "svm_accuracy = accuracy_score(y_test, svm_pred)\n",
    "svm_time = time.time() - start_time\n",
    "classifiers['SVM (RBF)'] = svm_grid.best_estimator_\n",
    "results.append({'Model': 'SVM (RBF)', 'Accuracy': svm_accuracy, 'Time (s)': svm_time})\n",
    "print(f\"   ‚úì Accuracy: {svm_accuracy:.4f} | Time: {svm_time:.2f}s\")\n",
    "\n",
    "# 5. KNN\n",
    "print(\"[5/8] K-Nearest Neighbors...\")\n",
    "start_time = time.time()\n",
    "knn_params = {'n_neighbors': [3, 5, 7], 'weights': ['uniform', 'distance'], 'metric': ['euclidean', 'manhattan']}\n",
    "knn_grid = GridSearchCV(KNeighborsClassifier(), knn_params, cv=cv, \n",
    "                        scoring='accuracy', n_jobs=-1, verbose=0)\n",
    "knn_grid.fit(X_train_enhanced, y_train)\n",
    "knn_pred = knn_grid.predict(X_test_enhanced)\n",
    "knn_accuracy = accuracy_score(y_test, knn_pred)\n",
    "knn_time = time.time() - start_time\n",
    "classifiers['KNN'] = knn_grid.best_estimator_\n",
    "results.append({'Model': 'KNN', 'Accuracy': knn_accuracy, 'Time (s)': knn_time})\n",
    "print(f\"   ‚úì Accuracy: {knn_accuracy:.4f} | Time: {knn_time:.2f}s\")\n",
    "\n",
    "# 6. AdaBoost\n",
    "print(\"[6/8] AdaBoost...\")\n",
    "start_time = time.time()\n",
    "ada_params = {'n_estimators': [100, 150], 'learning_rate': [0.5, 1.0]}\n",
    "ada_grid = GridSearchCV(AdaBoostClassifier(random_state=42), ada_params, \n",
    "                        cv=cv, scoring='accuracy', n_jobs=-1, verbose=0)\n",
    "ada_grid.fit(X_train_enhanced, y_train)\n",
    "ada_pred = ada_grid.predict(X_test_enhanced)\n",
    "ada_accuracy = accuracy_score(y_test, ada_pred)\n",
    "ada_time = time.time() - start_time\n",
    "classifiers['AdaBoost'] = ada_grid.best_estimator_\n",
    "results.append({'Model': 'AdaBoost', 'Accuracy': ada_accuracy, 'Time (s)': ada_time})\n",
    "print(f\"   ‚úì Accuracy: {ada_accuracy:.4f} | Time: {ada_time:.2f}s\")\n",
    "\n",
    "# 7. Logistic Regression\n",
    "print(\"[7/8] Logistic Regression...\")\n",
    "start_time = time.time()\n",
    "lr_params = {'C': [0.01, 0.1, 1], 'solver': ['lbfgs', 'liblinear'], 'penalty': ['l2'], 'max_iter': [1000]}\n",
    "lr_grid = GridSearchCV(LogisticRegression(random_state=42),\n",
    "                       lr_params, cv=cv, scoring='accuracy', n_jobs=-1, verbose=0)\n",
    "lr_grid.fit(X_train_enhanced, y_train)\n",
    "lr_pred = lr_grid.predict(X_test_enhanced)\n",
    "lr_accuracy = accuracy_score(y_test, lr_pred)\n",
    "lr_time = time.time() - start_time\n",
    "classifiers['Logistic Regression'] = lr_grid.best_estimator_\n",
    "results.append({'Model': 'Logistic Regression', 'Accuracy': lr_accuracy, 'Time (s)': lr_time})\n",
    "print(f\"   ‚úì Accuracy: {lr_accuracy:.4f} | Time: {lr_time:.2f}s\")\n",
    "\n",
    "# 8. Neural Network\n",
    "print(\"[8/8] Neural Network - MLP...\")\n",
    "start_time = time.time()\n",
    "mlp_params = {'hidden_layer_sizes': [(100,), (150,)], 'activation': ['relu', 'tanh'], \n",
    "              'alpha': [0.0001, 0.001], 'learning_rate': ['constant', 'adaptive']}\n",
    "mlp_grid = GridSearchCV(MLPClassifier(max_iter=1000, random_state=42, early_stopping=True, \n",
    "                                      validation_fraction=0.1, n_iter_no_change=5), \n",
    "                        mlp_params, cv=cv, scoring='accuracy', n_jobs=-1, verbose=0)\n",
    "mlp_grid.fit(X_train_enhanced, y_train)\n",
    "mlp_pred = mlp_grid.predict(X_test_enhanced)\n",
    "mlp_accuracy = accuracy_score(y_test, mlp_pred)\n",
    "mlp_time = time.time() - start_time\n",
    "classifiers['Neural Network'] = mlp_grid.best_estimator_\n",
    "results.append({'Model': 'Neural Network', 'Accuracy': mlp_accuracy, 'Time (s)': mlp_time})\n",
    "print(f\"   ‚úì Accuracy: {mlp_accuracy:.4f} | Time: {mlp_time:.2f}s\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: CREATE ENSEMBLE VOTING CLASSIFIER\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"STEP 3: ENSEMBLE VOTING\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Get top 3 models for ensemble\n",
    "results_df_temp = pd.DataFrame(results).sort_values('Accuracy', ascending=False)\n",
    "top_3_models = results_df_temp.head(3)['Model'].tolist()\n",
    "\n",
    "print(f\"\\nTop 3 models selected for ensemble: {top_3_models}\")\n",
    "\n",
    "# Soft Voting Classifier\n",
    "print(\"\\nCreating Soft Voting Classifier...\")\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[(name, classifiers[name]) for name in top_3_models],\n",
    "    voting='soft'\n",
    ")\n",
    "voting_clf.fit(X_train_enhanced, y_train)\n",
    "voting_pred = voting_clf.predict(X_test_enhanced)\n",
    "voting_accuracy = accuracy_score(y_test, voting_pred)\n",
    "results.append({'Model': 'Voting Ensemble (Soft)', 'Accuracy': voting_accuracy, 'Time (s)': 0})\n",
    "print(f\"‚úì Voting Ensemble Accuracy: {voting_accuracy:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4: RESULTS & MODEL SELECTION\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"FINAL RESULTS - ALL MODELS (RANKED BY ACCURACY)\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values('Accuracy', ascending=False).reset_index(drop=True)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Select best model\n",
    "best_model_name = results_df.iloc[0]['Model']\n",
    "best_accuracy = results_df.iloc[0]['Accuracy']\n",
    "\n",
    "if best_model_name == 'Voting Ensemble (Soft)':\n",
    "    best_classifier = voting_clf\n",
    "    best_predictions = voting_pred\n",
    "else:\n",
    "    best_classifier = classifiers[best_model_name]\n",
    "    # Map model names to their predictions\n",
    "    model_pred_map = {\n",
    "        'Extra Trees': et_pred,\n",
    "        'Gradient Boosting': gb_pred,\n",
    "        'Random Forest': rf_pred,\n",
    "        'SVM (RBF)': svm_pred,\n",
    "        'KNN': knn_pred,\n",
    "        'AdaBoost': ada_pred,\n",
    "        'Logistic Regression': lr_pred,\n",
    "        'Neural Network': mlp_pred\n",
    "    }\n",
    "    best_predictions = model_pred_map.get(best_model_name, et_pred)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(f\"üèÜ BEST MODEL SELECTED: {best_model_name}\")\n",
    "print(\"=\" * 100)\n",
    "print(f\"Validation/Test Accuracy: {best_accuracy:.4f} ({best_accuracy*100:.2f}%)\")\n",
    "print(f\"Improvement over random guessing (33.33%): +{(best_accuracy - 0.3333)*100:.2f}%\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 5: DETAILED EVALUATION\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(f\"DETAILED EVALUATION - {best_model_name}\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(y_test, best_predictions)\n",
    "print(cm)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "class_report = classification_report(y_test, best_predictions, target_names=label_encoder.classes_)\n",
    "print(class_report)\n",
    "\n",
    "# Feature importance (if available)\n",
    "if hasattr(best_classifier, 'feature_importances_'):\n",
    "    print(\"\\nTop 10 Feature Importances:\")\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': poly_features,\n",
    "        'Importance': best_classifier.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False).head(10)\n",
    "    print(feature_importance.to_string(index=False))\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 6: SUMMARY & STORAGE\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"TASK 5.2 SUMMARY - USER CLASSIFICATION\")\n",
    "print(\"=\" * 100)\n",
    "print(f\"‚úì Feature engineering: Polynomial features (degree 2) applied\")\n",
    "print(f\"‚úì Trained 8 advanced classifiers with hyperparameter tuning\")\n",
    "print(f\"‚úì Created ensemble voting classifier from top 3 models\")\n",
    "print(f\"‚úì Best model: {best_model_name}\")\n",
    "print(f\"‚úì Validation/Test accuracy: {best_accuracy:.4f} ({best_accuracy*100:.2f}%)\")\n",
    "print(f\"‚úì Context Detector ready for contextual bandit algorithms!\")\n",
    "\n",
    "# Store for later use\n",
    "context_detector = best_classifier\n",
    "poly_transformer = poly  # Store for test predictions\n",
    "\n",
    "print(f\"\\n‚úì Stored 'context_detector' and 'poly_transformer' for Tasks 5.3 and 5.4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ba537f",
   "metadata": {},
   "source": [
    "# `Contextual Bandit`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465388d6",
   "metadata": {},
   "source": [
    "## Reward Sampler Initialization\n",
    "\n",
    "The sampler is initialized using the student's roll number `i`.\n",
    "Rewards are obtained using `sampler.sample(j)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7405636b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TASK 5.1: REWARD SAMPLER INITIALIZATION\n",
      "================================================================================\n",
      "\n",
      "Student Roll Number: 120\n",
      "\n",
      "Initializing reward sampler...\n",
      "‚úì Sampler initialized with roll number: 120\n",
      "\n",
      "================================================================================\n",
      "SAMPLER INTERFACE\n",
      "================================================================================\n",
      "\n",
      "The sampler is initialized with your roll number (student ID).\n",
      "The sampler provides rewards based on:\n",
      "  - Arm index (j): 0-11 representing different arm combinations\n",
      "  - Context: Determined by user classification\n",
      "\n",
      "Arm Mapping:\n",
      "  - Arms 0-3: For User1 (Entertainment, Education, Tech, Crime)\n",
      "  - Arms 4-7: For User2 (Entertainment, Education, Tech, Crime)\n",
      "  - Arms 8-11: For User3 (Entertainment, Education, Tech, Crime)\n",
      "\n",
      "To obtain a reward, use: sampler_instance.sample(j)\n",
      "where j is the arm index (0-11)\n",
      "\n",
      "\n",
      "================================================================================\n",
      "SAMPLE REWARDS (DEMONSTRATION)\n",
      "================================================================================\n",
      "\n",
      "Sampling from a few arms to demonstrate:\n",
      "Arm 0: Reward = 8.2825\n",
      "Arm 1: Reward = 0.6549\n",
      "Arm 5: Reward = -3.4207\n",
      "Arm 9: Reward = 5.8091\n",
      "\n",
      "‚úì Reward sampler is ready for use in contextual bandit algorithms!\n",
      "‚úì Use 'sampler_instance.sample(j)' to get rewards for any arm j (0-11)\n"
     ]
    }
   ],
   "source": [
    "# Task 5.1: Reward Sampler Initialization\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TASK 5.1: REWARD SAMPLER INITIALIZATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ‚ö†Ô∏è IMPORTANT: Update your roll number here (e.g., 126 for U20230126)\n",
    "# Extract the last 3 digits of your roll number\n",
    "ROLL_NUMBER = 120  # TODO: Change this to your roll number (last 3 digits)\n",
    "\n",
    "# Validate roll number\n",
    "if ROLL_NUMBER == 0:\n",
    "    print(\"\\n‚ùå ERROR: Please update the ROLL_NUMBER variable with your student ID (last 3 digits)\")\n",
    "    print(\"Example: If your ID is U20230126, set ROLL_NUMBER = 126\")\n",
    "else:\n",
    "    print(f\"\\nStudent Roll Number: {ROLL_NUMBER}\")\n",
    "    \n",
    "    # Initialize the sampler with the roll number\n",
    "    print(\"\\nInitializing reward sampler...\")\n",
    "    sampler_instance = sampler(ROLL_NUMBER)\n",
    "    print(f\"‚úì Sampler initialized with roll number: {ROLL_NUMBER}\")\n",
    "    \n",
    "    # Explain the sampler interface\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"SAMPLER INTERFACE\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\"\"\n",
    "The sampler is initialized with your roll number (student ID).\n",
    "The sampler provides rewards based on:\n",
    "  - Arm index (j): 0-11 representing different arm combinations\n",
    "  - Context: Determined by user classification\n",
    "  \n",
    "Arm Mapping:\n",
    "  - Arms 0-3: For User1 (Entertainment, Education, Tech, Crime)\n",
    "  - Arms 4-7: For User2 (Entertainment, Education, Tech, Crime)\n",
    "  - Arms 8-11: For User3 (Entertainment, Education, Tech, Crime)\n",
    "\n",
    "To obtain a reward, use: sampler_instance.sample(j)\n",
    "where j is the arm index (0-11)\n",
    "\"\"\")\n",
    "    \n",
    "    # Example: Sample rewards from a few arms\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"SAMPLE REWARDS (DEMONSTRATION)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    sample_rewards = {}\n",
    "    print(\"\\nSampling from a few arms to demonstrate:\")\n",
    "    for arm in [0, 1, 5, 9]:\n",
    "        reward = sampler_instance.sample(arm)\n",
    "        sample_rewards[arm] = reward\n",
    "        print(f\"Arm {arm}: Reward = {reward:.4f}\")\n",
    "    \n",
    "    print(\"\\n‚úì Reward sampler is ready for use in contextual bandit algorithms!\")\n",
    "    print(\"‚úì Use 'sampler_instance.sample(j)' to get rewards for any arm j (0-11)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f0bb0f",
   "metadata": {},
   "source": [
    "## Arm Mapping\n",
    "\n",
    "| Arm Index (j) | News Category | User Context |\n",
    "|--------------|---------------|--------------|\n",
    "| 0‚Äì3          | Entertainment, Education, Tech, Crime | User1 |\n",
    "| 4‚Äì7          | Entertainment, Education, Tech, Crime | User2 |\n",
    "| 8‚Äì11         | Entertainment, Education, Tech, Crime | User3 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356c764d",
   "metadata": {},
   "source": [
    "## Epsilon-Greedy Strategy\n",
    "\n",
    "This section implements the epsilon-greedy contextual bandit algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b571e8a6",
   "metadata": {},
   "source": [
    "## Upper Confidence Bound (UCB)\n",
    "\n",
    "This section implements the UCB strategy for contextual bandits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff2fb86",
   "metadata": {},
   "source": [
    "## SoftMax Strategy\n",
    "\n",
    "This section implements the SoftMax strategy with temperature $ \\tau = 1$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb144662",
   "metadata": {},
   "source": [
    "## Reinforcement Learning Simulation\n",
    "\n",
    "We simulate the bandit algorithms for $T = 10,000$ steps and record rewards.\n",
    "\n",
    "P.S.: Change $T$ value as and if required.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca10b073",
   "metadata": {},
   "source": [
    "## Results and Analysis\n",
    "\n",
    "This section presents:\n",
    "- Average Reward vs Time\n",
    "- Hyperparameter comparisons\n",
    "- Observations and discussion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512fbb89",
   "metadata": {},
   "source": [
    "## Final Observations\n",
    "\n",
    "- Comparison of Epsilon-Greedy, UCB, and SoftMax\n",
    "- Effect of hyperparameters\n",
    "- Strengths and limitations of each approach\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5665d58e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
